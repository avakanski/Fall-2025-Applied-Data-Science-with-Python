{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd5d005-15fe-42a5-ad59-154b9dbb841f",
   "metadata": {},
   "source": [
    "# Tutorial 7 - TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0a8de-8a5b-4d05-9f11-8ae89ebbbc77",
   "metadata": {},
   "source": [
    "[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Tutorials/Tutorial_7-TensorFlow/Tutorial_7-TensorFlow.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Tutorials/Tutorial_7-TensorFlow/Tutorial_7-TensorFlow.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e13f4-f622-4766-a5d8-1d0d4fdc17ce",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6607f8f5",
   "metadata": {},
   "source": [
    "This tutorial is adapted from a blog post by Goku Mohandas posted on the website [Made With ML](https://madewithml.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61624c49-1bd8-44c1-a41e-740d03aa3c19",
   "metadata": {},
   "source": [
    "We will first import `TensorFlow` and `NumPy` and set the seed for their random number generators for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff53b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.17.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "print(f'TF version: {tf.__version__ }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7ae361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "np.random.seed(seed=1)\n",
    "tf.random.set_seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1465c",
   "metadata": {},
   "source": [
    "## TensorFlow Basics\n",
    "\n",
    "First, we will cover some basics such as creating TensorFlow tensors and converting from common data structures to TensorFlow tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffa9665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <dtype: 'float32'>\n",
      "shape: (2, 3)\n",
      "values:\n",
      "[[-1.1012203   1.5457517   0.383644  ]\n",
      " [-0.87965786 -1.2246722  -0.9811211 ]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((2, 3))\n",
    "\n",
    "print(f'Type: {x.dtype}')\n",
    "print(f'shape: {x.shape}')\n",
    "print(f'values:\\n{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabb7cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(2, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Zeros and ones tensors\n",
    "x = tf.zeros((2, 3), dtype=tf.float64)\n",
    "print(x)\n",
    "\n",
    "x = tf.ones((2, 3))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d69655-b385-427c-b68e-21c6fe274203",
   "metadata": {},
   "source": [
    "In TensorFlow, `tf.constant` creates an immutable tensor. \n",
    "\n",
    "To convert a Python list to a TensorFlow tensor, we can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8b46a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 3)\n",
      "values:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# List to Tensor\n",
    "x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(f'shape: {x.shape}')\n",
    "print(f'values:\\n{x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453e0b4-fa00-4bfc-ad83-c303ab3e0cc4",
   "metadata": {},
   "source": [
    "Also, we can use `tf.convert_to_tensor` as in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aaabfe2-2a10-48ec-a599-7acb98daef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 3)\n",
      "values:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# List to Tensor\n",
    "x = tf.convert_to_tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(f'shape: {x.shape}')\n",
    "print(f'values:\\n{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473d17c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 3)\n",
      "values:\n",
      "[[4.17022005e-01 7.20324493e-01 1.14374817e-04]\n",
      " [3.02332573e-01 1.46755891e-01 9.23385948e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Numpy array to Tensor\n",
    "x = tf.constant(np.random.rand(2, 3))\n",
    "\n",
    "print(f'shape: {x.shape}')\n",
    "print(f'values:\\n{x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0905afb-4e15-4ffd-bd60-6fe10732bce6",
   "metadata": {},
   "source": [
    "We can also use `tf.Variable` to create a tensor that is trainable and mutable, i.e., its values can be changed during computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad1a578-88af-4683-aa52-f381deaf0403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 3)\n",
      "values:\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float64, numpy=\n",
      "array([[0.18626021, 0.34556073, 0.39676747],\n",
      "       [0.53881673, 0.41919451, 0.6852195 ]])>\n"
     ]
    }
   ],
   "source": [
    "# Numpy array to Tensor\n",
    "x = tf.Variable(np.random.rand(2, 3))\n",
    "\n",
    "print(f'shape: {x.shape}')\n",
    "print(f'values:\\n{x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32036e4f",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959f09ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 3)\n",
      "values:\n",
      "[[5 5 5]\n",
      " [5 5 5]]\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "x = tf.constant([[2, 2, 2], [2, 2, 2]])\n",
    "y = tf.constant([[3, 3, 3], [3, 3, 3]])\n",
    "z = x + y\n",
    "\n",
    "print(f'shape: {z.shape}')\n",
    "print(f'values:\\n{z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36140745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 2)\n",
      "values:\n",
      "[[18 18]\n",
      " [18 18]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplicaton\n",
    "x = tf.constant([[2, 2, 2], [2, 2, 2]])\n",
    "y = tf.constant([[3, 3], [3, 3], [3, 3]])\n",
    "z = tf.matmul(x, y)\n",
    "\n",
    "print(f'shape: {z.shape}')\n",
    "print(f'values:\\n{z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fefd133b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (2, 3)\n",
      "Transposed shape: (3, 2)\n",
      "values: \n",
      "[[2 2]\n",
      " [2 2]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "# Transpose\n",
    "x = tf.constant([[2, 2, 2], [2, 2, 2]])\n",
    "print(f\"x shape: {x.shape}\")\n",
    "y = tf.transpose(x)\n",
    "\n",
    "print(f\"Transposed shape: {y.shape}\")\n",
    "print(f\"values: \\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729b4e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 2)\n",
      "values: \n",
      "[[2 2]\n",
      " [2 2]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "# Reshape\n",
    "x = tf.constant([[2, 2, 2], [2, 2, 2]])\n",
    "z = tf.reshape(x, (3, 2))\n",
    "\n",
    "print(f\"shape: {z.shape}\")\n",
    "print(f\"values: \\n{z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf410eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values: \n",
      "[[2 2 2]\n",
      " [2 2 2]]\n",
      "values: \n",
      "[4 4 4]\n",
      "values: \n",
      "[6 6]\n"
     ]
    }
   ],
   "source": [
    "# Dimensional operations\n",
    "x = tf.constant([[2, 2, 2], [2, 2, 2]])\n",
    "print(f\"values: \\n{x}\")\n",
    "y = tf.reduce_sum(x, axis=0) # sum over columns\n",
    "print(f\"values: \\n{y}\")\n",
    "z = tf.reduce_sum(x, axis=1) # sum over rows\n",
    "print(f\"values: \\n{z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97c86e",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "We can extract, separate, and join values from tensors in a similar way as with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f988ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8 10]\n",
      " [11 12 13]]\n",
      "\n",
      "x[:1]: \n",
      "[[1 2 3]]\n",
      "\n",
      "x[:1, 1:3]: \n",
      "[[2 3]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 10], [11, 12, 13]])\n",
    "print (f\"x: \\n{x}\")\n",
    "print()\n",
    "\n",
    "# first row\n",
    "print(f\"x[:1]: \\n{x[:1]}\")\n",
    "\n",
    "# first and second row, second and third columns\n",
    "print()\n",
    "print(f\"x[:1, 1:3]: \\n{x[:2, 1:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba1f45",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "173e9430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values: \n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "values: \n",
      "[[1 3]\n",
      " [4 6]]\n",
      "values: \n",
      "[2 6]\n"
     ]
    }
   ],
   "source": [
    "# Select with dimensional indices\n",
    "x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"values: \\n{x}\")\n",
    "\n",
    "col_indices = tf.constant([0, 2])\n",
    "chosen = tf.gather(x, axis=1, indices=col_indices) # values from column 0 & 2\n",
    "print(f\"values: \\n{chosen}\")\n",
    "\n",
    "row_indices = tf.constant([0, 1])\n",
    "col_indices = tf.constant([1, 2])\n",
    "chosen = tf.gather_nd(x, indices=[row_indices, col_indices]) # values from (0, 1) & (1, 2)\n",
    "print(f\"values: \\n{chosen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd08774",
   "metadata": {},
   "source": [
    "### Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7df7b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: \n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Values: \n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Concatenation\n",
    "x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Values: \\n{x}\")\n",
    "\n",
    "y = tf.concat([x, x], axis=0) # stack by rows (axis=1 to stack by columns)\n",
    "print(f\"Values: \\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3df83",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c5006-9a90-429c-91d3-3bc9eff62ad8",
   "metadata": {},
   "source": [
    "In TensorFlow, `tf.GradientTape()` records all operations over tensors to enable automatic differentiation. In the next example, it tracks the computation of $y = x² + 2x + 1$, and then uses `.gradient` to compute the derivative  ${dy}/{dx}$ which equals 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6beda",
   "metadata": {},
   "source": [
    "* $x = 3$\n",
    "* $y = x^2 + 2x + 2$\n",
    "* $\\frac{dy}{dx} = 2x + 2 = 2*3 + 2  = 8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2f90bd7-d70f-4938-a90e-7cfc21ed6af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define a variable\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    # Consider a simple quadratic equation: y = x² + 2x + 1\n",
    "    y = x**2 + 2*x + 1   \n",
    "\n",
    "# Calculate the derivative dy/dx = 2x+2 = 2*3+2 = 8\n",
    "dy_dx = g.gradient(y, x)\n",
    "\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79797d8",
   "metadata": {},
   "source": [
    "### CUDA\n",
    "\n",
    "This section details how to check if we are able to use GPU to accelerate computations with machine learning models.\n",
    "\n",
    "The Compute Unified Device Architecture or **CUDA** is a parallel computing platform and API that allows software to use certain types of GPUs for general purpose processing. It is an extension of the C and C++ programming languages.\n",
    "\n",
    "`TensorFlow` makes using the GPU quite transparent. If the GPU compatible version of TF is installed along with the proper drivers, TF will use the GPU.\n",
    "\n",
    "[Link](https://anaconda.org/anaconda/tensorflow-gpu) to metapackage for easily installing `TensorFlow` GPU using a conda `'metapackage'`, which installs the required GPU drivers alongside TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b971d492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# is CUDA available?\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72b497b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# set device to first gpu (if available)\n",
    "device = \"/gpu:0\" if tf.test.is_built_with_cuda() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36725788",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    a = tf.constant([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5776a3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3469315699727298106\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print info about local cpu/gpu devices through tensorflow library\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# the most useful information is found in the first two lines of the output\n",
    "# 1st line is name of device (cpu/gpu and number) \n",
    "# 2nd line gives the memory limit in bits \n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec5b90",
   "metadata": {},
   "source": [
    "## Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1bfcb",
   "metadata": {},
   "source": [
    "Public datasets are an important resource for accelerating machine learning research. However, writing custom scripts to fetch and prepare each dataset individually can be tedious. \n",
    "\n",
    "**TensorFlow DataSets (TFDS)** handles the tasks of sourcing the data and standardizing it into a consistent format. Furthermore, TFDS utilizes the `tensorflow.data API` to construct high-performance input pipelines that are seamlessly usable with tensorflow.keras models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36640975",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "TFDS is a set of ready-to-use datasets for various machine learning tasks, including Computer Vision datasets, Natural Language Processing datasets, and miscellaneous other datasets for performing Unsupervised Learning, Reinforcement learning, and more.\n",
    "\n",
    "The entire list of available datasets can be found [here](https://www.tensorflow.org/datasets/catalog/overview). \n",
    "\n",
    "All of these datasets are contained under the `tensorflow_datasets` module of TFDS.\n",
    "\n",
    "Note that installing TFDS might cause some dependency issues. If that happens, it is recommended to create a new conda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96114038",
   "metadata": {},
   "source": [
    "To install TFDS:\n",
    "\n",
    "    pip install tensorflow-datasets\n",
    "\n",
    "TFDS is pre-installed in Google Colab, and it can be directly imported as in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fa39686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399eceb5",
   "metadata": {},
   "source": [
    "The following line displays the first 10 datasets in TFDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf7735fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'accentdb',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'ag_news_subset',\n",
       " 'ai2_arc',\n",
       " 'ai2_arc_with_ir',\n",
       " 'aloha_mobile',\n",
       " 'amazon_us_reviews',\n",
       " 'anli']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.list_builders()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf627d",
   "metadata": {},
   "source": [
    "### Load Dataset with TFDS \n",
    "\n",
    "The easiest way of loading a dataset with TFDS is with `tfds.load`. \n",
    "\n",
    "It will download the data and save it as `tfrecord` files, and create the `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dbc13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data), info = tfds.load('mnist', with_info=True, shuffle_files=True, as_supervised=True, split=['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd14f0f3",
   "metadata": {},
   "source": [
    "Arguments in `tfds.load` include:\n",
    "\n",
    "- First argument is the name of dataset.\n",
    "- `'split'` controls which split we wish to load (e.g., train, test, or validation).\n",
    "- `'shuffle_files'` controls whether or not data is shuffled between each epoch.\n",
    "- `'data_dir'` controls where the dataset is saved (defaults to `~/tensorflow_datasets/`).\n",
    "- `'with_info'` controls whether or not the metadata for the dataset is included.\n",
    "- `'as_supervised'` controls whether or not a tuple `(features, label)` is returned (as opposed to just features).\n",
    "- `'download'` controls whether or not the library will attempt to download the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4338dc1",
   "metadata": {},
   "source": [
    "We can access the dataset metadata with `info` as in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fefda698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    full_name='mnist/3.0.1',\n",
      "    description=\"\"\"\n",
      "    The MNIST database of handwritten digits.\n",
      "    \"\"\",\n",
      "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
      "    data_dir='C:\\\\Users\\\\avaka\\\\tensorflow_datasets\\\\mnist\\\\3.0.1',\n",
      "    file_format=tfrecord,\n",
      "    download_size=11.06 MiB,\n",
      "    dataset_size=21.00 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871206e",
   "metadata": {},
   "source": [
    "Features metadata can include features shape, label shape, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b0fc6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
       "    'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dcb410",
   "metadata": {},
   "source": [
    "We can also inspect the number of classes and label names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1960f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "print(info.features[\"label\"].num_classes)\n",
    "print(info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2aaa224-98c2-4507-97c2-fa9ba802af50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': (28, 28, 1), 'label': ()}\n",
      "{'image': <class 'numpy.uint8'>, 'label': <class 'numpy.int64'>}\n",
      "(28, 28, 1)\n",
      "<class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "print(info.features.shape)\n",
    "print(info.features.np_dtype)\n",
    "print(info.features['image'].shape)\n",
    "print(info.features['image'].np_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fdf34",
   "metadata": {},
   "source": [
    "### Slicing API for Customized Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d23dd992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST dataset, get 25% to 75% of train dataset \n",
    "ds = tfds.load('mnist', split='train[25%:75%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98891aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first 4,000 of the data for training\n",
    "ds = tfds.load('fashion_mnist', split='train[:4000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "482a7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 25% of training and all of the test data\n",
    "ds = tfds.load('fashion_mnist', split='train[:25%]+test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68368d0a",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd4a6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the training data into 10 folds (each 10%) for cross-validation\n",
    "\n",
    "trains_ds = tfds.load('fashion_mnist', split=[\n",
    "    f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])\n",
    "\n",
    "vals_ds = tfds.load('fashion_mnist', split=[\n",
    "    f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])\n",
    "\n",
    "# k = 0, 10, 20, …, 90\n",
    "# Example: third fold (k=20)\n",
    "# train[:20%]+train[30%:]  → use all except data from 20% to 30% for training\n",
    "# train[20%:30%]            → use data from 20% to 30% for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b5652",
   "metadata": {},
   "source": [
    "### Iterate over the Dataset in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5094ece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img: (16, 28, 28, 1), labels: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3]\n",
      "img: (16, 28, 28, 1), labels: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3]\n",
      "img: (16, 28, 28, 1), labels: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3]\n",
      "img: (16, 28, 28, 1), labels: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3]\n",
      "img: (16, 28, 28, 1), labels: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3]\n",
      "img: (16, 28, 28, 1), labels: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3]\n",
      "img: (16, 28, 28, 1), labels: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3]\n",
      "img: (16, 28, 28, 1), labels: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3]\n",
      "img: (16, 28, 28, 1), labels: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3]\n",
      "img: (16, 28, 28, 1), labels: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "n = 0\n",
    "for img, label in training_data.batch(batch_size):\n",
    "    # print the first 10 batches\n",
    "    while n < 10:\n",
    "        print(f'img: {img.shape}, labels: {label}') \n",
    "        # notice that img.shape = [batch size, pixels width, pixels heights, channels number)\n",
    "        # notice that label.shape = 16, therefore 16 labels are shown\n",
    "        n = n+1            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431572b",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e487193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4bae0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4bae0_level0_col0\" class=\"col_heading level0 col0\" >image</th>\n",
       "      <th id=\"T_4bae0_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4bae0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4bae0_row0_col0\" class=\"data row0 col0\" ><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzElEQVR4nGNgGPQg5F8qjMmEIRn1XwinRvnvp2QxdTIyMjAwMDDksd17jCnpeN6CgYGBQZfhAhbzLP+WMzAwyPz8IAkXQuh8ycDAwMAQyHr1ORZJYQYGBgYGKYYDDFgkAxgZGBikMxnnISQZYQz2J0KXjwvpqV00+YfpnsS/f//++/v3bxiSGAuMYfp97rN3b1cz7MDiEQgI+bcGmYsatlH/T+PUyPD2jwVOOaOP23Br3P3vZyZOO///v7qGARd4/EkBt7FvbuOWoyIAAPBxN9oBRuu9AAAAAElFTkSuQmCC\" alt=\"Img\" /></td>\n",
       "      <td id=\"T_4bae0_row0_col1\" class=\"data row0 col1\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4bae0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4bae0_row1_col0\" class=\"data row1 col0\" ><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAmklEQVR4nGNgGOyAc/5KJlxyjIv+/TPEJan9798HWRgH3YhQBoZHj3HpfP/vVxQuOYF//54ieGjGNjEwXMalkeHbv3+eeCTfseAy1oCVYeofXBp3/f8lgUtO/su/azhtnPLvXwJOycv//uGU0//5bx1OySP//hngkuN5+u8tG4oIkj/VJBmO/8Il6cvAMBunlSIvX3DjlKQmAACHtTHZmy2LVAAAAABJRU5ErkJggg==\" alt=\"Img\" /></td>\n",
       "      <td id=\"T_4bae0_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4bae0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4bae0_row2_col0\" class=\"data row2 col0\" ><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+0lEQVR4nM3QvyuEARzH8feVniSFiTBcKf8A85ESNqwkdbeY5NdmY/UXiM0z+Ad0SXZ1g0vJjw0Xi86pc13eD8NleJ47q3zG76tv3x/wJxk+jQ7bf8NldT9obdNl1fWW1nGh6mtLPIm0oJVWlvuIvOvMWx1qtqWa0U2aULNNNnCtT+MQapi0waK6CoR6mcS8Wuhq4Fqj1PZjU5NQnX0DUine433dD1qZBwjOrM/EsVc9AmCj6c5MWT8XADjX2mgMi+otABN1k0/YVncAFu/VLWLbPgNBduRxLBMQHe/FZ+Zs5EtLm8kP9F81MHrZTScNelZK6sFcXzP9o3wDadaKxdoXqEQAAAAASUVORK5CYII=\" alt=\"Img\" /></td>\n",
       "      <td id=\"T_4bae0_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4bae0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_4bae0_row3_col0\" class=\"data row3 col0\" ><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA40lEQVR4nGNgGJpg//8OdCEWKM2obvgv/++6/zcZnJQebPuNqkjvLxI4IIgiJ3/v79/3b//9/fvv7993f/9NQzE2TZ6hc8J3JwYGBgaGK7cYeJE12n79+1cCxlH9+/cVB5LOlxy/pryHSUYyMO78gSR5W/vzU7gxfAz/cfq36fvfDw445Fr+/P3bg12KMebb37+7WbDKKSz89/fvNRmscjpb//79u0YBq5z0yb9//2bhcEvXv78fs1ixyzX//PsuHYc+gbvwAMcEmX//3kFzJxOc9YCRoesJLp3sx9+p4JIjGQAAnrpmBs0pxioAAAAASUVORK5CYII=\" alt=\"Img\" /></td>\n",
       "      <td id=\"T_4bae0_row3_col1\" class=\"data row3 col1\" >7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4bae0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_4bae0_row4_col0\" class=\"data row4 col0\" ><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABRUlEQVR4nM2RMUgCcRTGvwvFjjiuhqgIpG4Kg8QaQi6aWoo2JXAot6ZokxwdGxybWjJoapLCQRoaJBKkoIJuKDc7DAruIEmH967hvPJ/Nkdv+vh+fI//+/7Afxglyy+qK6UeW1MSg5p2twVp4lUMhGYuLSYiJiIec72AlzqNAVf1klUGYLaF3BqzXdQBbLJtc0Fgsx26ngQARA72miQLMEU08qM/PDkAAIgBC56TQV58apSZ+ex4Pw6k2fQdHly9fSYi7phmi6pRz/4uQZleVyUnPSo5wEPmAv2zYlFto9BgS+tnuRZXNGCJec6PgiWmExXALt8HRDS+U3n6zMkAhgxOCUg+YqJyEgAQp4bayxZr7OS7HYXfebtru8sT8w4MJQwAena4fihsjRjE9Ob+Zftc8ZUQWtan5KTkPN40i9VfCvi7+QIpz4HjFNztwwAAAABJRU5ErkJggg==\" alt=\"Img\" /></td>\n",
       "      <td id=\"T_4bae0_row4_col1\" class=\"data row4 col1\" >8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "                                               image  label\n",
       "0  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      4\n",
       "1  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      1\n",
       "2  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      0\n",
       "3  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      7\n",
       "4  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list 5 images and labels from mnist dataset as pandas dataframe\n",
    "ds, info = tfds.load('mnist', split='train', with_info=True)\n",
    "\n",
    "tfds.as_dataframe(ds.take(5), info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da9b55",
   "metadata": {},
   "source": [
    "The function `tfds.show_examples` returns a matplotlib figure with data samples and labels (only image datasets are supported with this method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b39ac4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfds.show_examples(ds, info, rows=1, cols=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824225f",
   "metadata": {},
   "source": [
    "You can also create your own TFDS with `tf.data.Dataset.from_tensor_slices` as in the following example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a3bfe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch shape: (4, 100, 100), y_batch shape: (4, 1)\n",
      "x_batch shape: (4, 100, 100), y_batch shape: (4, 1)\n",
      "x_batch shape: (4, 100, 100), y_batch shape: (4, 1)\n",
      "x_batch shape: (4, 100, 100), y_batch shape: (4, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's create random features and labels\n",
    "np.random.seed(1)\n",
    "features = np.random.uniform(0,1, size=(16, 100, 100))\n",
    "labels = np.random.randint(0, 2, size=(16, 1))\n",
    "\n",
    "# create a TFDS dataset, and use a batch of 4\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(4)\n",
    "\n",
    "# print the features x and labels y in the dataset\n",
    "# there are 16 data points, and each batch has 4 data points\n",
    "for (x_batch, y_batch) in dataset:\n",
    "    print(f'x_batch shape: {x_batch.shape}, y_batch shape: {y_batch.shape}')\n",
    "print()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7be41",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. Moroney, L. (n.d.). AI and Machine Learning for Coders. O’Reilly Online Learning. https://www.oreilly.com/library/view/ai-and-machine/9781492078180/ch04.html \n",
    "2. TensorFlow datasets. TensorFlow. (n.d.). https://www.tensorflow.org/datasets/overview#iterate_over_a_dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2910aa3-43ff-4edc-a0a8-f50ab6c43648",
   "metadata": {},
   "source": [
    "[BACK TO TOP](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
