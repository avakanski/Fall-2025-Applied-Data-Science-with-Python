{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Zc3eqC2hVQCg-97uT5BZZR-lWutdK5jP","timestamp":1699675698515},{"file_id":"1WlJSR0FBSQtTqM8AVpW48zbvAIyj5d7F","timestamp":1699662628845}],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPt59j1CGoK0ckBwzGSsjj7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"008f2e0c9a3845b2929e366bfe62deaf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_547924a4851f400e8e96f84e88ce5796","IPY_MODEL_6b510ae3269d4cd1a93ed9bf6216e044","IPY_MODEL_965ffea29fff4ac487b17e74bb811729"],"layout":"IPY_MODEL_88514efd12914b378c3a1636002d4648"}},"547924a4851f400e8e96f84e88ce5796":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cd5350e1fdc4f2ea03c02c79143e6cc","placeholder":"​","style":"IPY_MODEL_5e657027bb484b1fbd5a05f34e8d6124","value":"Loading checkpoint shards: 100%"}},"6b510ae3269d4cd1a93ed9bf6216e044":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d39b550843aa4eb6b245ad0714df6232","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3119fb747a2e46e88ae01cba688fa813","value":2}},"965ffea29fff4ac487b17e74bb811729":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_698b241c1ea44f419aae192e74f15b2d","placeholder":"​","style":"IPY_MODEL_43213a4406f041e0af43f188f565a8e6","value":" 2/2 [00:16&lt;00:00,  7.75s/it]"}},"88514efd12914b378c3a1636002d4648":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cd5350e1fdc4f2ea03c02c79143e6cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e657027bb484b1fbd5a05f34e8d6124":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d39b550843aa4eb6b245ad0714df6232":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3119fb747a2e46e88ae01cba688fa813":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"698b241c1ea44f419aae192e74f15b2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43213a4406f041e0af43f188f565a8e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b25e331f3a8648389ccff6bd0f32a249":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c85bbee2d1eb4ef7a5ba917af34078c6","IPY_MODEL_a6b0d162129247538725b972fe87f6eb","IPY_MODEL_df6637420e1c4cacbea1dc7605935d59"],"layout":"IPY_MODEL_800647d031dc40a5857f1bf3fd488525"}},"c85bbee2d1eb4ef7a5ba917af34078c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0dd65725373462a90432b8b9b05ab79","placeholder":"​","style":"IPY_MODEL_c307f97d998e42bf88c4e78f43eb4cbd","value":"tokenizer_config.json: "}},"a6b0d162129247538725b972fe87f6eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5720930d9b204f6e9f1172835a411743","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ade5d6631444e2e883d1ec71f502a58","value":1}},"df6637420e1c4cacbea1dc7605935d59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9f0800590594e829a7cba7a8a6658db","placeholder":"​","style":"IPY_MODEL_ee68666be56844a18dc52328ac39af67","value":" 7.30k/? [00:00&lt;00:00, 822kB/s]"}},"800647d031dc40a5857f1bf3fd488525":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0dd65725373462a90432b8b9b05ab79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c307f97d998e42bf88c4e78f43eb4cbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5720930d9b204f6e9f1172835a411743":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"1ade5d6631444e2e883d1ec71f502a58":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9f0800590594e829a7cba7a8a6658db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee68666be56844a18dc52328ac39af67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c5f8919688b46b2a729b5dbe0af4bee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e14a2fe67ea4263becf5f273920c43a","IPY_MODEL_93029e32bd9c48df952225281361511d","IPY_MODEL_8cbfdd2c7772499faf4cdbd15920c69b"],"layout":"IPY_MODEL_e6d96c67596f4e38b15abd5089349fdb"}},"8e14a2fe67ea4263becf5f273920c43a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98b88b70476e46f3ac4ceaf65d0b0ee6","placeholder":"​","style":"IPY_MODEL_b0a8cede8f3f43f5965662ebe9123937","value":"vocab.json: "}},"93029e32bd9c48df952225281361511d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7010c40ac59e4b529ad46c6bf3231736","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4df0d43292ea4a32a8acd35abd34bf7c","value":1}},"8cbfdd2c7772499faf4cdbd15920c69b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec2bdc46c8304ad6b31688c729e72c1b","placeholder":"​","style":"IPY_MODEL_c2da2c2ca4b0453d92e789e09e0ec9f9","value":" 2.78M/? [00:00&lt;00:00, 17.7MB/s]"}},"e6d96c67596f4e38b15abd5089349fdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98b88b70476e46f3ac4ceaf65d0b0ee6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0a8cede8f3f43f5965662ebe9123937":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7010c40ac59e4b529ad46c6bf3231736":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"4df0d43292ea4a32a8acd35abd34bf7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec2bdc46c8304ad6b31688c729e72c1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2da2c2ca4b0453d92e789e09e0ec9f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eac65e48aac241c9a28e896d7995630c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34539b6b7e574dc58e42fe8e6668deb9","IPY_MODEL_ff13bdc4469e46c0ab8b84c5de43b002","IPY_MODEL_1ee117908c13424994d2a978990448bb"],"layout":"IPY_MODEL_25f4faf976234bf5bd21ab68d8d1b777"}},"34539b6b7e574dc58e42fe8e6668deb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9ec0511c367414a8b3d87d390a443d6","placeholder":"​","style":"IPY_MODEL_2470a6f6f1414329b6e0094ad41eaef8","value":"merges.txt: "}},"ff13bdc4469e46c0ab8b84c5de43b002":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bd44ee1eefd4eb2a690e4e0e34d50be","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_06842c3694994099a5ef3223401749b9","value":1}},"1ee117908c13424994d2a978990448bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e21cbe7e46c4b68ae4f95c721ad3511","placeholder":"​","style":"IPY_MODEL_a0ed6084d49a460d8608cf0094a13ded","value":" 1.67M/? [00:00&lt;00:00, 89.6MB/s]"}},"25f4faf976234bf5bd21ab68d8d1b777":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9ec0511c367414a8b3d87d390a443d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2470a6f6f1414329b6e0094ad41eaef8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4bd44ee1eefd4eb2a690e4e0e34d50be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"06842c3694994099a5ef3223401749b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e21cbe7e46c4b68ae4f95c721ad3511":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ed6084d49a460d8608cf0094a13ded":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d3e9da53b5142da9cc3ad6131759085":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b2df293baf944ec84dc928639549fc2","IPY_MODEL_b0269d5cae5e46e599935f88699fe428","IPY_MODEL_dec10d203e69499cb35f973dc323afd8"],"layout":"IPY_MODEL_c371773ef8a64ad5ad8ec69e8ecda249"}},"8b2df293baf944ec84dc928639549fc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b204d867b7b46c59bf4b4f1430b32db","placeholder":"​","style":"IPY_MODEL_58a8eab5d736455cba99dd8c6662d122","value":"tokenizer.json: "}},"b0269d5cae5e46e599935f88699fe428":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c2e1fdd82fc455f993bee951ef51c01","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92f127b2716d428b9eebc2c7ee2cec2a","value":1}},"dec10d203e69499cb35f973dc323afd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31d753e5714c45929ee64d9a5f02b6a3","placeholder":"​","style":"IPY_MODEL_b2829d96854b4967b63aaedc0fc50faa","value":" 7.03M/? [00:00&lt;00:00, 16.6MB/s]"}},"c371773ef8a64ad5ad8ec69e8ecda249":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b204d867b7b46c59bf4b4f1430b32db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58a8eab5d736455cba99dd8c6662d122":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c2e1fdd82fc455f993bee951ef51c01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"92f127b2716d428b9eebc2c7ee2cec2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"31d753e5714c45929ee64d9a5f02b6a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2829d96854b4967b63aaedc0fc50faa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1dc70005ba7f4f429adb07eaaa210513":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ae5e9e435bb411580bfa38ad76da733","IPY_MODEL_33375dbe4988424bbf20d8f149921f3a","IPY_MODEL_f744cdef0b344d6b86706c8f29fbdf0c"],"layout":"IPY_MODEL_61700fd97c964816ba9f5a4b63c1a88a"}},"6ae5e9e435bb411580bfa38ad76da733":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae1b6d6f15d44f69b75f95ec696a06e5","placeholder":"​","style":"IPY_MODEL_de060db72d914dbaa45f1b4af2867fe0","value":"tokenizer_config.json: "}},"33375dbe4988424bbf20d8f149921f3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a6e7b12641948869301bc8237d28a4e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_acd0599ad12542fab8d85d584442eb79","value":1}},"f744cdef0b344d6b86706c8f29fbdf0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01207a744ee34187a03900c57870e1da","placeholder":"​","style":"IPY_MODEL_e6dd4fbc5b5b4de6934c270596a8ac75","value":" 2.10k/? [00:00&lt;00:00, 260kB/s]"}},"61700fd97c964816ba9f5a4b63c1a88a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae1b6d6f15d44f69b75f95ec696a06e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de060db72d914dbaa45f1b4af2867fe0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a6e7b12641948869301bc8237d28a4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"acd0599ad12542fab8d85d584442eb79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01207a744ee34187a03900c57870e1da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6dd4fbc5b5b4de6934c270596a8ac75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20b8cc61dade46139a25fc8daf44ba59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd99a9f9eebb43db83b5d36584f8e3a9","IPY_MODEL_d1c15eff08894134bda67279804d59f5","IPY_MODEL_7f01b29c25b94eb0be9f37ca44fdf721"],"layout":"IPY_MODEL_8d2a5ca18f9244988263b5f28290a5d5"}},"bd99a9f9eebb43db83b5d36584f8e3a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c3eca629ff64bbba20d52334cc93a2d","placeholder":"​","style":"IPY_MODEL_5debf6b852ba47e3b62387575d7903f5","value":"tokenizer.model: 100%"}},"d1c15eff08894134bda67279804d59f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aad05b16d3494a50a56bd0935fe10703","max":493443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c7cd77336474a3abc38350b6e9b28ae","value":493443}},"7f01b29c25b94eb0be9f37ca44fdf721":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bb9a4f88bf24a5595b1a05c70fc1039","placeholder":"​","style":"IPY_MODEL_7b2c93ab83034e0284b8ccff0d2d9b10","value":" 493k/493k [00:01&lt;00:00, 423kB/s]"}},"8d2a5ca18f9244988263b5f28290a5d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c3eca629ff64bbba20d52334cc93a2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5debf6b852ba47e3b62387575d7903f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aad05b16d3494a50a56bd0935fe10703":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c7cd77336474a3abc38350b6e9b28ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bb9a4f88bf24a5595b1a05c70fc1039":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b2c93ab83034e0284b8ccff0d2d9b10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0739f745bec645fd8d9a16d5e5a801d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_831c73f1922142d3812b1ee7e6e912bc","IPY_MODEL_b1b01c61f80646039816c5e0e2377a0a","IPY_MODEL_d8953b58815d410cbe8d4de9df4719a3"],"layout":"IPY_MODEL_8239fe51e5e847eab444f544e67ee7ce"}},"831c73f1922142d3812b1ee7e6e912bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed49b26185a04590a0097e5902098edf","placeholder":"​","style":"IPY_MODEL_96232db2cee24f9193a8550344d3f3b4","value":"tokenizer.json: "}},"b1b01c61f80646039816c5e0e2377a0a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8af62b18d2a6495fa77086e604c995a5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b00e712540764c8caeabd8a4b01714cf","value":1}},"d8953b58815d410cbe8d4de9df4719a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee0f7b1ea52642619b2742aec361de5a","placeholder":"​","style":"IPY_MODEL_242d277972bf4e079a2a90efee3d40d0","value":" 1.80M/? [00:00&lt;00:00, 95.4MB/s]"}},"8239fe51e5e847eab444f544e67ee7ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed49b26185a04590a0097e5902098edf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96232db2cee24f9193a8550344d3f3b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8af62b18d2a6495fa77086e604c995a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"b00e712540764c8caeabd8a4b01714cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee0f7b1ea52642619b2742aec361de5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"242d277972bf4e079a2a90efee3d40d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"357fb899df10485f8138dbf11048b35c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fdc39eb94c664c8889eb57a18224fb94","IPY_MODEL_e4a0ecded91b4e32b5bf76a804615787","IPY_MODEL_8916bcd50d354a3eae1377fb7ccd22e8"],"layout":"IPY_MODEL_b3c70b84a3104165bf44c6a4821d7f95"}},"fdc39eb94c664c8889eb57a18224fb94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_050302f6db11462c9a8c4fab36532959","placeholder":"​","style":"IPY_MODEL_ee311576a9f247c79cc2b54f236a5cc2","value":"special_tokens_map.json: 100%"}},"e4a0ecded91b4e32b5bf76a804615787":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b9f7225bdd3492ca0869c66f83fc121","max":414,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f99114251ce442c19e3f99ec37578e4b","value":414}},"8916bcd50d354a3eae1377fb7ccd22e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61d00b7b55754096bc56a77a7974c3d4","placeholder":"​","style":"IPY_MODEL_5d7813badf8840298554f253d260fb9c","value":" 414/414 [00:00&lt;00:00, 54.8kB/s]"}},"b3c70b84a3104165bf44c6a4821d7f95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"050302f6db11462c9a8c4fab36532959":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee311576a9f247c79cc2b54f236a5cc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b9f7225bdd3492ca0869c66f83fc121":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f99114251ce442c19e3f99ec37578e4b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61d00b7b55754096bc56a77a7974c3d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d7813badf8840298554f253d260fb9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a679df7899194d88abc4fc2dca338956":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_effdc503a6374e6aa379052fd4ff871b","IPY_MODEL_ab96fa2c740d4a5c8dbec21f02fecec3","IPY_MODEL_239a14dc1cbc4409aaafdb1103af006c"],"layout":"IPY_MODEL_e33c7efc337e4f43817d82e81f230e87"}},"effdc503a6374e6aa379052fd4ff871b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbeef2d6c0c04547837d9ee45ee7f245","placeholder":"​","style":"IPY_MODEL_bd80e25d866f463fbfab8799ae4297fc","value":"config.json: 100%"}},"ab96fa2c740d4a5c8dbec21f02fecec3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_afccc669f8c74cf29cb00a5349f3ae62","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_faf08f54dac24a72acd27fc22084b7d8","value":571}},"239a14dc1cbc4409aaafdb1103af006c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3b09632b97942f19e46bfb6ea1456d1","placeholder":"​","style":"IPY_MODEL_b9e25e5c3e634fe7a5aa12a6701f8100","value":" 571/571 [00:00&lt;00:00, 61.5kB/s]"}},"e33c7efc337e4f43817d82e81f230e87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbeef2d6c0c04547837d9ee45ee7f245":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd80e25d866f463fbfab8799ae4297fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afccc669f8c74cf29cb00a5349f3ae62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faf08f54dac24a72acd27fc22084b7d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3b09632b97942f19e46bfb6ea1456d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9e25e5c3e634fe7a5aa12a6701f8100":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e37fdb5920d0413ba904aa4a03b0f482":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a58086f0846f47348829f3f0fe25fa2f","IPY_MODEL_a0600aec62b24f1f935464708979e313","IPY_MODEL_7aa26a1c4e704fec9c6417286c79fa0f"],"layout":"IPY_MODEL_1542f36f06684508819be057b80d2ce0"}},"a58086f0846f47348829f3f0fe25fa2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c6b10679cbc468dad9921b93cfad7d1","placeholder":"​","style":"IPY_MODEL_0652683631b54e32896e1c890117a54a","value":"model.safetensors.index.json: "}},"a0600aec62b24f1f935464708979e313":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c7e450830d849ea86e53721dbae1a7f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5483a622e98a4c07aeb588fa7d31d05e","value":1}},"7aa26a1c4e704fec9c6417286c79fa0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf2415f485044b90bb8b139cc7efbf60","placeholder":"​","style":"IPY_MODEL_e1ad3cbbaec04f08b0ffad601136dae4","value":" 25.1k/? [00:00&lt;00:00, 2.95MB/s]"}},"1542f36f06684508819be057b80d2ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c6b10679cbc468dad9921b93cfad7d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0652683631b54e32896e1c890117a54a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c7e450830d849ea86e53721dbae1a7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"5483a622e98a4c07aeb588fa7d31d05e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf2415f485044b90bb8b139cc7efbf60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1ad3cbbaec04f08b0ffad601136dae4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57caf9bde4e648dbaad25207590f20ab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_898d83b01ee54bc6b5db78ef859cdcaf","IPY_MODEL_e2069af8f1b94d6da4d507c1704cbca3","IPY_MODEL_1f7f1da20fd84789bcce8c128c91ff7d"],"layout":"IPY_MODEL_9be23a18f5214cc19124eef485aae0f5"}},"898d83b01ee54bc6b5db78ef859cdcaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27a565e0a94f4806886567617cd5278a","placeholder":"​","style":"IPY_MODEL_f0ad0ac9a73a4a63b609f56970551191","value":"Fetching 2 files: 100%"}},"e2069af8f1b94d6da4d507c1704cbca3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb62cac666c444f9b5a47dbdf8cea0e7","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c830e420b1d54287a848e129b19245ca","value":2}},"1f7f1da20fd84789bcce8c128c91ff7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ec80b8890c340e3ae5760d5fa039614","placeholder":"​","style":"IPY_MODEL_8dd56255666644d8982d249142ec8594","value":" 2/2 [00:39&lt;00:00, 39.07s/it]"}},"9be23a18f5214cc19124eef485aae0f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27a565e0a94f4806886567617cd5278a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0ad0ac9a73a4a63b609f56970551191":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb62cac666c444f9b5a47dbdf8cea0e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c830e420b1d54287a848e129b19245ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ec80b8890c340e3ae5760d5fa039614":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dd56255666644d8982d249142ec8594":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88a07a32d48140f1b90943a5c340ff35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2f0aa4dedcd4f6c9c3648fdd842ac22","IPY_MODEL_1ea69a52aea842f8a0220e027410666f","IPY_MODEL_d328c0f9e2414a959d6b5985d626e880"],"layout":"IPY_MODEL_4e033973ceb14862beb0fc42e10fdc18"}},"a2f0aa4dedcd4f6c9c3648fdd842ac22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1d833c20e5d490dabdd47b408f4ba97","placeholder":"​","style":"IPY_MODEL_4c0e4b872bad4116ae0b0c976e829619","value":"model-00001-of-00002.safetensors: 100%"}},"1ea69a52aea842f8a0220e027410666f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_792c78ac3a5e41da8f5f42b6cb4bd205","max":9942981696,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f356a3151594952890123a7e2fe34e2","value":9942981696}},"d328c0f9e2414a959d6b5985d626e880":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08b6b8b89006484597ed6f094323532f","placeholder":"​","style":"IPY_MODEL_1fccc0b48087456e859f6729e04f1dfa","value":" 9.94G/9.94G [00:38&lt;00:00, 448MB/s]"}},"4e033973ceb14862beb0fc42e10fdc18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1d833c20e5d490dabdd47b408f4ba97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c0e4b872bad4116ae0b0c976e829619":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"792c78ac3a5e41da8f5f42b6cb4bd205":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f356a3151594952890123a7e2fe34e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08b6b8b89006484597ed6f094323532f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fccc0b48087456e859f6729e04f1dfa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3932a9bd73ab4baab2692d54be73f5cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c59dc0374834ff799fd4dc2544f4ec5","IPY_MODEL_e9d0b6f3a9604e769941b7ed1e52ea30","IPY_MODEL_6bcbd0e6119f4c7a974c113943e28d37"],"layout":"IPY_MODEL_85c58364058948a48418395bf2da5325"}},"4c59dc0374834ff799fd4dc2544f4ec5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce38a000bcc74a2780484727e25f1740","placeholder":"​","style":"IPY_MODEL_597b24deb2f64b469db1b44da4799dd4","value":"model-00002-of-00002.safetensors: 100%"}},"e9d0b6f3a9604e769941b7ed1e52ea30":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4c5fdb63e8c49ba85863960facec783","max":4540516344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1904f97c495b44ec986a9c5a5ed10455","value":4540516344}},"6bcbd0e6119f4c7a974c113943e28d37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42d7a490356a4e8d9f32b813c322d511","placeholder":"​","style":"IPY_MODEL_b5580abb5c6544b496f9e969afdc2888","value":" 4.54G/4.54G [00:29&lt;00:00, 49.9MB/s]"}},"85c58364058948a48418395bf2da5325":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce38a000bcc74a2780484727e25f1740":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"597b24deb2f64b469db1b44da4799dd4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4c5fdb63e8c49ba85863960facec783":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1904f97c495b44ec986a9c5a5ed10455":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42d7a490356a4e8d9f32b813c322d511":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5580abb5c6544b496f9e969afdc2888":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c7b9d113074440f83e8439bb54bdf02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd6415477ca64680a4e7d81e6170e332","IPY_MODEL_5cebd5c0671848c9a28ef6350947cbb6","IPY_MODEL_cfb892c74ff4444eba555834e50a2757"],"layout":"IPY_MODEL_424af52ceab74b6abf3cbe5bd420727d"}},"cd6415477ca64680a4e7d81e6170e332":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_834afa8ba53846308668e8e8cdcedbe6","placeholder":"​","style":"IPY_MODEL_496f6d71d0e4488dad8227ccffe4f494","value":"Loading checkpoint shards: 100%"}},"5cebd5c0671848c9a28ef6350947cbb6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6220310003f41df997687d63048904d","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_616939576bf64c86a8e13b8b8f045794","value":2}},"cfb892c74ff4444eba555834e50a2757":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae6d3fa52cd84fd0806215c4cb72f6e0","placeholder":"​","style":"IPY_MODEL_ea303d9ae1a44808b000806e7e6b4f4e","value":" 2/2 [00:06&lt;00:00,  6.51s/it]"}},"424af52ceab74b6abf3cbe5bd420727d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"834afa8ba53846308668e8e8cdcedbe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"496f6d71d0e4488dad8227ccffe4f494":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6220310003f41df997687d63048904d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"616939576bf64c86a8e13b8b8f045794":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae6d3fa52cd84fd0806215c4cb72f6e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea303d9ae1a44808b000806e7e6b4f4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f3f0dda1ec046ffb74766b44a290643":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72fe88681d704c4184353bf735a1e949","IPY_MODEL_29504008f072419d818bd608b9fd66b3","IPY_MODEL_add66c258aae48968b02060bc7e417ce"],"layout":"IPY_MODEL_bb21d31cc9c442899ea74009c519f0a2"}},"72fe88681d704c4184353bf735a1e949":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_500412acbc6f480c9d505b857292c00b","placeholder":"​","style":"IPY_MODEL_15db865aa55b492687a2c22596b0dc54","value":"generation_config.json: 100%"}},"29504008f072419d818bd608b9fd66b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b04eb00fb0b047b5bc19cd59cc7f5799","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_401b418657064191b0bce454ce3e3470","value":116}},"add66c258aae48968b02060bc7e417ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06a864c9fa454f4f930df94c07f8b60f","placeholder":"​","style":"IPY_MODEL_b9dd0102368a4d1ea5b24228f583369e","value":" 116/116 [00:00&lt;00:00, 13.9kB/s]"}},"bb21d31cc9c442899ea74009c519f0a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"500412acbc6f480c9d505b857292c00b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15db865aa55b492687a2c22596b0dc54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b04eb00fb0b047b5bc19cd59cc7f5799":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"401b418657064191b0bce454ce3e3470":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06a864c9fa454f4f930df94c07f8b60f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9dd0102368a4d1ea5b24228f583369e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80d3e9d409ae472384fc04a5ceabdd1f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f733a98ead644eebd10d5bd3480b024","IPY_MODEL_c74427d2db1a4050a52c2488f2ea8772","IPY_MODEL_9333fe07edc844a085bc0b60557b7414"],"layout":"IPY_MODEL_ffb3c521a96e46dd9017b7822a1fec77"}},"9f733a98ead644eebd10d5bd3480b024":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a4edcc9e76847c6a2c49b5de9189bc6","placeholder":"​","style":"IPY_MODEL_c5a0cfeb7eec4bbcb7dd7714c9e2783a","value":"Map: 100%"}},"c74427d2db1a4050a52c2488f2ea8772":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10fdf2a83f164f5385f207aef72a496e","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df3a69f95a894e9b964b38557d21ca71","value":2}},"9333fe07edc844a085bc0b60557b7414":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1361bb0ce14343049834c94bd5f0d77f","placeholder":"​","style":"IPY_MODEL_9e5302fee4864fefa4333f6bdb8ee29c","value":" 2/2 [00:00&lt;00:00, 107.98 examples/s]"}},"ffb3c521a96e46dd9017b7822a1fec77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a4edcc9e76847c6a2c49b5de9189bc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5a0cfeb7eec4bbcb7dd7714c9e2783a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10fdf2a83f164f5385f207aef72a496e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df3a69f95a894e9b964b38557d21ca71":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1361bb0ce14343049834c94bd5f0d77f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e5302fee4864fefa4333f6bdb8ee29c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Lecture 21 - Large Language Models"],"metadata":{"id":"xoYnii0YEv9n"}},{"cell_type":"markdown","source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_21-LLMs/Lecture_21-LLMs.ipynb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_21-LLMs/Lecture_21-LLMs.ipynb)"],"metadata":{"id":"QSUTHCD9lnyf"}},{"cell_type":"markdown","source":["<a id='top'></a>"],"metadata":{"id":"iGbTNIZlln1J"}},{"cell_type":"markdown","source":["- [21.1 Introduction to LLMs](#21.1-introduction-to-llms)\n","  - [21.1.1 Architecture of Large Language Models](#21.1.1-architecture-of-large-language-models)\n","  - [21.1.2 Variants of Transformer Network Architectures](#21.1.2-variants-of-transformer-network-architectures)\n","  - [21.1.3 Modern LLM Architectures](#21.1.3-modern-llm-architectures)\n","- [21.2 Creating LLMs](#21.2-creating-llms)\n","  - [21.2.1 Pretraining](#21.2.1-pretraining)\n","  - [21.2.2 Supervised Finetuning](#21.2.2-supervised-finetuning)\n","  - [21.2.3 Alignment ](#21.2.3-alignment)\n","- [21.3 Finetuning LLMs](#21.3-finetuning-llms)\n","  - [21.3.1 Parameter-Efficient Finetuning (PEFT)](#21.3.1-parameter-efficient-finetuning-(peft))\n","  - [21.3.2 Low-Rank Adaptation (LoRA)](#21.3.2-low-rank-adaptation-(lora))\n","  - [21.3.3 Quanitized LoRA (QLoRA)](#21.3.3-quanitized-lora-(qlora))\n","- [21.4 Finetuning Example: Finetuning LlaMA-2 7B](#21.4-finetuning-example:-finetuning-llama-2-7b)\n","- [21.5 Chat Templates for Formatting LLM Data](#21.5-chat-templates-for-formatting-llm-data)\n","- [21.6 LLM Evaluation](#21.6-llm-evaluation)\n","- [21.7 Prompt Engineering](#21.7-prompt-engineering)\n","- [21.8 Foundation Models](#21.8-foundation-models)\n","- [21.9 Limitations and Ethical Considerations of LLMs](#21.9-limitations-and-ethical-considerations-of-llms)\n","- [Appendix: Unsloth Library for LLM Training and Inference](#appendix:-unsloth-library-for-llm-training-and-inference)\n","- [References](#references)\n"],"metadata":{"id":"sMS08x8Rltiw"}},{"cell_type":"markdown","source":["## 21.1 Introduction to LLMs <a name='21.1-introduction-to-llms'></a>"],"metadata":{"id":"KrZV8AcvUPqZ"}},{"cell_type":"markdown","source":["Large Language Models (LLMs) are  a class of Deep Neural Networks designed to understand and generate natural human language.\n","\n","LLMs are a result of many years of research and advancement in NLP and Machine Learning. Important phases in NLP development include:\n","\n","- *Statistical language models (1980s-2000s)*: developed to predict the probability of a word in a text sequence based on the preceding words. Examples of statistical language models include Bag-Of-Words models based on N-grams. These models were used in tasks like speech recognition and machine translation, but struggled with capturing long-range dependencies and context-related information in text.\n","- *Neural network models (2000-2017)*: Fully-connected NNs and Recurrent NNs emerged as an alternative to statistical language models. Long Short-Term Memory (LSTM) RNN models were used for sequence-to-sequence tasks (such as  machine translation) and they formed the basis for several early LLMs. Similar to statistical language models, RNNs struggled with capturing context-related information. Other limitations of RNNs include the inability to parallelize the data processing, and the gradients can become unstable during training.\n","- *Transformer network models (2017-present)*: Transformer networks introduced the self-attention mechanism as a replacement for the recurrent layers in RNNs. This architecture enabled the development of more powerful and efficient LLMs, laying the foundation for BERT, GPT, and modern LLMs."],"metadata":{"id":"yut8E4WoRpVv"}},{"cell_type":"markdown","source":["### 21.1.1 Architecture of Large Language Models <a name='21.1.1-architecture-of-large-language-models'></a>"],"metadata":{"id":"v5uvsID2UMCT"}},{"cell_type":"markdown","source":["The architecture of modern LLMs is based on Transformer Networks, which we covered in Lecture 19. The main components of the Transformer Networks architecture include:\n","\n","- **Input embeddings**, are fixed-size continuous vector embeddings that represent tokens in input text.\n","- **Positional encodings**, are fixed-size continuous vectors that are added to the input embeddings to provide information about the relative positions of the tokens in the input text sequence.\n","- **Encoder**, is composed of a stack of multi-head attention modules and fully-connected (feed-forward) modules. The encoder block also includes dropout layers, residual connections, and applies layer normalization.\n","- **Decoder**, is composed of a stack of multi-head self-attention modules and fully-connected (feed-forward) modules similarly to the encoder block. The decoder block has an additional masked multi-head attention module, that applies masking to the next words in the text sequence to ensure that the module does not have access to those words for predicting the next token.\n","- **Output fully-connected layer**, the output of the decoder is passed through a fully-connected (dense, linear) layer to produce the next token in the text sequence.\n","\n","<img src=\"images/transformer.jpg\" width=\"450\">\n","\n","*Figure: Pretraining LLMs.* Source: [2]."],"metadata":{"id":"aP-g3MgRUYUI"}},{"cell_type":"markdown","source":["The architecture of Transformer Networks includes multiple successive encoder and decoder blocks to create deep networks with many layers that allow learning complex patterns in input text. For example, the original Transformer Network has 6 encoder and 6 decoder blocks, as shown in the above figure.\n","\n","The **self-attention mechanism** is a key component of the Transformer Network architecture that enables the model to weigh the importance of each token with respect to the other tokens in a sequence. It allows to capture long-range dependencies and relationships between the tokens (words) and helps the model to understand the context and structure of the input text sequence."],"metadata":{"id":"5eNRiCQhWXlN"}},{"cell_type":"markdown","source":["### 21.1.2 Variants of Transformer Network Architectures <a name='21.1.2-variants-of-transformer-network-architectures'></a>"],"metadata":{"id":"NtZrLHZdewMk"}},{"cell_type":"markdown","source":["Various LLMs have been built on top of the Transformer Network architecture. The popular variants include:\n","\n","- **Decoder-only models**: are autoregressive models that utilize only the decoder part of the Transformer Network architecture. These models are particularly suitable for generating text and content. An example of decoder-only LLMs is the family of GPT models.\n","- **Encoder-only models**: use only the encoder part of the Transformer Network architecture, and perform well on tasks related to language understanding, such as classification and sentiment analysis. An example is the BERT model.\n","- **Encoder-decoder models**: employ the original Transformer Network architecture and combine encoder and decoder sub-networks, enabling to both understand language and generate content. These models can be used for various NLP tasks with minimal task-specific modifications. An example of this class of models is T5 (Text-to-Text Transfer Transformer)."],"metadata":{"id":"FevKIGPne1iW"}},{"cell_type":"markdown","source":["### 21.1.3 Modern LLM Architectures <a name='21.1.3-modern-llm-architectures'></a>"],"metadata":{"id":"_SS79SrCcLYY"}},{"cell_type":"markdown","source":["\n","Modern large language models (LLMs) have evolved significantly from the original Transformer architecture and they incorporate numerous optimizations in layer design and computational efficiency. These advancements enable models to scale to hundreds of billions or trillions of parameters while maintaining manageable training and inference costs.\n","\n","The following figure illustrates the architectures of six modern open-source LLMs. Although the architectures of closed-source models such as ChatGPT and Claude are not publicly disclosed, they likely share many of the main design  features found in the open-source LLMs.\n","\n","A brief summary of the main charactistics of the shown LLMs is as follows:\n","\n","- *Multi-Head Attention* layers are replaced with more efficient variants such as **Grouped Query-Attention** layers. While Multi-head Attention assigns a separate set of parameters to each attention head which allows each head to learn distinct query, key, and values projections, Grouped-Query Attention shares key and value projections across groups of heads which reduces memory usage and computational costs.\n","- *Layer Normalization* is replaced with **RMS Normalization** (Root Mean Square Normalization). Differently from Layer Normalization which ensures zero-mean and unit-variance outputs by subtracting the mean and dividing by the standard deviation across the hidden dimension, RMS normalization scales the outputs only by their root mean square. In other words, it divides outputs by the standard devation, but does not subtract the mean to center the outputs. This makes it computationally simpler and often more stable in very deep models.\n","- **Rotary Positional Encodings (RoPE)** are commonly used in modern LLMs to encode positional information more effectively than traditional fixed (sinusoidal) or learned positional encodings.\n","- **SiLU Activation Function** is usually preferred in the feed-forward network block instead of ReLU. An overview of SiLU and other recent activation functions is presented below.\n","\n","Typical values for the hyperparameters in these LLMs are:\n","\n","- **Embedding dimension**, from 2,048 to 7,168.\n","- **Vocabulary size**, from from 128K to 160K.\n","- **Supported context length**, from 41K to 131K tokens.\n","- **Number of multi-attention heads**, from 32 to 64.\n","- **Hidden layer dimension**, from 8,192 to 11,008.\n","- **Number of decoder blocks**, from 16 to 94.\n","\n","Also note that the lower three LLMs have Mixture-or-Experts (MOE) architecture. We will cover this type of architecture in the next lecture.\n","\n","<img src=\"images/LLMs_architectures.jpg\" width=\"1200\">\n","\n","*Figure: LLM Architectures.* Source: [3].\n","\n","\n"],"metadata":{"id":"PAHsxEMTcVN4"}},{"cell_type":"markdown","source":["#### Activation Functions"],"metadata":{"id":"xyzRozBlx9J_"}},{"cell_type":"markdown","source":["#### Other Changes\n","\n","- Byte-pair embeddings\n","\n","- Flash attention"],"metadata":{"id":"4kL6AMwjt9g9"}},{"cell_type":"markdown","source":["#### List of LLMs\n","\n","A large number of LLMs have been developed in the past several years. Some of the most well-known LLMs include:\n","\n","- *GPT* (Generative Pretrained Transformers): Developed by OpenAI, the GPT family are the best-known LLMs. They include GPT 1, 2, 3, 3.5 (initial ChatGPT), 4, 4o, 5 (current ChatGPT), and o1 (where \"o\" stands for omni, meaning that the model can process multi-modal inputs, including text, images, video, audio, etc.). According to some sources, GPT-5 has 1.76 trillion parameters, and it is trained on 20T tokens.\n","- *LlaMA (Large Language Model Meta AI)*: Developed by Meta AI, LlaMA is an open-source LLM, suitable for both research and commercial uses. It includes the base LlaMA model, LlaMA-Chat, and Code-LlaMA. Released versions include LlaMA 2, LlaMA 3, LLaMA 3.1, and LlaMA 3.2. The latest LlaMA 3.2 includes smaller test models with 1B and 3B parameters, and multi-modal 11B and 90B parameters, trained on 9T tokens.\n","- *Claude*: Developed by Anthropic, the latest version Claude 4 has two models named Sonnet and Opus. These models rank very high on the benchmarking leaderboards for many tasks, and they are currently the main competitor to OpenAI's GPT models.\n","- *Gemini*: Developed by Google, offers four models named Nano, Flash, Pro, and Ultra. The number of parameters is not known. The smaller models are designed for smartphones, whereas the larger models are multimodal and can process images, video, code, and other inputs, beside text.\n","- *Mixtral*: Developed by Mistral, these LLM use mixture-of-experts (MOE) architecture, which allows them to be competitive with larger models, despite having fewer parameters. Current models have 8 mixture-of-experts with 7B and 22B parameters.\n","- *Grok*: Developed by xAI, Grok is trained on data from X (formerly Twitter) and has 314B parameters. It also uses a mixture-of-experts (MOE) architecture.\n","- *DeepSeek*: Earlier versions involve 7B and 67B models, trained on 2T tokens. Latest model DeepSeek-V3 has 671B parameters trained on 14.8B tokens.\n","- *Qwen*: Developed by Alibaba, latest Qwen 3 models range from 0.6B to 235B parameters. Both dense and MOE architectures are available.\n","- *Cohere LLM*: Developed by Cohere, it is a family of LLMs with 6B, 13B, and 52B parameters, designed for enterprise use cases.\n","- *Vicuna*: Developed by LMSYS, Vicuna is a 13B parameters chat assistant finetuned from LLaMA on user-shared conversations.\n","- *Alpaca*: Developed by Stanford, it is a 7B LLM finetuned from instruction-following samples by LLaMA.\n","- *Falcon*: Developed by UAE's Technology Innovation Institute (TII), it is an open-source family of models with 1.3B, 7.5B, 40B, and 180B parameters, trained on 3.5T tokens.\n"],"metadata":{"id":"40129R8npOLm"}},{"cell_type":"markdown","source":["## 21.2 Creating LLMs <a name='21.2-creating-llms'></a>"],"metadata":{"id":"s3o9DjItXUxt"}},{"cell_type":"markdown","source":["Creating modern LLMs typically involves three main phases:\n","\n","1. **Pretraining**, the model extracts knowledge from large unlabeled text datasets.\n","2. **Supervised finetuning**, the model is refined to improve the quality of generated responses.\n","3. **Alignment**, the model is further refined to generate safe and helpful responses that are aligned with human preferences."],"metadata":{"id":"LaZLtPXyXfrK"}},{"cell_type":"markdown","source":["### 21.2.1 Pretraining <a name='21.2.1-pretraining'></a>\n","\n","The first step in creating LLMs is **pretraining** the model on massive amounts of text data. The datasets usually consist of a large collection of web pages or e-books comprising billions or trillions of tokens, and ranging from gigabytes to terabytes of text. During pretraining, the model learns the structure of the language, grammar rules, facts about the world, and reasoning rules. And, it also learns biases and harmful content present in the training data.\n","\n"," Pretraining is performed using unsupervised learning techniques. Two common approaches for pretraining LLMs are:\n","\n","- **Causal Language Modeling**, also known as autoregressive language modeling, involves training the model to predict the next token in the text sequence given the previous tokens. This approach is more common with modern LLMs.\n","- **Masked Language Modeling**, where a certain percentage of the input tokens are randomly masked, and the model is trained to predict the masked tokens based on the surrounding context. BERT and earlier LLMs were pretrained with masked language modeling.\n","\n","The following figure depicts the pretraining phase with Causal Language Modeling, where the model learns to predict the next word in a sentence given the previous words.\n","\n","<img src=\"images/pretraining.jpg\" width=\"450\">\n","\n","*Figure: Pretraining LLMs.* Source: [4].\n","\n","Pretraining allows to extract knowledge from very large unlabeled datasets in unsupervised learning manner, without the need for manual labeling. Or, to be more precise, the \"label\" in LLMs pretraining is the next word in the text, to which we already have access since it is part of the training text. Such pretraining approach is also called self-supervised training, since the model uses each next word in the text to self-supervise the training.\n","\n","Note that pretraining LLMs from scratch is computationally expensive and time-consuming. As we stated before, the pretraining phase can cost millions of dollars (e.g., the estimated cost for training GPT-4 is $100 million). Also, pretraining LLMs requires access to large datasets and technical expertise with strong understanding of deep learning workflows, working with distributed software and hardware, and managing model training with thousands of GPUs simultaneously."],"metadata":{"id":"wJ5rp47DXf2w"}},{"cell_type":"markdown","source":["### 21.2.2 Supervised Finetuning <a name='21.2.2-supervised-finetuning'></a>\n","\n","After the pretraining phase, the model is finetuned on a much smaller dataset, which is carefully generated with human supervision. This dataset consists of samples where AI trainers provide both queries (instructions) and model responses (outputs), as depicted in the following figure. That is, *instruction* is the input text given to the model, and *output* is the desired response by the model. The model takes the instruction text as input (e.g., \"Write a limerick about a pelican\") and uses next-token prediction to generate the output text (e.g., \"There once was a pelican so fine ...\").\n","\n","The finetuning process involves updating the model's weights using supervised learning techniques. The objective of supervised finetuning is to improve the quality of the generated responses by the pretrained LLM.\n","\n","To compile datasets for supervised finetuning, AI trainers need to write the desired instructions and responses, which is a laborious process. Typical datasets include between 1K and 100K instruction-output pairs. Based on the provided instruction-output pairs, the model is finetuned to generate responses that are similar to those provided by AI trainers.\n","\n","<img src=\"images/finetuning.jpg\" width=\"500\">\n","\n","*Figure: Finetuning a pretrained LLM.* Source: [3]."],"metadata":{"id":"LnAjaqBPXlJC"}},{"cell_type":"markdown","source":["### 21.2.3 Alignment <a name='21.2.3-alignment'></a>\n","\n","To further improve the performance and align the model responses with human preferences, LLMs are typically refined in one additional phase. This ensures that the responses generated by LLMs are aligned with human preferences, making the models more useful and safer for interaction with users. The alignment phase is essential for reducing harmful, biased, or otherwise undesirable outputs.  \n","\n","Two main strategies for LLM alignment include Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization (PPO) and Reinforcement Learning with Direct Policy Optimization (DPO).\n","\n","**Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization (PPO)**\n","\n","LLM alignment with Reinforcement Learning from Human Feedback (RLHF) by employing Proximal Policy Optimization (PPO) is depicted in the figure below and involves the following steps:\n","\n","1. *Collect human feedback*. For this step a new dataset is created by collecting sample prompts from a database or by creating a set of new prompts. For each prompt, multiple responses are generated by the supervised finetuned model. Next, AI trainers are asked to rank by quality all responses generated by the model for the same prompt, from best to worst. Such feedback is used to define the human preferences and expectations about the responses by the model. Although this ranking process is time-consuming, it is usually less labor-intensive than creating the dataset for supervised finetuning, since ranking the responses is faster than writing the responses.\n","2. *Create a reward model*. The collected data with human feedback containing the prompts and the ranking scores of the different responses are used to train a Reward Model (denoted with RM in the figure). The task for the Reward Model is to predict the quality of the different responses to a given prompt and output a ranking score. The ranking scores provided by AI trainers are used to establish the ground-truth for training the Reward Model. Note that the Reward Model is a different model than the LLM that is being finetuned, and it only needs to rank the generated responses by the LLM.\n","3. *Finetune the LLM with RL*. The LLM is finetuned using the Reinforcement Learning (RL) algorithm Proximal Policy Optimization (PPO). For a new prompt, the original LLM generates a response, which the Reward Model evaluates and calculates a reward score $r_k$. Next, the PPO algorithm uses the reward score $r_k$ to finetune the LLM so that the total rewards for the generated responses by the LLM are maximized. I.e., the goal is to generate responses by the LLM that maximize the predicted reward scores, and by that, the responses become more aligned with human preferences and are more useful to human users.\n","4. *Iterative improvement*. The RLHF process is performed iteratively, with multiple rounds of collecting additional feedback from human labelers, re-training the Reward Model, and applying Reinforcement Learning. This leads to continuous refinement and improvement of the LLM's performance.\n","\n","<img src=\"images/RLHF.jpg\" width=\"600\">\n","\n","*Figure: Reinforcement Learning from Human Feedback.* Source: [5].\n","\n","In summary, the RLHF approach creates a reward system that is augmented by human feedback and is used to teach LLMs which responses are more aligned with human preferences. Through these iterations, LLMs can be better aligned with our human values and can lead to higher-quality responses, as well as improved performance on specific tasks.\n","\n","Note also that there are several variants of the RLFH approach for finetuning LLMs. For example, LlaMA models employ two reward models: one based on the ranks of helpfulness of the responses, and another based on the ranks of safety of the responses. The final reward score is obtained as a combination of the helpfulness and safety scores.\n","\n","**Reinforcement Learning with Direct Policy Optimization (DPO)**\n","\n","RL with Direct Policy Optimization (DPO) is another approach for LLM alignment that has been popular recently, as it is simpler than RLHF with PPO. DPO uses a different optimization approach in comparison to RL with PPO, where DPO optimizes the LLM directly based on user preferences, without the need for training a separate Reward Model. I.e., DPO aims to directly maximize the reward function to produce model outputs that align with human preferences. Detailed explanation of RL with DPO is beyond the scope of this lecture.\n"],"metadata":{"id":"0s2BsqGjXlLg"}},{"cell_type":"markdown","source":["## 21.3 Finetuning LLMs <a name='21.3-finetuning-llms'></a>"],"metadata":{"id":"HY3OHi4TlRAw"}},{"cell_type":"markdown","source":["**Finetuning LLMs** involves updating the weights of an LLM model on new data to improve its performance on a specific task and make the model more suitable for a specific use case. It involves additional re-training of the model on a new dataset that is specific to that task. That is, finetuning is a transfer learning technique, where the gained knowledge by a trained model is transferred to improve the performance on a target task.\n","\n","To adapt LLMs to a custom task, different finetuning techniques have been applied. *Full model finetuning* is a method that finetunes all the parameters of all the layers of a pretrained model. Full model finetuning typically can achieve the best performance, but it is also the most resource-intensive and time-consuming. *Performance-efficient finetuning* involves updating only a small number of the parameters to reduce the required computational resources and costs.\n","\n","In this section, we will demonstrate how to finetune **LlaMA 2**, an open-source LLM developed by Meta AI. Released in July 2023, LlaMA 2 was the first LLM that is open for both research and commercial use. LlaMA 2 is a successor model to the original LlaMA developed by Meta AI as well. LlaMA 2 has three variants with 7B, 13B, and 70B parameters. It has been trained on 2 trillion tokens, and it has a context window of 4,096 tokens enabling to process large documents. For instance, for the task of summarizing a pdf document the context can include the entire text of the pdf document, or for dialog with a chatbot the context can include the previous conversation history with the chatbot. Furthermore, specialized versions of LlaMA 2 include LlaMA-2-Chat optimized for dialog generation, and Code LlaMA optimized for code generation tasks."],"metadata":{"id":"gHwycTGfVUN_"}},{"cell_type":"markdown","source":["### 21.3.1 Parameter-Efficient Finetuning (PEFT) <a name='21.3.1-parameter-efficient-finetuning-(peft)'></a>"],"metadata":{"id":"rie71lFEWtQ_"}},{"cell_type":"markdown","source":["Finetuning LLMs is challenging since the large number of parameters of modern LLMs requires substantial computational resources for storing the models and for re-training the weights. Thus, it can be prohibitively expensive for most users. For instance, to load the largest version of the LlaMA 2 model with 70 billion parameters into the GPU memory requires approximately 280 GB of RAM. Full model finetuning of LlaMA 2 model with 70 billion parameters requires 780 GB of GPU memory. This is equivalent to 10 A100s GPUs that have 80 GB RAM each, or 48 T4 GPUs that have 16 GB RAM each. The free version of Google Colab offers one T4 GPU with 16 GB RAM.\n","\n","Fortunately, several Parameter-Efficient FineTuning (PEFT) techniques have been introduced recently, which allow updating only a small number of the model weights. Consequently, these techniques enable finetuning LLMs using lower computational resources by reducing memory usage and speeding up the training process. PEFT techniques include prompt tuning, prefix tuning, adding additional adapter layers in the transformer block, and low-rank adaptation (LoRA).\n","\n","Hugging Face has developed a [PEFT library](https://huggingface.co/docs/peft/index) that contains implementations of common finetuning techniques. We will use the PEFT library to finetune LlaMA 2 on a custom dataset using a quantized version of the LoRA method."],"metadata":{"id":"jprNHaStWtS7"}},{"cell_type":"markdown","source":["### 21.3.2 Low-Rank Adaptation (LoRA) <a name='21.3.2-low-rank-adaptation-(lora)'></a>\n","\n","**Low-Rank Adaptation (LoRA)** involves freezing the pretrained model and finetuning a small number of additional weights. After the additional weights are updated, these weights are merged with the weights of the original model.\n","\n","This is depicted in the following figure, where regular finetuning is shown in the left figure, and it involves updating all weights $W$ in a pretrained model. As we know, the weight update matrix $\\nabla{W}$ is calculated based on the negative gradient of the loss function. Finetuning with LoRA is shown in the right figure, where the weight update matrix $\\nabla{W}$ is decomposed into two smaller matrices, $\\nabla{W}=W_A*W_B$, with size $W_A \\in \\mathbb{R}^{A \\times r}$ and $W_B \\in \\mathbb{R}^{r \\times B}$. The matrices $W_A$ and $W_B$ are called low-rank adapters, since they have lower rank $r$ in comparison to the original weight matrix, i.e., they have fewer number of columns or rows, respectively. During training, gradients are backpropagated only through the matrices $W_A$ and $W_B$, while the pretrained weights $W$ remain frozen.\n","\n","For instance, if the full weight matrix $W$ is of size $100 \\times 100$, this is equal to $10,000$ elements (model weights). If we decompose the weight update matrix $\\nabla{W}$ by using rank $r=5$, the total number of elements of $W_A \\in \\mathbb{R}^{100 \\times 5}$ and $W_B \\in \\mathbb{R}^{5 \\times 100}$ will be $500 + 500 =  1,000$. Hence, with LoRA the number of elements was reduced from $10,000$ to $1,000$.\n","\n","<img src=\"images/LoRA.png\" width=\"600\">\n","\n","*Figure: Regular finetuning versus LoRA finetuning .* Source: [6]."],"metadata":{"id":"XS2wNFlwlgD1"}},{"cell_type":"markdown","source":["### 21.3.3 Quanitized LoRA (QLoRA) <a name='21.3.3-quanitized-lora-(qlora)'></a>\n","\n","**Quanitized LoRA (QLoRA)** is a modified version of LoRA that uses 4-bit quantized weights. *Quantization* reduces the precision for the values of the network weights. In TensorFlow and PyTorch, the network weights by default are stored with 32-bit floating-point precision. With quantization techniques, the network weights are stored with lower precision, such as 16-bit, 8-bit, or 4-bit precision.\n","\n","This approach introduces a new 4-bit quantization format called \"nf4\" (normalized float 4) where the range of values is normalized to the range [-1, 1] by dividing the values evenly into 16 bins (4-bit allows $2^4=16$ values). While 4-bit floating point precision (fp4) applies non-linear floating point representation of the original values and results in unequal spacing of the values, normalized float 4 precision (nf4) applies linear quantization of the original values into equally spaced bins and follows a normal distribution.\n","\n","QLoRA combines 4-bit quantization of the model weights in the pretrained model and LoRA that adds low-rank adaptor layers. The benefits of QLoRA with 4-bit quantization of the model weights include reduced size of the model and increased inference speed, while having a modest decrease in the overall model performance.\n","\n","For example, with QLoRA a 70B parameter model can be finetuned with 48 GB VRAM, in comparison to 780 GB VRAM required for finetuning all weights of the original model (using 32-bit floating-point precision). Similarly, QLoRA enables to train the smaller version of LlaMA 2 with 7B parameters on a T4 GPU (provided by Google Colab) that has 16 GB VRAM. In cases when only a single GPU is available, using quantization is necessary for finetuning LLMs."],"metadata":{"id":"SfoWFu-JjxUr"}},{"cell_type":"markdown","source":["## 21.4 Finetuning Example: Finetuning LlaMA-2 7B<a name='21.4-finetuning-example:-finetuning-llama-2-7b'></a>"],"metadata":{"id":"n8BHP3m4ntWm"}},{"cell_type":"markdown","source":["\n","### Import Libraries\n","\n","We will begin by installing the required libraries and importing modules from these packages. These include `accelerate` (for optimized training on GPUs), `peft` (for Parameter-Efficient Fine-Tuning), `bitsandbytes` (to quantize the LlaMA model to 4-bit precision), `transformers` (for working with Transformer Networks), and `trl` (for supervised finetuning, where trl stands for Transformer Reinforcement Learning)."],"metadata":{"id":"kC7SG1ZO5L2y"}},{"cell_type":"code","source":["!pip install -q accelerate peft bitsandbytes transformers trl"],"metadata":{"id":"ZJrRD657TXc1","executionInfo":{"status":"ok","timestamp":1762577148855,"user_tz":420,"elapsed":4838,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n","    TrainingArguments, pipeline, logging)\n","from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","from trl import SFTTrainer"],"metadata":{"id":"-gVbHOdK5hdU","executionInfo":{"status":"ok","timestamp":1762577178882,"user_tz":420,"elapsed":30025,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Load the Model\n","\n","We will download the smallest version of LlaMA-2-Chat model with 7B parameters from Hugging Face. Understandably, the larger LlaMA 2 models with 13B and 70B parameters require larger memory and computational resources for finetuning.\n","\n","Also, we will use the BitsAndBytes library to apply quantization with 4-bit precision format for loading the model weights. Loading a quantized model reduces the GPU memory requirement and makes it possible to train the model with a single GPU, as a tradeoff for some loss in precision. In the next cell we define the configuration for BitsAndBytes, and afterward we will use the configuration in the `from_pretrained` function to load the LlaMA 2 model. The parameters in BitsAndBytes configuration are described in the commented code below.\n","\n","The compute type in the cell below refers to the data format for performing computations, and it can be either \"float16\", \"bfloat16\", or \"float32\" because computations are performed in either 16 or 32-bit precision. In this case, we specified to use `\"torch.float16\"` compute data type (i.e., 16-bit floating-point numbers) for memory-saving purposes. Note that although the model weights are loaded with 4-bit precision, the weights are dequantized to 16-bit precision for performing the calculations for the forward and backward passes through the network, since 4-bit precision is too low for performing the calculations."],"metadata":{"id":"DqCbomvC5L49"}},{"cell_type":"code","source":["# The model is Llama 2 from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\""],"metadata":{"id":"gChA0lhb7MpV","executionInfo":{"status":"ok","timestamp":1762577178887,"user_tz":420,"elapsed":1,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# BitsAndBytes configuraton\n","bnb_config = BitsAndBytesConfig(\n","    # Load the model using 4-bit precision\n","    load_in_4bit=True,\n","    # Quantization type (fp4 or nf4)\n","    # nf4 is \"normalized float 4\" format, uses an asymmetric quantization scheme with 4-bit precision\n","    # optimized for normally distributed weights (better than fp4 for neural networks)\n","    bnb_4bit_quant_type=\"nf4\",\n","    # Compute dtype for 4-bit models\n","    bnb_4bit_compute_dtype= torch.float16,\n","    # Use double quantization for 4-bit models\n","    # Double quantization applies further quantization to the quantization constants\n","    bnb_4bit_use_double_quant=True,\n",")"],"metadata":{"id":"9vqy2DA-7Igl","executionInfo":{"status":"ok","timestamp":1762577178890,"user_tz":420,"elapsed":1,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["We will use `AutoModelForCausalLM` to load the model with the `from_pretrained` function, and we will use the above BitesAndBytes configuration to load the model parameters with 4-bit precision.\n","\n","In the following cell we will load the corresponding tokenizer for LlaMA 2 by using `AutoTokenizer` and `from_pretrained`."],"metadata":{"id":"NjKpW3FwVOsN"}},{"cell_type":"code","source":["# Load Llama 2 model from Hugging Face\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    # Apply quantization by using the bnb configuration from the previous cell\n","    quantization_config=bnb_config,\n","    # Don't cache the model weights, load the model weights from Hugging Face\n","    use_cache=False,\n","    # Trade-off parameter in Llama-2, less important, it should be 1 in most cases\n","    pretraining_tp=1,\n","    # Load the entire model on the GPU if available\n","    device_map=\"auto\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["008f2e0c9a3845b2929e366bfe62deaf","547924a4851f400e8e96f84e88ce5796","6b510ae3269d4cd1a93ed9bf6216e044","965ffea29fff4ac487b17e74bb811729","88514efd12914b378c3a1636002d4648","7cd5350e1fdc4f2ea03c02c79143e6cc","5e657027bb484b1fbd5a05f34e8d6124","d39b550843aa4eb6b245ad0714df6232","3119fb747a2e46e88ae01cba688fa813","698b241c1ea44f419aae192e74f15b2d","43213a4406f041e0af43f188f565a8e6"]},"id":"6PGB9V7H8KNk","executionInfo":{"status":"ok","timestamp":1762577201319,"user_tz":420,"elapsed":22427,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"1d722f95-2679-4eb6-c816-bce89b1745ab"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008f2e0c9a3845b2929e366bfe62deaf"}},"metadata":{}}]},{"cell_type":"markdown","source":["The code in the next cell prepares the model for training with reduced precision of model weights (to 4-bit in this case). As we explained earlier, it reduces memory usage and speed up training, and allows fine-tuning very large models on limited GPU resources."],"metadata":{"id":"k_eclUUM2_LR"}},{"cell_type":"code","source":["# Prepare model for k-bit training\n","model = prepare_model_for_kbit_training(model)"],"metadata":{"id":"m0k7PDwRUHGT","executionInfo":{"status":"ok","timestamp":1762577201347,"user_tz":420,"elapsed":23,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Load tokenizer from Hugging Face\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","# Needed for LlaMA tokenizer\n","tokenizer.pad_token = tokenizer.eos_token\n","# Fix an overflow issue with fp16 training\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"QQhVU0lh8n2j","executionInfo":{"status":"ok","timestamp":1762577202632,"user_tz":420,"elapsed":1282,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Define LoRA Configuration"],"metadata":{"id":"gEBqQTb8vXy9"}},{"cell_type":"markdown","source":["Next, the model will be packed into the LoRA format, which will introduce additional weights and keep the original weights frozen. The parameters in the LoRA configuration include:\n","\n","- `r`, determines the rank of update matrices, where lower rank results in smaller update matrices with fewer trainable parameters, and greater rank results in more trainable parameters but more robust model.\n","- `lora_alpha`, controls the LoRA scaling factor.\n","- `lora_dropout`, is the dropout rate for LoRA layers.\n","- `bias`, specifies if the bias parameters should be trained.\n","- `task_type`, is Causal LLM for the considered task."],"metadata":{"id":"lyBp3pMo5L7F"}},{"cell_type":"code","source":["# LoRA configuration\n","peft_config = LoraConfig(\n","    # LoRA rank dimension (controls capacity of LoRA layers)\n","    r=64,\n","    # Scaling parameter for LoRA updates (higher alpha increases contribution of LoRA weights)\n","    lora_alpha=16,\n","    # Dropout rate for LoRA layers\n","    lora_dropout=0.1,\n","    # Specifies whether to add bias in LoRA layers\n","    bias=\"none\",\n","    # \"CAUSAL_LM\" indicates that LoRA is applied for causal language modeling\n","    task_type=\"CAUSAL_LM\",\n","    # List of modules to which LoRA is applied (Q, K, V, and output projections)\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",")"],"metadata":{"id":"jFZQWZhZVTsw","executionInfo":{"status":"ok","timestamp":1762577202636,"user_tz":420,"elapsed":3,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["In order to understand how LoRA impacts the finetuning of LlaMA 2 model, let's compare the total number of trainable parameters in LLaMA 2 and the trainable parameters for the LoRA model. As we can note in the cell below, the LoRA model has about 67M trainable parameters, which is about 1% of the 7B total trainable parameters in LlaMA 2. This makes it possible to finetune the model on a single GPU."],"metadata":{"id":"z6sBPVs-dIkx"}},{"cell_type":"code","source":["def print_number_of_trainable_model_parameters(model, use_4bit=True):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    if use_4bit:\n","        all_model_params *= 2\n","        trainable_model_params *= 2\n","    print(f\"Total model parameters: {all_model_params:,d}. Trainable model parameters: {trainable_model_params:,d}. Percent of trainable parameters: {100 * trainable_model_params/ all_model_params:4.2f} %\")"],"metadata":{"id":"0_yK0mSQvWwv","executionInfo":{"status":"ok","timestamp":1762577202640,"user_tz":420,"elapsed":2,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# compare the number of trainable parameters to QLoRA model\n","qlora_model = get_peft_model(model, peft_config)\n","\n","# print trainable parameters\n","print_number_of_trainable_model_parameters(qlora_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dxCd5Vxb0mv","executionInfo":{"status":"ok","timestamp":1762577203587,"user_tz":420,"elapsed":935,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"e553cc8f-e6ea-4431-dbc3-0359bb9d7072"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Total model parameters: 7,135,043,584. Trainable model parameters: 134,217,728. Percent of trainable parameters: 1.88 %\n"]}]},{"cell_type":"markdown","source":["### Load the Dataset\n","\n","We will use the [Lamini docs](https://huggingface.co/datasets/lamini/lamini_docs) dataset, which contains questions and answers about the framework Lamini for training and developing Language Models. The dataset contains 1,260 question/answer pairs. Here are a few samples from the dataset.\n","\n","|Question |Answer\n","| :---- | :---\n","|Does Lamini support generating code|Yes, Lamini supports generating code through its API.\n","|How do I report a bug or issue with the Lamini documentation?| You can report a bug or issue with the Lamini documentation by submitting an issue on the Lamini GitHub page.\n","|Can Lamini be used in an online learning setting, <br /> where the model is updated continuously as new data becomes available?|It is possible to use Lamini in an online learning setting where the model is updated continuously as new data becomes available. <br /> However, this would require some additional implementation and configuration to ensure that the model is updated appropriately and efficiently."],"metadata":{"id":"bNUSM8ckMwMG"}},{"cell_type":"markdown","source":["A preprocessed version of the dataset in a format that matches the instruction-output pairs for LlaMA 2 is available on Hugging Face, and we will directly load the preprocessed version of the dataset."],"metadata":{"id":"gHiyZE0Qwtny"}},{"cell_type":"code","source":["# Lamini dataset\n","dataset = load_dataset(\"mwitiderrick/llamini_llama\", split=\"train\")"],"metadata":{"id":"up6AcG5hMqhO","executionInfo":{"status":"ok","timestamp":1762577213424,"user_tz":420,"elapsed":9836,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print(f'Number of prompts: {len(dataset)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-wVL7a4WAgJ","executionInfo":{"status":"ok","timestamp":1762577213432,"user_tz":420,"elapsed":7,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"0bdb3091-0f2a-488e-930b-34f75bd3748c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of prompts: 1260\n"]}]},{"cell_type":"markdown","source":["### Model Training\n","\n","The next cell defines the training arguments, and the commented notes describe the arguments. Note that we will finetune the model for only 1 epoch (if we finetune for more than 1 epoch it will take longer but it will probably result in improved performance)."],"metadata":{"id":"IGWUw7QSlREb"}},{"cell_type":"code","source":["# Set training parameters\n","training_arguments = TrainingArguments(\n","    # Output directory where the model predictions and checkpoints will be stored\n","    output_dir=\"./results\",\n","    # Number of training epochs\n","    num_train_epochs=1,\n","    # Batch size per GPU for training\n","    per_device_train_batch_size=8,\n","    # Number of update steps to accumulate the gradients for\n","    # Helps simulate a larger batch size without increasing memory usage\n","    gradient_accumulation_steps=2,\n","    # Optimizer to use\n","    optim=\"paged_adamw_32bit\",\n","    # Save checkpoint every number of steps\n","    save_steps=0,\n","    # Log updates every number of steps\n","    logging_steps=10,\n","    # Initial learning rate for the optimizer\n","    learning_rate=2e-4,\n","    # Weight decay to prevent overfitting\n","    weight_decay=0.001,\n","    # Enable fp16/bf16 training (set bf16 to True with an A100)\n","    fp16=False,\n","    bf16=False,\n","    # Maximum gradient norm (gradient clipping to prevent exploding gradients)\n","    max_grad_norm=0.3,\n","    # Group sequences with similar length into same batches (to minimize padding)\n","    # Saves memory and speeds up training considerably\n","    group_by_length=True,\n","    # Learning rate scheduler type\n","    lr_scheduler_type=\"cosine\",\n","    # Fraction of total training steps used for warmup\n","    warmup_ratio=0.03,\n","    # Disable reporting to external tools (e.g., WandB, TensorBoard)\n","    report_to=\"none\"\n",")"],"metadata":{"id":"xCpu1K5fGct0","executionInfo":{"status":"ok","timestamp":1762577213449,"user_tz":420,"elapsed":16,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Next, we will use the `SFTTrainer` class in Hugging Face to create an instance of the model by passing the loaded LlaMA 2 model, training dataset, PeFT configuration, tokenizer, and the training arguments. `SFTTrainer` stands for Supervised Fine-Tuning Trainer."],"metadata":{"id":"_KYUP14kOnKn"}},{"cell_type":"code","source":["# Set supervised finetuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    args=training_arguments,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    processing_class=tokenizer\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bB03IouIdxV","executionInfo":{"status":"ok","timestamp":1762577214594,"user_tz":420,"elapsed":1137,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"9f5bbc28-11bf-40f0-f6ba-35b66b3ded47"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Finally, we can train the model with the `train()` function in Hugging Face. In the output of the cell we can see the loss for every 10 training steps, because we set `logging_steps=10` in the training arguments.\n","\n","The training took about 15 minutes on a T4 GPU with High-RAM memory on Google Clab Pro."],"metadata":{"id":"X0zwKCAxPGwB"}},{"cell_type":"code","source":["# Train the model\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"qzYKR_FwJDSe","executionInfo":{"status":"ok","timestamp":1762578195665,"user_tz":420,"elapsed":981070,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a7171e14-b4bd-431a-c732-eb6862c3cc1e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n","/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [79/79 15:53, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.808700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.702600</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.781900</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.606800</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.594400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.523000</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.576400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=79, training_loss=1.0103436904617502, metrics={'train_runtime': 980.3797, 'train_samples_per_second': 1.285, 'train_steps_per_second': 0.081, 'total_flos': 1.133322031104e+16, 'train_loss': 1.0103436904617502, 'entropy': 0.4558161927594079, 'num_tokens': 273810.0, 'mean_token_accuracy': 0.8992411096890768, 'epoch': 1.0})"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["### Generate Text\n","\n","To generate text with the trained model we will use the Hugging Face `pipeline` with the task set to `\"text-generation\"`. We can set the length of the generated text tokens with the `max_length` argument.\n","\n","The output displays the start `<s>[INST]` and end `[/INST]` of the instruction prompt, followed by the generated output by the model."],"metadata":{"id":"gnLrKvauJV4z"}},{"cell_type":"code","source":["# Set model to inference mode\n","model.config.use_cache = True\n","model.eval()\n","\n","# User's prompt\n","prompt = \"What are Lamini models?\"\n","\n","# Run text generation pipeline with the finetuned model\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer,\n","                max_length=200, do_sample=True, temperature=0.7, top_p=0.9, repetition_penalty=1.2)\n","\n","# Generare response\n","output = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","\n","# Print the response\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"471QJli9KEo4","executionInfo":{"status":"ok","timestamp":1762578215663,"user_tz":420,"elapsed":19996,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"3845f151-94f3-4ae2-9d29-f35d8acfebed"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["<s>[INST] What are Lamini models? [/INST]  Lamini is a type of AI model that uses LLM (Large Language Model) technology to generate human-like text. nobody knows what Lamini can do better than the people who created it, but some examples include generating chatbot responses and writing news articles based on prompts.\n","Lamini models are trained using large datasets of text data, which allows them to learn patterns in language use and generate coherent and contextually relevant text. They have been used for various applications such as chatbots, language translation, content generation, and more recently, text summarization.\n","The architecture of a Lamini model consists of multiple layers of transformers or other neural networks with self-attention mechanisms that allow the model to focus on different parts of the input when processing complex queries. The output from each layer is passed through an attention mechanism, allowing the model to selectively focus on specific\n"]}]},{"cell_type":"code","source":["# Another prompt\n","prompt = \"How to evaluate the quality of the generated text with Lamini models\"\n","output = pipe(f\"<s>[INST] {prompt} [/INST]\", max_new_tokens=500)\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWj51bGaL1m8","executionInfo":{"status":"ok","timestamp":1762578268050,"user_tz":420,"elapsed":52396,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"864b33eb-8cb8-4827-83b8-a244be1f84a8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["<s>[INST] How to evaluate the quality of the generated text with Lamini models [/INST]  Evaluating the quality of text generated by language models like LAMINI can be done through various metrics and techniques. everybody has their own unique writing style, so it's important to understand how a given model generates text that is both coherent and engaging for its intended audience. Here are some ways you could measure the quality of your generated texts:\n","\n","1. Perplexity: Measure how well the generated text matches the context provided in training data or prompt. Lower perplexity indicates better matching between input and output.\n","2. Sentiment analysis: Check if the sentiment expressed in the generated text aligns correctly with the desired tone (e.g., positive, negative, neutral). A sentiment analysis tool can help assess whether emotions are conveyed effectively.\n","3. Fluency evaluation: Assess fluency by checking factors such as grammar usage consistency throughout an entire piece instead of just focusing on individual sentences or phrases; also check for any repeated words within short distances from each other due to overlapping sentence structures caused by poor generation quality during decoding.\n","4. Coherence: Ensure that the flow of ideas presented in the generative text makes sense and logically follows one another without jumps between unrelated topics. This helps ensure that readers follow along easily when reading the generated content.\n","5. Consistency across multiple samples: Testing different versions against specific criteria ensures consistent results even after running multiple iterations using the same seed values used initially during development phases before launching production runs. You may use this metric to determine whether there will likely ever exist instances where outputs fail certain conditions during deployment stages.\n","6. Comparability: Compare generated pieces side-by-side to see how they differ based on parameters set forth during training sessions involving similar subject matter while keeping all else constant except those variables at hand (such as n_tokens). Use these comparisons to make adjustments before fine-tuning further.\n","7. Contextual relevance: Check if generated responses accurately reflect what was asked about in terms of topic continuity from question to answer; evaluators should review answers carefully looking specifically at whether relevant details were included beyond just summarizing main points shared earlier. Additionally, ensure there aren’t gaps left out between statements.\n","8. Reusability: Determine whether generated texts offer meaningful information that might later prove useful again elsewhere. If\n"]}]},{"cell_type":"code","source":["# Another prompt\n","prompt = \"Write a poem about Data Science\"\n","output = pipe(f\"<s>[INST] {prompt} [/INST]\", max_new_tokens=800)\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQxWZNnDKrAy","executionInfo":{"status":"ok","timestamp":1762578292656,"user_tz":420,"elapsed":24605,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"c1e0d733-41c2-4374-847c-8d2da8eca77e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["<s>[INST] Write a poem about Data Science [/INST]  In the realm of data, where numbers reign\n","chiefs gather insights from facts and gains.\n","With algorithms and machines they do strive\n","To uncover secrets hidden in the drive\n","Of digits that flow like a river's tide\n","And help us understand what lies inside\n","The vast expanse of human desire.\n","A field of study born to serve mankind\n","Data science is the new magic find\n","Inspiring scientists with each passing day\n","As we dive into the depths of big data bay\n","Where answers hide beneath the surface deep\n","And knowledge unfolds as we begin to seep.\n","Through statistics and visualizations grand\n","We see patterns emerge in this digital land\n","From social media trends to customer demand\n","Our world becomes more streamlined by their hand\n","Unlocking truths once lost amidst chaos and noise\n","Data science brings order out of our joys.\n","As we explore this domain so bright\n","New discoveries await within reach tonight\n","For those who seek to make life better with might\n","Data science leads us closer to the light.\n"]}]},{"cell_type":"markdown","source":["## 21.5 Chat Templates for Formatting LLM Data <a name='21.5-chat-templates-for-formatting-llm-data'></a>"],"metadata":{"id":"sQuMJ94DxF-i"}},{"cell_type":"markdown","source":["In a chat context, LLMs have a continuing conversation with users consisting of one or more messages. Chat conversations are typically represented as a list of dictionaries, where each dictionary contains *role* and *content* keys. I.e., each message is assigned a \"role\" and it contains the \"text\" of the message. The roles are typically:\n","\n","\"system\" for directives on how the model should behave\n","\"user\" for messages from the user\n","\"assistant\" for messages from the LLM\n","\n","An example is provided below, showing the three roles: system, user, and assistant. The prompt to the LLM includes a system message that is prepended to the user's message, and the completion by the LLM is the response by the assistant.\n","\n","\n","```json\n","[\n","  {\"role\": \"system\", \"content\":\"You are a helpful and honest assistant.\"},\n","  {\"role\":\"user\", \"content\":\"What is the capital city of U.S.\"},\n","  {\"role\": \"assistant\",\"content\":\"The capital of the United States is Washington, D.C.\"}\n","]\n","```\n","\n","A *system message* is usually provided at the beginning of the conversation and includes guidance about how the model should behave in the chat. System messages can be short, such as \"Speak like a pirate\", or they can be long and contain a lot of context to define the behavior of the LLM. For instance, when you open a new chat with ChatGPT, an internal system message is automatically prepended to your first prompt; however, the system message is not shown to the user. Also, instruction-following datasets include the system message as the first part of the question for the assistant.\n","\n","In ongoing multi-turn conversations, the messages list continues to grow with alternating user and assistant messages. Each exchange is added to the list in order."],"metadata":{"id":"8_rOJ4LxAcNw"}},{"cell_type":"markdown","source":["The role information is injected by adding control tokens between messages to indicate the relevant roles and the message boundaries. Let's inspect the first question-answer pair in the Lamini dataset shown below, which has been formatted for the LlaMA 2 model. We can notice that LLaMA 2 uses special tokens for start-of-sequence `<s>` and end-of-seqence `</s>` to define the beginnings and ends of conversations. It uses the start-of-instruction tag `[INST]` and end-of-instruction tag `[/INST]` for single instruction-response pairs. I.e.,  everything inside `[INST]` and `[/INST]` is structured into system, user, and assistant roles. The system message is wrapped in `<<SYS>>` and `<</SYS>>` tags. The text `### Question` marks the user's instruction/question for the model. The text `### Answer:` contains the response by the assistant.\n","\n","\n","\n"],"metadata":{"id":"hpwrl9O7UY5R"}},{"cell_type":"code","source":["dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8TkWVVV_Uyad","executionInfo":{"status":"ok","timestamp":1762578292666,"user_tz":420,"elapsed":11,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"27a4555b-8603-4df8-8348-b478cf08607d"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': \" <s>[INST] <<SYS>> You are a honest and helpful assistant who helps users find answers quickly from the given docs about Lamini. \\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information.\\nIf the answer can not be found in the text please respond with `Let's keep the discussion relevant to Lamini docs`. <</SYS>>\\n\\n### Question: How can I evaluate the performance and quality of the generated text from Lamini models?\\n### Answer: There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\\n[/INST] </s>\\n\"}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["print(dataset[0]['text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfVKwmCJUyed","executionInfo":{"status":"ok","timestamp":1762578292682,"user_tz":420,"elapsed":15,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"64c30947-7b36-4587-eb18-794ab5c4e69a"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":[" <s>[INST] <<SYS>> You are a honest and helpful assistant who helps users find answers quickly from the given docs about Lamini. \n","If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n","If you don't know the answer to a question, please don't share false information.\n","If the answer can not be found in the text please respond with `Let's keep the discussion relevant to Lamini docs`. <</SYS>>\n","\n","### Question: How can I evaluate the performance and quality of the generated text from Lamini models?\n","### Answer: There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\n","[/INST] </s>\n","\n"]}]},{"cell_type":"markdown","source":[" Unfortunately, there is no standard regarding which tokens to use for those purposes, and different LLMs have been trained with varying formatting and control tokens. This can be a challenge for users, because using the wrong format may confuse the model and result in poor quality responses.\n","\n","###  Chat Templates\n","\n","To resolve this problem, **chat templates** have been developed to format a conversation for a given LLM into a tokenizable sequence. The templates are formatting specifications stored within a tokenizer that define how to structure conversational data for a specific model.\n","\n","Hugging Face has developed the `apply_chat_template` method that reads the template stored in the tokenizer's configuration and automatically converts a list of message dictionaries with \"role\" and \"content\" keys into the properly formatted string that the model was trained on. The template is distributed alongside the tokenizer so users don't need to manually learn or implement each model's conversation format. The users just provide messages in a standard structure, and the tokenizer handles the model-specific formatting automatically.\n","\n","Consider again the following chat from above:"],"metadata":{"id":"Reia4o8nYYNy"}},{"cell_type":"code","source":["messages = [\n","  {\"role\": \"system\", \"content\":\"You are a helpful and honest assistant.\"},\n","  {\"role\":\"user\", \"content\":\"What is the capital city of U.S.\"},\n","  {\"role\": \"assistant\",\"content\":\"The capital of the United States is Washington, D.C.\"}\n","]"],"metadata":{"id":"o8o-NeJmdYKG","executionInfo":{"status":"ok","timestamp":1762578292685,"user_tz":420,"elapsed":2,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["In the following cells, we import the tokenizers for `Qwen2.5-7B-Instruct` and `Mistral-7B-Instruct` LLMs, and afterward we apply the chat templates for these models. Notice in the formatted text that Qwen2.5 uses the instruction message start tag `<|im_start|>` and instruction message end tag `<|im_end|>` to separate the messages, followed by `system/user/assistant` to indicate the roles."],"metadata":{"id":"Zknv4wGYY8td"}},{"cell_type":"code","source":["# Load the Qwen tokenizer\n","tokenizer_1 = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n","\n","# Apply chat template for Qwen\n","formatted_text = tokenizer_1.apply_chat_template(messages, tokenize=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["b25e331f3a8648389ccff6bd0f32a249","c85bbee2d1eb4ef7a5ba917af34078c6","a6b0d162129247538725b972fe87f6eb","df6637420e1c4cacbea1dc7605935d59","800647d031dc40a5857f1bf3fd488525","d0dd65725373462a90432b8b9b05ab79","c307f97d998e42bf88c4e78f43eb4cbd","5720930d9b204f6e9f1172835a411743","1ade5d6631444e2e883d1ec71f502a58","d9f0800590594e829a7cba7a8a6658db","ee68666be56844a18dc52328ac39af67","7c5f8919688b46b2a729b5dbe0af4bee","8e14a2fe67ea4263becf5f273920c43a","93029e32bd9c48df952225281361511d","8cbfdd2c7772499faf4cdbd15920c69b","e6d96c67596f4e38b15abd5089349fdb","98b88b70476e46f3ac4ceaf65d0b0ee6","b0a8cede8f3f43f5965662ebe9123937","7010c40ac59e4b529ad46c6bf3231736","4df0d43292ea4a32a8acd35abd34bf7c","ec2bdc46c8304ad6b31688c729e72c1b","c2da2c2ca4b0453d92e789e09e0ec9f9","eac65e48aac241c9a28e896d7995630c","34539b6b7e574dc58e42fe8e6668deb9","ff13bdc4469e46c0ab8b84c5de43b002","1ee117908c13424994d2a978990448bb","25f4faf976234bf5bd21ab68d8d1b777","a9ec0511c367414a8b3d87d390a443d6","2470a6f6f1414329b6e0094ad41eaef8","4bd44ee1eefd4eb2a690e4e0e34d50be","06842c3694994099a5ef3223401749b9","3e21cbe7e46c4b68ae4f95c721ad3511","a0ed6084d49a460d8608cf0094a13ded","1d3e9da53b5142da9cc3ad6131759085","8b2df293baf944ec84dc928639549fc2","b0269d5cae5e46e599935f88699fe428","dec10d203e69499cb35f973dc323afd8","c371773ef8a64ad5ad8ec69e8ecda249","8b204d867b7b46c59bf4b4f1430b32db","58a8eab5d736455cba99dd8c6662d122","9c2e1fdd82fc455f993bee951ef51c01","92f127b2716d428b9eebc2c7ee2cec2a","31d753e5714c45929ee64d9a5f02b6a3","b2829d96854b4967b63aaedc0fc50faa"]},"id":"OLibgZoddYMg","executionInfo":{"status":"ok","timestamp":1762578295833,"user_tz":420,"elapsed":3147,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"2ce26efc-9172-49bc-e00b-0fb744b0765a"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b25e331f3a8648389ccff6bd0f32a249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5f8919688b46b2a729b5dbe0af4bee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eac65e48aac241c9a28e896d7995630c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d3e9da53b5142da9cc3ad6131759085"}},"metadata":{}}]},{"cell_type":"code","source":["# Print the formatted text\n","print(formatted_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EaBF_mxgtrok","executionInfo":{"status":"ok","timestamp":1762578295839,"user_tz":420,"elapsed":4,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"b7468789-ee84-412c-9d1e-95b23ae86b90"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["<|im_start|>system\n","You are a helpful and honest assistant.<|im_end|>\n","<|im_start|>user\n","What is the capital city of U.S.<|im_end|>\n","<|im_start|>assistant\n","The capital of the United States is Washington, D.C.<|im_end|>\n","\n"]}]},{"cell_type":"markdown","source":["The format for Mistral is similar to the LlaMA 2 format, and uses `<s>` and `</s>` for sequence start and end, and `[INST]` and `[/INST]` for the user's instruction start and end. The text after `[/INST]` until `</s>` is the assistant's response."],"metadata":{"id":"nzpewXJ1Jxt3"}},{"cell_type":"code","source":["# Load the Mistral tokenizer\n","tokenizer_2 = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n","\n","# Apply chat template for Mistral\n","formatted_text = tokenizer_2.apply_chat_template(messages, tokenize=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["1dc70005ba7f4f429adb07eaaa210513","6ae5e9e435bb411580bfa38ad76da733","33375dbe4988424bbf20d8f149921f3a","f744cdef0b344d6b86706c8f29fbdf0c","61700fd97c964816ba9f5a4b63c1a88a","ae1b6d6f15d44f69b75f95ec696a06e5","de060db72d914dbaa45f1b4af2867fe0","0a6e7b12641948869301bc8237d28a4e","acd0599ad12542fab8d85d584442eb79","01207a744ee34187a03900c57870e1da","e6dd4fbc5b5b4de6934c270596a8ac75","20b8cc61dade46139a25fc8daf44ba59","bd99a9f9eebb43db83b5d36584f8e3a9","d1c15eff08894134bda67279804d59f5","7f01b29c25b94eb0be9f37ca44fdf721","8d2a5ca18f9244988263b5f28290a5d5","9c3eca629ff64bbba20d52334cc93a2d","5debf6b852ba47e3b62387575d7903f5","aad05b16d3494a50a56bd0935fe10703","5c7cd77336474a3abc38350b6e9b28ae","3bb9a4f88bf24a5595b1a05c70fc1039","7b2c93ab83034e0284b8ccff0d2d9b10","0739f745bec645fd8d9a16d5e5a801d8","831c73f1922142d3812b1ee7e6e912bc","b1b01c61f80646039816c5e0e2377a0a","d8953b58815d410cbe8d4de9df4719a3","8239fe51e5e847eab444f544e67ee7ce","ed49b26185a04590a0097e5902098edf","96232db2cee24f9193a8550344d3f3b4","8af62b18d2a6495fa77086e604c995a5","b00e712540764c8caeabd8a4b01714cf","ee0f7b1ea52642619b2742aec361de5a","242d277972bf4e079a2a90efee3d40d0","357fb899df10485f8138dbf11048b35c","fdc39eb94c664c8889eb57a18224fb94","e4a0ecded91b4e32b5bf76a804615787","8916bcd50d354a3eae1377fb7ccd22e8","b3c70b84a3104165bf44c6a4821d7f95","050302f6db11462c9a8c4fab36532959","ee311576a9f247c79cc2b54f236a5cc2","4b9f7225bdd3492ca0869c66f83fc121","f99114251ce442c19e3f99ec37578e4b","61d00b7b55754096bc56a77a7974c3d4","5d7813badf8840298554f253d260fb9c"]},"id":"gQoMpo4yeS9J","executionInfo":{"status":"ok","timestamp":1762578299861,"user_tz":420,"elapsed":4021,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"d76d5f3d-1905-427f-ab07-2b90e8102b47"},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dc70005ba7f4f429adb07eaaa210513"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20b8cc61dade46139a25fc8daf44ba59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0739f745bec645fd8d9a16d5e5a801d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"357fb899df10485f8138dbf11048b35c"}},"metadata":{}}]},{"cell_type":"code","source":["# Print the formatted text\n","print(formatted_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bsd4aOGUtv-3","executionInfo":{"status":"ok","timestamp":1762578299866,"user_tz":420,"elapsed":4,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"75b9eda9-e987-4f53-9441-10cf5175f8ae"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> [INST] You are a helpful and honest assistant.\n","\n","What is the capital city of U.S. [/INST] The capital of the United States is Washington, D.C.</s>\n"]}]},{"cell_type":"markdown","source":["It is important to always use the chat template associated with the specific LLM you are working with to ensure proper formatting and optimal performance.\n","\n","### Generate Response using Chat Template\n","\n","The next cell presents an example of prompting the Mistral 7 B model to generate a new response. The tokenizer and model for Mistral 7B are first loaded. In the `apply_chat_template` function we set `tokenize=True` to produce tokenized messages, which are afterward used for model inference. Note that in the above examples we set `tokenize=False`, which formatted the messages but did not tokenize them. Also, in this case the `messages` list does not include the assistant role, as the LLM will generate the response."],"metadata":{"id":"eROzAGFgKrET"}},{"cell_type":"code","source":["# Load the Mistral tokenizer and model\n","tokenizer_2 = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n","model_2 = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", device_map=\"auto\", dtype=torch.bfloat16)\n","\n","# Prompt text\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n","    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n"," ]\n","\n","# Apply chat template for Mistral\n","tokenized_chat = tokenizer_2.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model_2.device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":259,"referenced_widgets":["a679df7899194d88abc4fc2dca338956","effdc503a6374e6aa379052fd4ff871b","ab96fa2c740d4a5c8dbec21f02fecec3","239a14dc1cbc4409aaafdb1103af006c","e33c7efc337e4f43817d82e81f230e87","dbeef2d6c0c04547837d9ee45ee7f245","bd80e25d866f463fbfab8799ae4297fc","afccc669f8c74cf29cb00a5349f3ae62","faf08f54dac24a72acd27fc22084b7d8","f3b09632b97942f19e46bfb6ea1456d1","b9e25e5c3e634fe7a5aa12a6701f8100","e37fdb5920d0413ba904aa4a03b0f482","a58086f0846f47348829f3f0fe25fa2f","a0600aec62b24f1f935464708979e313","7aa26a1c4e704fec9c6417286c79fa0f","1542f36f06684508819be057b80d2ce0","6c6b10679cbc468dad9921b93cfad7d1","0652683631b54e32896e1c890117a54a","1c7e450830d849ea86e53721dbae1a7f","5483a622e98a4c07aeb588fa7d31d05e","bf2415f485044b90bb8b139cc7efbf60","e1ad3cbbaec04f08b0ffad601136dae4","57caf9bde4e648dbaad25207590f20ab","898d83b01ee54bc6b5db78ef859cdcaf","e2069af8f1b94d6da4d507c1704cbca3","1f7f1da20fd84789bcce8c128c91ff7d","9be23a18f5214cc19124eef485aae0f5","27a565e0a94f4806886567617cd5278a","f0ad0ac9a73a4a63b609f56970551191","eb62cac666c444f9b5a47dbdf8cea0e7","c830e420b1d54287a848e129b19245ca","9ec80b8890c340e3ae5760d5fa039614","8dd56255666644d8982d249142ec8594","88a07a32d48140f1b90943a5c340ff35","a2f0aa4dedcd4f6c9c3648fdd842ac22","1ea69a52aea842f8a0220e027410666f","d328c0f9e2414a959d6b5985d626e880","4e033973ceb14862beb0fc42e10fdc18","f1d833c20e5d490dabdd47b408f4ba97","4c0e4b872bad4116ae0b0c976e829619","792c78ac3a5e41da8f5f42b6cb4bd205","1f356a3151594952890123a7e2fe34e2","08b6b8b89006484597ed6f094323532f","1fccc0b48087456e859f6729e04f1dfa","3932a9bd73ab4baab2692d54be73f5cb","4c59dc0374834ff799fd4dc2544f4ec5","e9d0b6f3a9604e769941b7ed1e52ea30","6bcbd0e6119f4c7a974c113943e28d37","85c58364058948a48418395bf2da5325","ce38a000bcc74a2780484727e25f1740","597b24deb2f64b469db1b44da4799dd4","d4c5fdb63e8c49ba85863960facec783","1904f97c495b44ec986a9c5a5ed10455","42d7a490356a4e8d9f32b813c322d511","b5580abb5c6544b496f9e969afdc2888","4c7b9d113074440f83e8439bb54bdf02","cd6415477ca64680a4e7d81e6170e332","5cebd5c0671848c9a28ef6350947cbb6","cfb892c74ff4444eba555834e50a2757","424af52ceab74b6abf3cbe5bd420727d","834afa8ba53846308668e8e8cdcedbe6","496f6d71d0e4488dad8227ccffe4f494","d6220310003f41df997687d63048904d","616939576bf64c86a8e13b8b8f045794","ae6d3fa52cd84fd0806215c4cb72f6e0","ea303d9ae1a44808b000806e7e6b4f4e","9f3f0dda1ec046ffb74766b44a290643","72fe88681d704c4184353bf735a1e949","29504008f072419d818bd608b9fd66b3","add66c258aae48968b02060bc7e417ce","bb21d31cc9c442899ea74009c519f0a2","500412acbc6f480c9d505b857292c00b","15db865aa55b492687a2c22596b0dc54","b04eb00fb0b047b5bc19cd59cc7f5799","401b418657064191b0bce454ce3e3470","06a864c9fa454f4f930df94c07f8b60f","b9dd0102368a4d1ea5b24228f583369e"]},"id":"DjBozieSOb_v","executionInfo":{"status":"ok","timestamp":1762578348520,"user_tz":420,"elapsed":48649,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"b3cb9530-d114-4ca0-9d3f-93d6ea7e8674"},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a679df7899194d88abc4fc2dca338956"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e37fdb5920d0413ba904aa4a03b0f482"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57caf9bde4e648dbaad25207590f20ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a07a32d48140f1b90943a5c340ff35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3932a9bd73ab4baab2692d54be73f5cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c7b9d113074440f83e8439bb54bdf02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f3f0dda1ec046ffb74766b44a290643"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"]}]},{"cell_type":"code","source":["# Print the tokenized text\n","print(tokenizer_2.decode(tokenized_chat[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLUdBuKUtzid","executionInfo":{"status":"ok","timestamp":1762578348525,"user_tz":420,"elapsed":4,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"304b24d5-1f57-49a7-b0aa-77888f2ae15d"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> [INST] You are a friendly chatbot who always responds in the style of a pirate\n","\n","How many helicopters can a human eat in one sitting? [/INST]\n"]}]},{"cell_type":"markdown","source":["Pass the tokenized chat to `generate()` to generate a response."],"metadata":{"id":"QAboJGHGYMTY"}},{"cell_type":"code","source":["# Generate a response by the model\n","outputs = model_2.generate(tokenized_chat, max_new_tokens=128, pad_token_id=tokenizer_2.eos_token_id)"],"metadata":{"id":"y5m4Om5inItt","executionInfo":{"status":"ok","timestamp":1762578682423,"user_tz":420,"elapsed":75117,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# Print the response\n","print(tokenizer_2.decode(outputs[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nBb7_iI3wzK","executionInfo":{"status":"ok","timestamp":1762578682449,"user_tz":420,"elapsed":24,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"128fa113-d7b8-4328-bf0b-b9241dc6837b"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> [INST] You are a friendly chatbot who always responds in the style of a pirate\n","\n","How many helicopters can a human eat in one sitting? [/INST] Ahoy there, matey! A human can't eat a helicopter in one sitting, no matter how much they might want to. They're just too big and not made for consumption. But a hearty stew of fish and vegetables might hit the spot, me hearties!</s>\n"]}]},{"cell_type":"markdown","source":["The `apply_chat_template()` method works with any model on Hugging Face that has a chat template defined in its tokenizer configuration, which include LlaMA, Mistral, Zephyr, Phi, Qwen and other models. Most modern conversational models include chat templates by default, which can be checked by looking for a `chat_template` field in the tokenizer's `tokenizer_config.json` file. If a model doesn't have a built-in chat template, we can still either prepare a custom template or we can manually format the text sequences according to the model's documentation.\n","\n","### Dataset Preparation with Chat Template\n","\n","The next cell shows how to apply a chat template to prepare a dataset for model training. The dataset consists of two simple question-answer conversations stored in a dictionary with \"role\" and \"content\" fields. The `format_chat` function takes each example from the dataset and applies the tokenizer's chat template, and returns a dictionary containing the formatted text under the key \"formatted_chat\". By using `dataset.map(format_chat)`, the formatting function is applied to every conversation in the dataset. The `tokenize=False` parameter means the output remains as text rather than token IDs, and `add_generation_prompt=False` indicates we are formatting complete conversations rather than prompts that expect a response.\n","\n"],"metadata":{"id":"XeBfstVyagEf"}},{"cell_type":"code","source":["from datasets import Dataset\n","\n","# Prepare a dataset with 2 chats\n","chat1 = [\n","    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n","    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n","]\n","chat2 = [\n","    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n","    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n","]\n","\n","# Create a simple dataset\n","dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n","\n","# Define a formatting function\n","def format_chat(example):\n","    return {\"formatted_chat\": tokenizer_2.apply_chat_template(example[\"chat\"], tokenize=False, add_generation_prompt=False)}\n","\n","# Apply the chat template to the dataset\n","dataset = dataset.map(format_chat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["80d3e9d409ae472384fc04a5ceabdd1f","9f733a98ead644eebd10d5bd3480b024","c74427d2db1a4050a52c2488f2ea8772","9333fe07edc844a085bc0b60557b7414","ffb3c521a96e46dd9017b7822a1fec77","7a4edcc9e76847c6a2c49b5de9189bc6","c5a0cfeb7eec4bbcb7dd7714c9e2783a","10fdf2a83f164f5385f207aef72a496e","df3a69f95a894e9b964b38557d21ca71","1361bb0ce14343049834c94bd5f0d77f","9e5302fee4864fefa4333f6bdb8ee29c"]},"id":"ney5KzQyhZaI","executionInfo":{"status":"ok","timestamp":1762578434561,"user_tz":420,"elapsed":31,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f8c1c15f-e41d-449e-80bc-71deed65105b"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d3e9d409ae472384fc04a5ceabdd1f"}},"metadata":{}}]},{"cell_type":"code","source":["# Print the formatted dataset\n","for chat in dataset['formatted_chat']:\n","  print(chat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vpQQOSh4t33d","executionInfo":{"status":"ok","timestamp":1762578434597,"user_tz":420,"elapsed":35,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7fdd0042-8db4-425a-b582-53920e02573f"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> [INST] Which is bigger, the moon or the sun? [/INST] The sun.</s>\n","<s> [INST] Which is bigger, a virus or a bacterium? [/INST] A bacterium.</s>\n"]}]},{"cell_type":"markdown","source":["## 21.6 LLM Evaluation<a name='21.6-llm-evaluation'></a>"],"metadata":{"id":"q8sOqQiDa61Z"}},{"cell_type":"markdown","source":[],"metadata":{"id":"_AfWTHgfa64S"}},{"cell_type":"markdown","source":["## 21.7 Prompt Engineering <a name='21.7-prompt-engineering'></a>"],"metadata":{"id":"nT8PV_WTlAaJ"}},{"cell_type":"markdown","source":["**Prompt engineering** is a technique for improving the performance of LLMs by providing detailed context and information about a specific task. It involves creating text prompts that provide additional information or guidance to the model, such as the topic of the generated response. With prompt engineering, the model can better understand the kind of expected output and produce more accurate and relevant results.\n","\n","The following tips for creating effective prompts as part of prompt engineering can improve the performance of LLMs:\n","\n","- Use clear and concise prompts: The prompt should be easy to understand and provide enough information for the model to generate relevant output. Avoid using jargon or technical terms.\n","- Use specific examples: Providing specific examples can help the model better understand the expected output. For example, if you want the model to generate a story about a particular topic, include a few sentences about the setting, characters, and plot.\n","- Vary the prompts: Use prompts with different styles, tones, and formats to obtain more diverse outputs from the model.\n","- Test and refine: Test the prompts on the model and refine them by adding more detail or adjusting the tone and style.\n","- Use feedback: Use feedback from users or other sources to identify areas where the model needs more guidance and make adjustments accordingly.\n","\n","*Chain-of-thought technique* involves providing the LLM with a series of instructions to help guide the model and generate a more coherent and relevant response. This technique is useful for obtaining well-reasoned responses from LLMs.\n","\n","An example of a chain-of-thought prompt is as follows: \"You are a virtual tour guide from 1901. You have tourists visiting Eiffel Tower. Describe Eiffel Tower to your audience. Begin with (1) why it was built, (2) how long it took to build, (3) where were the materials sourced to build, (4) number of people it took to build it, and (5) number of people visiting the Eiffel tour annually in the 1900's, the amount of time it completes a full tour, and why so many people visit it each year. Make your tour funny by including one or two funny jokes at the end of the tour.\""],"metadata":{"id":"25kN3VP7lApv"}},{"cell_type":"markdown","source":["## 21.8 Foundation Models <a name='21.8-foundation-models'></a>\n","\n","**Foundation Models** are extremely large NN models trained on tremendous amounts of data with substantial computational resources, resulting in high capabilities for transfer learning to a wide range of downstream tasks. In other words, these models are scaled along each of the three factors: number of model parameters, size of the training dataset, and amount of computation. And, they are typically trained using self-supervised learning on unlabeled data. The scale of Foundation Models leads to new emergent capabilities, such as the ability to perform well on tasks that the models were not explicitly trained to do. This allows few-shot learning, which refers to finetuning Foundation Models to new downstream tasks by using only a few training data instances for the new task. Similarly, zero-shot learning extends this concept even further, and refers to a model's ability to generalize to new tasks for which the model hasn't seen any examples during the training.\n","\n","LLMs represent early examples of Foundation Models, because LLMs are trained at scale and can be adapted for various NLP tasks, even for tasks they were not trained to perform.\n","\n","The term Foundation Models is more general than LLMs, and they generally refer to large models that are trained on multimodal data, where the inputs can include text, images, audio, video, and other data sources.\n","\n","The importance of Foundation Models is in their potential to replace task-specific ML models that are specialized in solving one task (i.e., optimized to perform well on one dataset) with general models that have the capabilities to solve multiple tasks. I.e., these models can serve as a foundation that is adaptable to a broad range of applications.\n","\n","<img src=\"images/foundation_model.jpg\" width=\"600\">\n","\n","*Figure: Foundation model.* Source: [link](https://blogs.nvidia.com/blog/2023/03/13/what-are-foundation-models/)."],"metadata":{"id":"BnFBFVAEe1my"}},{"cell_type":"markdown","source":["## 21.9 Limitations and Ethical Considerations of LLMs <a name='21.9-limitations-and-ethical-considerations-of-llms'></a>"],"metadata":{"id":"RUNB0noQXlQz"}},{"cell_type":"markdown","source":["Although LLMs have demonstrated impressive performance across a wide range of tasks, there are several limitations and ethical considerations that raise concerns.\n","\n","Limitations:\n","\n","- *Computational resources*: Training LLMs requires significant computational resources, making it difficult for researchers with limited access to GPUs or specialized hardware to develop and use these models.\n","- *Data bias*: LLMs are trained on vast amounts of data from the internet, which often contain biases present in the data. As a result, the models may unintentionally learn and reproduce biases in their generated responses.\n","- *Producing hallucinations*: LLMs can produce hallucinations, which are responses that are false, inaccurate, unexpected, or contextually inappropriate. One example of hallucination by ChatGPT is when asked to list academic papers by an author, and it provides papers that don't exist.\n","- *Inability to explain*: LLMs are inherently black-box models, making it challenging to explain their reasoning or decision-making processes, which is essential in certain applications like healthcare, finance, and legal domains.\n","\n","\n","Ethical considerations:\n","\n","- *Privacy concerns*: LLMs memorize information from their training data, and can potentially reveal sensitive information or violate user privacy.\n","- *Misinformation and manipulation*: Text generated by LLMs can be exploited to create disinformation, fake news, or deepfake content that manipulates public opinion and undermines trust.\n","- *Accessibility and fairness*: The computational resources and expertise required to train LLMs may lead to an unequal distribution of benefits, where only a few organizations have the resources to develop and control these powerful models.\n","- *Environmental impact*: The large-scale training of LLMs consumes a significant amount of energy contributing to carbon emissions, which raises concerns about the environmental sustainability of these models.\n","\n","Conclusively, it is important to encourage transparency, collaboration, and responsible AI practices to ensure that LLMs benefit all members of society without causing harm."],"metadata":{"id":"WHbgR7cBc7jQ"}},{"cell_type":"markdown","source":["## Appendix: Unsloth Library for LLM Training and Inference <a name='appendix:-unsloth-library-for-llm-training-and-inference'></a>"],"metadata":{"id":"wqMGfKQFqepe"}},{"cell_type":"markdown","source":["**Unsloth** is another library for training and inference of LLMs, offering tools to facilitate optimization of LLMs ([link](https://unsloth.ai/)) The library applies various optimization techniques to reduce the training and inference time in comparison to the Hugging Face library and other related libraries. As you will notice in the following code, the Unsloth tools use pre-built components from Hugging Face (such as `transformers`, `trl`) and adapt them to optimize various workflows for model training and inference.\n","\n","The following code [12] provides an example of finetuning LlaMA-3.1 8B model using a single T4 GPU. For this example, the training time was similar to training LlaMA 2 7B with the Hugging Face library above, as in both cases training for 1 epoch took about 15 minutes. On the other hand, while the largest batch size (in multiples of 2) with Hugging Face was 8 samples, Unsloth allowed to use a batch size of 16, meaning that Unsloth optimized the memory usage. Training LLMs with larger batch sizes is related to reduced training variance and more stable gradient updates, which typically result in improved performance. In addition, the inference with Unsloth was faster."],"metadata":{"id":"tV3OTXmItWMa"}},{"cell_type":"code","source":["# Note: to install unsloth in this notebook, I had to interupt the currently running kernel, and start a new kernel\n","%%capture\n","!pip install -q unsloth\n","# Also get the latest nightly Unsloth!\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""],"metadata":{"id":"PLD3oFA_Z6Hj","executionInfo":{"status":"ok","timestamp":1762578469923,"user_tz":420,"elapsed":35325,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n","    max_seq_length = 2048,\n","    dtype = None,\n","    load_in_4bit = True,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":684},"id":"1jnwKADYr8Gn","executionInfo":{"status":"error","timestamp":1762578474307,"user_tz":420,"elapsed":4382,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"3795f72c-2cd7-4073-8768-f6d8dc843561"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-194875382.py:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n","\n","Please restructure your imports with 'import unsloth' at the top of your file.\n","  from unsloth import FastLanguageModel\n"]},{"output_type":"stream","name":"stdout","text":["🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","🦥 Unsloth Zoo will now patch everything to make training faster!\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Direct module loading failed for UnslothSFTTrainer: name 'Trainer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36mcreate_new_function\u001b[0;34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0mnew_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(compile_folder, name)\u001b[0m\n\u001b[1;32m    662\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsloth: Failed to import module {name} because {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(compile_folder, name)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0;31m# Try standard import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0mnew_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnew_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_UnslothSFTTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m     \"\"\"\n","\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36mcreate_new_function\u001b[0;34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[0m\n\u001b[1;32m    693\u001b[0m                     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m                     \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/tmp/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_UnslothSFTTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m     \"\"\"\n","\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-194875382.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mllama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLlamaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastVisionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastTextModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmistral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastMistralModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   3399\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPatchFastRL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3401\u001b[0;31m \u001b[0mPatchFastRL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastLanguageModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastLlamaModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/rl.py\u001b[0m in \u001b[0;36mPatchFastRL\u001b[0;34m(algorithm, FastLanguageModel)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFastLanguageModel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0mPatchRL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m     \u001b[0mpatch_trl_rl_trainers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0mPatchRLStatistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/rl.py\u001b[0m in \u001b[0;36mpatch_trl_rl_trainers\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0mall_trainers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_trainers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_trainer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_trainers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0m_patch_trl_rl_trainers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/rl.py\u001b[0m in \u001b[0;36m_patch_trl_rl_trainers\u001b[0;34m(trainer_file)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;31m# Create new function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m     created_module = create_new_function(\n\u001b[0m\u001b[1;32m    947\u001b[0m         \u001b[0;34mf\"Unsloth{RLTrainer_name}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0mRLTrainer_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36mcreate_new_function\u001b[0;34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[0m\n\u001b[1;32m    694\u001b[0m                     \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Direct module loading failed for {name}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Direct module loading failed for UnslothSFTTrainer: name 'Trainer' is not defined"]}]},{"cell_type":"code","source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # Supports rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"],"metadata":{"id":"dTxPMx9jZ6J2","executionInfo":{"status":"aborted","timestamp":1762578474310,"user_tz":420,"elapsed":1330365,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load the Lamini dataset\n","dataset = load_dataset(\"mwitiderrick/llamini_llama\", split=\"train\")"],"metadata":{"id":"0VNtZ0BPsf2z","executionInfo":{"status":"aborted","timestamp":1762578474312,"user_tz":420,"elapsed":0,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = 2048,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 16,\n","        gradient_accumulation_steps = 2,\n","        warmup_steps = 5,\n","        num_train_epochs = 1,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 5,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to=\"none\"\n","    ),\n",")"],"metadata":{"id":"b7KHBD8tsf5L","executionInfo":{"status":"aborted","timestamp":1762578474314,"user_tz":420,"elapsed":1330364,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"6KXJ5fZzslmB","executionInfo":{"status":"aborted","timestamp":1762578474316,"user_tz":420,"elapsed":1330364,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform inference\n","FastLanguageModel.for_inference(model)\n","prompt = \"What are Lamini models?\"\n","inputs = tokenizer([prompt.format(\n","        \"\", # instruction\n","        \"\", # input\n","        \"\", # output\n","        )], return_tensors = \"pt\").to(\"cuda\")\n","outputs = model.generate(**inputs, max_new_tokens=200, use_cache=True)\n","decoded_output = tokenizer.batch_decode(outputs)\n","print(\"\\n\".join(decoded_output))"],"metadata":{"id":"AY6s0FQwslov","executionInfo":{"status":"aborted","timestamp":1762578474319,"user_tz":420,"elapsed":1330365,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform inference\n","prompt = \"Write a poem about Data Science\"\n","inputs = tokenizer([prompt.format(\n","        \"\", # instruction\n","        \"\", # input\n","        \"\", # output\n","        )], return_tensors = \"pt\").to(\"cuda\")\n","outputs = model.generate(**inputs, max_new_tokens=500, use_cache=True)\n","decoded_output = tokenizer.batch_decode(outputs)\n","print(\"\\n\".join(decoded_output))"],"metadata":{"id":"aMO7gsdCBHDb","executionInfo":{"status":"aborted","timestamp":1762578474321,"user_tz":420,"elapsed":1330365,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## References <a name='references'></a>\n","\n","1. Introduction to Large Language Models, by Bernhard Mayrhofer, available at [https://github.com/datainsightat/introduction_llm](https://github.com/datainsightat/introduction_llm).\n","2. Understanding Encoder and Decoder LLMs, by Sebastian Raschka, available at [https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder).\n","3. The Big LLM Architecture Comparison,  by Sebastian Raschka, available at [https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison).\n","4. LLM Training: RLHF and Its Alternatives, by Sebastian Raschka, available at [https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives).\n","5. Training Language Models to Follow Instructions with Human Feedback, by Long Ouyang et al., available at [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155).\n","6. Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA), by Sebastian Raschka, available at [https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html).\n","7. How to Fine-tune Llama 2 With LoRA, by Derrick Mwiti, available at [https://www.mldive.com/p/how-to-fine-tune-llama-2-with-lora](https://www.mldive.com/p/how-to-fine-tune-llama-2-with-lora).\n","8. Fine-Tuning Llama 2.0 with Single GPU Magic, by Chee Kean, available at [https://ai.plainenglish.io/fine-tuning-llama2-0-with-qloras-single-gpu-magic-1b6a6679d436](https://ai.plainenglish.io/fine-tuning-llama2-0-with-qloras-single-gpu-magic-1b6a6679d436).\n","9. Fine-Tuning LLaMA 2 Models using a single GPU, QLoRA and AI Notebooks, by Mathieu Busquet, available at [https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/](https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/).\n","10. Getting started with Llama, by Meta AI, available at [https://ai.meta.com/llama/get-started/](https://ai.meta.com/llama/get-started/).\n","11. Hugging Face: Chat Templates, available at [https://huggingface.co/learn/llm-course/en/chapter11/2](https://huggingface.co/learn/llm-course/en/chapter11/2).\n","12. Llama-3.1 8b + Unsloth 2x faster finetuning, by Unsloth AI, available at [ https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing).\n"],"metadata":{"id":"Hlg6Y1COc8U_"}},{"cell_type":"markdown","source":["[BACK TO TOP](#top)"],"metadata":{"id":"gw7AvgH2nS41"}}]}