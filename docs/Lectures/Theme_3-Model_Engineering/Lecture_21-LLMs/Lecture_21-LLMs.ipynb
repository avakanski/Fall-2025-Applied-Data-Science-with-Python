{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Zc3eqC2hVQCg-97uT5BZZR-lWutdK5jP","timestamp":1699675698515},{"file_id":"1WlJSR0FBSQtTqM8AVpW48zbvAIyj5d7F","timestamp":1699662628845}],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPwb44h6NVq/4MqV1DG/MXH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Lecture 21 - Large Language Models"],"metadata":{"id":"xoYnii0YEv9n"}},{"cell_type":"markdown","source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_21-LLMs/Lecture_21-LLMs.ipynb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_21-LLMs/Lecture_21-LLMs.ipynb)"],"metadata":{"id":"QSUTHCD9lnyf"}},{"cell_type":"markdown","source":["<a id='top'></a>"],"metadata":{"id":"iGbTNIZlln1J"}},{"cell_type":"markdown","source":["- [21.1 Introduction to LLMs](#21.1-introduction-to-llms)\n","  - [21.1.1 Architecture of Large Language Models](#21.1.1-architecture-of-large-language-models)\n","  - [21.1.2 Variants of Transformer Network Architectures](#21.1.2-variants-of-transformer-network-architectures)\n","  - [21.1.3 Modern LLM Architectures](#21.1.3-modern-llm-architectures)\n","- [21.2 Creating LLMs](#21.2-creating-llms)\n","  - [21.2.1 Pretraining](#21.2.1-pretraining)\n","  - [21.2.2 Supervised Finetuning](#21.2.2-supervised-finetuning)\n","  - [21.2.3 Alignment ](#21.2.3-alignment)\n","- [21.3 Finetuning LLMs](#21.3-finetuning-llms)\n","  - [21.3.1 Parameter-Efficient Finetuning (PEFT)](#21.3.1-parameter-efficient-finetuning-(peft))\n","  - [21.3.2 Low-Rank Adaptation (LoRA)](#21.3.2-low-rank-adaptation-(lora))\n","  - [21.3.3 Quantized LoRA (QLoRA)](#21.3.3-quantized-lora-(qlora))\n","- [21.4 Finetuning Example: Finetuning LlaMA-2 7B](#21.4-finetuning-example:-finetuning-llama-2-7b)\n","- [21.5 Chat Templates for Formatting LLM Data](#21.5-chat-templates-for-formatting-llm-data)\n","- [21.6 LLM Evaluation](#21.6-llm-evaluation)\n","- [21.7 Prompt Engineering](#21.7-prompt-engineering)\n","- [21.8 Foundation Models](#21.8-foundation-models)\n","- [21.9 Limitations and Ethical Considerations of LLMs](#21.9-limitations-and-ethical-considerations-of-llms)\n","- [Appendix: Unsloth Library for LLM Training and Inference](#appendix:-unsloth-library-for-llm-training-and-inference)\n","- [References](#references)\n"],"metadata":{"id":"sMS08x8Rltiw"}},{"cell_type":"markdown","source":["## 21.1 Introduction to LLMs <a name='21.1-introduction-to-llms'></a>"],"metadata":{"id":"KrZV8AcvUPqZ"}},{"cell_type":"markdown","source":["Large Language Models (LLMs) are  a class of Deep Neural Networks designed to understand and generate natural human language.\n","\n","LLMs are a result of many years of research and advancement in NLP and Machine Learning. Important phases in NLP development include:\n","\n","- *Statistical language models (1980s-2000s)*: developed to predict the probability of a word in a text sequence based on the preceding words. Examples of statistical language models include Bag-Of-Words models based on N-grams. These models were used in tasks like speech recognition and machine translation, but struggled with capturing long-range dependencies and context-related information in text.\n","- *Neural network models (2000-2017)*: Fully-connected NNs and Recurrent NNs emerged as an alternative to statistical language models. Long Short-Term Memory (LSTM) RNN models were used for sequence-to-sequence tasks (such as  machine translation) and they formed the basis for several early LLMs. Similar to statistical language models, RNNs struggled with capturing context-related information. Other limitations of RNNs include the inability to parallelize the data processing, and the gradients can become unstable during training.\n","- *Transformer network models (2017-present)*: Transformer networks introduced the self-attention mechanism as a replacement for the recurrent layers in RNNs. This architecture enabled the development of more powerful and efficient LLMs, laying the foundation for BERT, GPT, and modern LLMs."],"metadata":{"id":"yut8E4WoRpVv"}},{"cell_type":"markdown","source":["### 21.1.1 Architecture of Large Language Models <a name='21.1.1-architecture-of-large-language-models'></a>"],"metadata":{"id":"v5uvsID2UMCT"}},{"cell_type":"markdown","source":["The architecture of modern LLMs is based on Transformer Networks, which we covered in Lecture 19. The main components of the Transformer Networks architecture include:\n","\n","- **Input embeddings**, are fixed-size continuous vector embeddings that represent tokens in input text.\n","- **Positional encodings**, are fixed-size continuous vectors that are added to the input embeddings to provide information about the relative positions of the tokens in the input text sequence.\n","- **Encoder**, is composed of a stack of multi-head attention modules and fully-connected (feed-forward) modules. The encoder block also includes dropout layers, residual connections, and applies layer normalization.\n","- **Decoder**, is composed of a stack of multi-head self-attention modules and fully-connected (feed-forward) modules similarly to the encoder block. The decoder block has an additional masked multi-head attention module, that applies masking to the next words in the text sequence to ensure that the module does not have access to those words for predicting the next token.\n","- **Output fully-connected layer**, the output of the decoder is passed through a fully-connected (dense, linear) layer to produce the next token in the text sequence.\n","\n","<img src=\"images/transformer.jpg\" width=\"450\">\n","\n","*Figure: Transformer architecture.* Source: [2]."],"metadata":{"id":"aP-g3MgRUYUI"}},{"cell_type":"markdown","source":["The architecture of Transformer Networks includes multiple successive encoder and decoder blocks to create deep networks with many layers that allow learning complex patterns in input text. For example, the original Transformer Network has 6 encoder and 6 decoder blocks, as shown in the above figure.\n","\n","The **self-attention mechanism** is a key component of the Transformer Network architecture that enables the model to weigh the importance of each token with respect to the other tokens in a sequence. It allows to capture long-range dependencies and relationships between the tokens (words) and helps the model to understand the context and structure of the input text sequence."],"metadata":{"id":"5eNRiCQhWXlN"}},{"cell_type":"markdown","source":["### 21.1.2 Variants of Transformer Network Architectures <a name='21.1.2-variants-of-transformer-network-architectures'></a>"],"metadata":{"id":"NtZrLHZdewMk"}},{"cell_type":"markdown","source":["A variety of LLMs have been developed based on the Transformer Network architecture. The main variants differ in how they use the encoder and decoder  of the original Transformer design. The three primary types are:\n","\n","- **Decoder-only models**: are autoregressive models that use only the decoder part of the Transformer Network architecture. These models are particularly suitable for generating text and content. An example of decoder-only LLMs is the family of GPT models. The majority of modern LLMs are decoder-only models.\n","- **Encoder-only models**: use only the encoder part of the Transformer Network architecture. This makes them well-suited for tasks related to language understanding, rather than text generation. They are used for text classification, sentiment analysis, named entity recognition, and similar tasks. An example is the BERT model.\n","- **Encoder-decoder models**: employ the original Transformer Network architecture and combine encoder and decoder sub-networks, enabling to both understand language and generate content. These models are used for translation, summarization, question answering, and similar tasks. An example of this class of models is T5 (Text-to-Text Transfer Transformer)."],"metadata":{"id":"FevKIGPne1iW"}},{"cell_type":"markdown","source":["### 21.1.3 Modern LLM Architectures <a name='21.1.3-modern-llm-architectures'></a>"],"metadata":{"id":"_SS79SrCcLYY"}},{"cell_type":"markdown","source":["\n","Modern LLMs have evolved significantly from the original Transformer architecture and they incorporate numerous optimizations in their layer design and computational efficiency. These advancements enable models to scale to hundreds of billions or trillions of parameters while maintaining manageable training and inference costs.\n","\n","The following figure illustrates the architectures of four modern open-source LLMs. Although the architectures of closed-source models such as ChatGPT and Claude are not publicly disclosed, they likely share many of the main design  features found in the open-source LLMs.\n","\n","A brief summary of the main charactistics of the shown LLMs is as follows:\n","\n","- *Multi-Head Attention* layers are replaced with more efficient variants such as **Grouped Query-Attention** layers. While Multi-head Attention assigns a separate set of parameters to each attention head which allows each head to learn distinct query, key, and values projections, Grouped-Query Attention shares key and value projections across groups of heads which reduces memory usage and computational costs.\n","- *Layer Normalization* is replaced with **RMS Normalization** (Root Mean Square Normalization). Differently from Layer Normalization which ensures zero-mean and unit-variance outputs by subtracting the mean and dividing by the standard deviation across the hidden dimension, RMS normalization scales the outputs only by their root mean square. In other words, it divides outputs by the standard devation, but does not subtract the mean to center the outputs. This makes it computationally simpler and often more stable in very deep models.\n","- **Rotary Positional Encodings (RoPE)** are commonly used in modern LLMs to encode positional information more effectively than traditional fixed (sinusoidal) or learned positional encodings.\n","- **SiLU Activation Function** is usually preferred in the feed-forward network block instead of ReLU. An overview of SiLU and other recent activation functions is presented below.\n","\n","Typical values for the hyperparameters in these LLMs are:\n","\n","- **Embedding dimension**, from 2,048 to 7,168.\n","- **Vocabulary size**, from from 128K to 151K.\n","- **Supported context length**, from 41K to 131K tokens.\n","- **Number of multi-attention heads**, from 32 to 64.\n","- **Hidden layer dimension**, from 1,536 to 9,728.\n","- **Number of decoder blocks**, from 16 to 94.\n","\n","Also note that the lower two LLMs have Mixture-Of-Experts (MOE) architecture. We will cover this type of architecture in the next lecture.\n","\n","<img src=\"images/LLMs_architectures.jpg\" width=\"1200\">\n","\n","*Figure: LLM Architectures.* Source: [3].\n","\n","\n"],"metadata":{"id":"PAHsxEMTcVN4"}},{"cell_type":"markdown","source":["#### Activation Functions\n","\n","In previous lectures, we learned that activation functions introduce non-linearity into NNs, enabling them to learn complex patterns.Traditional activation functions include hyperbolic tangent (tanh), sigmoid, and rectified linear unit (ReLU). In LLMs, activation functions are applied after the dense layers in the feed-forward network, and typically use variants of ReLU activations.\n","\n","Common activations functions used in modern LLMs are shown in the next figure and include:\n","\n","- **GELU (Gaussian Error Linear Unit)**: Is similar to ReLU, but instead of outputting zero for negative inputs, it maintains small gradients for negative values. GELU smoothly weights inputs by their probability under a Gaussian distribution. It generally performs better than ReLU in transformer models, although it is more computationally expensive.\n","- **SiLU (Sigmoid Linear Unit)**: Defined as the product of the input and its sigmoid $\\text{SiLU} = x \\cdot \\text{Sigmoid}(x) = \\frac{x}{1 + e^{x}}$, it results in a smooth and non-monotonic function. It preserves small negative activations, which helps improve optimization and generalization.\n","- **Swish**: Defined as $\\text{Swish}(x) = x \\cdot \\text{Sigmoid}(\\beta x) = \\frac{x}{1 + e^{-\\beta x}}$, it is similar to SiLU, but it introduces a parameter ùõΩ that controls the slope near zero. When $ùõΩ=1$, Swish becomes SiLU.\n","- **SwiGLU (Swish-Gated Linear Unit)** : Defined as $\\text{SwiGLU}(x) = \\text{Swish}(xW + b) \\cdot (xV + c)$, it is a product of a Swish function and a linear function, with the parameters $W, V, b,$ and $c$ are learned during training. In SwiGLU, the term $(xW + b)$ acts as a gate, and controls how much of the input signal passes through. This gating mechanism allows the model to learn complex patterns without additional layers.\n","- **GeGLU (GELU-Gated Linear Unit)**:  Defined as $\\text{GeGLU}(x) = \\text{GELU}(xW + b) \\cdot (xV + c)$, it is a variant of SwiGLU that replaces Swish with GELU activation to serve as the gate. It enhances representation power and training stability in some architectures.\n","\n","<img src=\"images/activations.png\" width=\"450\">\n","\n","*Figure: Activation functions in LLMs.* Source: [4]"],"metadata":{"id":"xyzRozBlx9J_"}},{"cell_type":"markdown","source":["The following figure provide another visualization of the layers in a LlaMA model, and shows in more detail the gating mechanism in SwiGLU activation functions.\n","\n","<img src=\"images/Llama_architecture.png\" width=\"250\">\n","\n","*Figure: LlaMA architecture.* Source: [4]"],"metadata":{"id":"GqAPZDr05jM3"}},{"cell_type":"markdown","source":["#### Other Characteristics of Modern LLMs\n","\n","- **Byte Pair Encoding (BPE)** is used in modern LLMS as a tokenization technique to represent text. Instead of using whole words as tokens, BPE breaks text into smaller pieces based on subwords. It starts with individual characters and repeatedly merges the most frequent pairs of characters or subwords in the text. This creates a vocabulary of frequently occurring chunks, and it helps the model to handle rare words and keep the vocabulary smaller. Importantly, BPE avoids the issue of out-of-vocabulary words, because any word can be represented as a sequence of indiviudal characters, if no subwords match that word.\n","\n","- **Flash Attention** is an optimized implementation of the attention mechanism in Transformers that reduces memory usage and computational cost on GPUs. Unlike the standard attention module which stores large intermediate matrices for all queries, keys, and values, Flash Attention computes attention scores in small blocks and fuses operations, avoiding the need to store these large intermediate tensors. Almost all modern LLMs use Flash Attention to reduce memory requirements and speed up training and inference.  \n","\n","\n"],"metadata":{"id":"4kL6AMwjt9g9"}},{"cell_type":"markdown","source":["#### List of LLMs\n","\n","A large number of LLMs have been developed in the past several years. Some of the most well-known LLMs include:\n","\n","- *GPT* (Generative Pretrained Transformers): Developed by OpenAI, the GPT family are the best-known LLMs. They include GPT 1, 2, 3, 3.5 (initial ChatGPT), 4, 4o, 5 (current ChatGPT), and o1 (where \"o\" stands for omni, meaning that the model can process multi-modal inputs, including text, images, video, audio, etc.). According to some sources, GPT-5 has 1.76 trillion parameters, and it is trained on 20T tokens.\n","- *LlaMA (Large Language Model Meta AI)*: Developed by Meta AI, LlaMA is an open-source LLM, suitable for both research and commercial uses. It includes the base LlaMA model, LlaMA-Chat, and Code-LlaMA. Released versions include LlaMA 2, LlaMA 3, LLaMA 3.1, and LlaMA 3.2. The latest LlaMA 3.2 includes smaller test models with 1B and 3B parameters, and multi-modal 11B and 90B parameters, trained on 9T tokens.\n","- *Claude*: Developed by Anthropic, the latest version Claude 4 has two models named Sonnet and Opus. These models rank very high on the benchmarking leaderboards for many tasks, and they are currently the main competitor to OpenAI's GPT models.\n","- *Gemini*: Developed by Google, offers four models named Nano, Flash, Pro, and Ultra. The number of parameters is not known. The smaller models are designed for smartphones, whereas the larger models are multimodal and can process images, video, code, and other inputs, beside text.\n","- *Mixtral*: Developed by Mistral, these LLM use mixture-of-experts (MOE) architecture, which allows them to be competitive with larger models, despite having fewer parameters. Current models have 8 mixture-of-experts with 7B and 22B parameters.\n","- *Grok*: Developed by xAI, Grok is trained on data from X (formerly Twitter) and has 314B parameters. It also uses a mixture-of-experts (MOE) architecture.\n","- *DeepSeek*: Earlier versions involve 7B and 67B models, trained on 2T tokens. Latest model DeepSeek-V3 has 671B parameters trained on 14.8B tokens.\n","- *Qwen*: Developed by Alibaba, latest Qwen 3 models range from 0.6B to 235B parameters. Both dense and MOE architectures are available.\n","- *Cohere LLM*: Developed by Cohere, it is a family of LLMs with 6B, 13B, and 52B parameters, designed for enterprise use cases.\n","- *Vicuna*: Developed by LMSYS, Vicuna is a 13B parameters chat assistant finetuned from LLaMA on user-shared conversations.\n","- *Alpaca*: Developed by Stanford, it is a 7B LLM finetuned from instruction-following samples by LLaMA.\n","- *Falcon*: Developed by UAE's Technology Innovation Institute (TII), it is an open-source family of models with 1.3B, 7.5B, 40B, and 180B parameters, trained on 3.5T tokens.\n"],"metadata":{"id":"40129R8npOLm"}},{"cell_type":"markdown","source":["## 21.2 Creating LLMs <a name='21.2-creating-llms'></a>"],"metadata":{"id":"s3o9DjItXUxt"}},{"cell_type":"markdown","source":["Creating modern LLMs typically involves three main phases:\n","\n","1. **Pretraining**, the model extracts knowledge from large unlabeled text datasets.\n","2. **Supervised finetuning**, the model is refined to improve the quality of generated responses.\n","3. **Alignment**, the model is further refined to generate safe and helpful responses that are aligned with human preferences."],"metadata":{"id":"LaZLtPXyXfrK"}},{"cell_type":"markdown","source":["### 21.2.1 Pretraining <a name='21.2.1-pretraining'></a>\n","\n","The first step in creating LLMs is **pretraining** the model on massive amounts of text data. The datasets usually consist of a large collection of web pages or e-books comprising billions or trillions of tokens, and ranging from gigabytes to terabytes of text. During pretraining, the model learns the structure of the language, grammar rules, facts about the world, and reasoning rules. And, it also learns biases and harmful content present in the training data.\n","\n"," Pretraining is performed using unsupervised learning techniques. Two common approaches for pretraining LLMs are:\n","\n","- **Causal Language Modeling**, also known as autoregressive language modeling, involves training the model to predict the next token in the text sequence given the previous tokens. This approach is more common with modern LLMs.\n","- **Masked Language Modeling**, where a certain percentage of the input tokens are randomly masked, and the model is trained to predict the masked tokens based on the surrounding context. BERT and earlier LLMs were pretrained with masked language modeling.\n","\n","The following figure depicts the pretraining phase with Causal Language Modeling, where the model learns to predict the next word in a sentence given the previous words.\n","\n","<img src=\"images/pretraining.jpg\" width=\"450\">\n","\n","*Figure: Pretraining LLMs.* Source: [5].\n","\n","Pretraining allows to extract knowledge from very large unlabeled datasets in unsupervised learning manner, without the need for manual labeling. Or, to be more precise, the \"label\" in LLMs pretraining is the next word in the text, to which we already have access since it is part of the training text. Such pretraining approach is also called self-supervised training, since the model uses each next word in the text to self-supervise the training.\n","\n","Note that pretraining LLMs from scratch is computationally expensive and time-consuming. As we stated before, the pretraining phase can cost millions of dollars (e.g., the estimated cost for training GPT-4 is $100 million). Also, pretraining LLMs requires access to large datasets and technical expertise with strong understanding of deep learning workflows, working with distributed software and hardware, and managing model training with thousands of GPUs simultaneously."],"metadata":{"id":"wJ5rp47DXf2w"}},{"cell_type":"markdown","source":["### 21.2.2 Supervised Finetuning <a name='21.2.2-supervised-finetuning'></a>\n","\n","After the pretraining phase, the model is finetuned on a much smaller dataset, which is carefully generated with human supervision. This dataset consists of samples where AI trainers provide both queries (instructions) and model responses (outputs), as depicted in the following figure. That is, *instruction* is the input text given to the model, and *output* is the desired response by the model. The model takes the instruction text as input (e.g., \"Write a limerick about a pelican\") and uses next-token prediction to generate the output text (e.g., \"There once was a pelican so fine ...\").\n","\n","The finetuning process involves updating the model's weights using supervised learning techniques. The objective of supervised finetuning is to improve the quality of the generated responses by the pretrained LLM.\n","\n","To compile datasets for supervised finetuning, AI trainers need to write the desired instructions and responses, which is a laborious process. Typical datasets include between 1K and 100K instruction-output pairs. Based on the provided instruction-output pairs, the model is finetuned to generate responses that are similar to those provided by AI trainers.\n","\n","<img src=\"images/finetuning.jpg\" width=\"500\">\n","\n","*Figure: Finetuning a pretrained LLM.* Source: [2]."],"metadata":{"id":"LnAjaqBPXlJC"}},{"cell_type":"markdown","source":["### 21.2.3 Alignment <a name='21.2.3-alignment'></a>\n","\n","To further improve the performance and align the model responses with human preferences, LLMs are typically refined in one additional phase. This ensures that the responses generated by LLMs are aligned with human preferences, making the models more useful and safer for interaction with users. The alignment phase is essential for reducing harmful, biased, or otherwise undesirable outputs.  \n","\n","Two main strategies for LLM alignment include Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization (PPO) and Reinforcement Learning with Direct Policy Optimization (DPO).\n","\n","**Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization (PPO)**\n","\n","LLM alignment with Reinforcement Learning from Human Feedback (RLHF) by employing Proximal Policy Optimization (PPO) is depicted in the figure below and involves the following steps:\n","\n","1. *Collect human feedback*. For this step a new dataset is created by collecting sample prompts from a database or by creating a set of new prompts. For each prompt, multiple responses are generated by the supervised finetuned model. Next, AI trainers are asked to rank by quality all responses generated by the model for the same prompt, from best to worst. Such feedback is used to define the human preferences and expectations about the responses by the model. Although this ranking process is time-consuming, it is usually less labor-intensive than creating the dataset for supervised finetuning, since ranking the responses is faster than writing the responses.\n","2. *Create a reward model*. The collected data with human feedback containing the prompts and the ranking scores of the different responses are used to train a Reward Model (denoted with RM in the figure). The task for the Reward Model is to predict the quality of the different responses to a given prompt and output a ranking score. The ranking scores provided by AI trainers are used to establish the ground-truth for training the Reward Model. Note that the Reward Model is a different model than the LLM that is being finetuned, and it only needs to rank the generated responses by the LLM.\n","3. *Finetune the LLM with RL*. The LLM is finetuned using the Reinforcement Learning (RL) algorithm Proximal Policy Optimization (PPO). For a new prompt, the original LLM generates a response, which the Reward Model evaluates and calculates a reward score $r_k$. Next, the PPO algorithm uses the reward score $r_k$ to finetune the LLM so that the total rewards for the generated responses by the LLM are maximized. I.e., the goal is to generate responses by the LLM that maximize the predicted reward scores, and by that, the responses become more aligned with human preferences and are more useful to human users.\n","4. *Iterative improvement*. The RLHF process is performed iteratively, with multiple rounds of collecting additional feedback from human labelers, re-training the Reward Model, and applying Reinforcement Learning. This leads to continuous refinement and improvement of the LLM's performance.\n","\n","<img src=\"images/RLHF.jpg\" width=\"600\">\n","\n","*Figure: Reinforcement Learning from Human Feedback.* Source: [6].\n","\n","In summary, the RLHF approach creates a reward system that is augmented by human feedback and is used to teach LLMs which responses are more aligned with human preferences. Through these iterations, LLMs can be better aligned with our human values and can lead to higher-quality responses, as well as improved performance on specific tasks.\n","\n","Note also that there are several variants of the RLFH approach for finetuning LLMs. For example, LlaMA models employ two reward models: one based on the ranks of helpfulness of the responses, and another based on the ranks of safety of the responses. The final reward score is obtained as a combination of the helpfulness and safety scores.\n","\n","**Reinforcement Learning with Direct Policy Optimization (DPO)**\n","\n","RL with Direct Policy Optimization (DPO) is another approach for LLM alignment that has been popular recently, as it is simpler than RLHF with PPO. DPO uses a different optimization approach in comparison to RL with PPO, where DPO optimizes the LLM directly based on user preferences, without the need for training a separate Reward Model. I.e., DPO aims to directly maximize the reward function to produce model outputs that align with human preferences. Detailed explanation of RL with DPO is beyond the scope of this lecture.\n"],"metadata":{"id":"0s2BsqGjXlLg"}},{"cell_type":"markdown","source":["## 21.3 Finetuning LLMs <a name='21.3-finetuning-llms'></a>"],"metadata":{"id":"HY3OHi4TlRAw"}},{"cell_type":"markdown","source":["**Finetuning LLMs** involves updating the weights of an LLM model on new data to improve its performance on a specific task and make the model more suitable for a specific use case. It involves additional re-training of the model on a new dataset that is specific to that task. That is, finetuning is a transfer learning technique, where the gained knowledge by a trained model is transferred to improve the performance on a target task.\n","\n","To adapt LLMs to a custom task, different finetuning techniques have been applied. *Full model finetuning* is a method that finetunes all the parameters of all the layers of a pretrained model. Full model finetuning typically can achieve the best performance, but it is also the most resource-intensive and time-consuming. *Performance-efficient finetuning* involves updating only a small number of the parameters to reduce the required computational resources and costs.\n","\n","In this section, we will demonstrate how to finetune **LlaMA 2**, an open-source LLM developed by Meta AI. Released in July 2023, LlaMA 2 was the first LLM that is open for both research and commercial use. LlaMA 2 is a successor model to the original LlaMA developed by Meta AI as well. LlaMA 2 has three variants with 7B, 13B, and 70B parameters. It has been trained on 2 trillion tokens, and it has a context window of 4,096 tokens enabling to process large documents. For instance, for the task of summarizing a pdf document the context can include the entire text of the pdf document, or for dialog with a chatbot the context can include the previous conversation history with the chatbot. Furthermore, specialized versions of LlaMA 2 include LlaMA-2-Chat optimized for dialog generation, and Code LlaMA optimized for code generation tasks."],"metadata":{"id":"gHwycTGfVUN_"}},{"cell_type":"markdown","source":["### 21.3.1 Parameter-Efficient Finetuning (PEFT) <a name='21.3.1-parameter-efficient-finetuning-(peft)'></a>"],"metadata":{"id":"rie71lFEWtQ_"}},{"cell_type":"markdown","source":["Finetuning LLMs is challenging since the large number of parameters of modern LLMs requires substantial computational resources for storing the models and for re-training the weights. Thus, it can be prohibitively expensive for most users. For instance, to load the largest version of the LlaMA 2 model with 70 billion parameters into the GPU memory requires approximately 280 GB of RAM. Full model finetuning of LlaMA 2 model with 70 billion parameters requires 780 GB of GPU memory. This is equivalent to 10 A100s GPUs that have 80 GB RAM each, or 48 T4 GPUs that have 16 GB RAM each. The free version of Google Colab offers one T4 GPU with 16 GB RAM.\n","\n","Fortunately, several Parameter-Efficient FineTuning (PEFT) techniques have been introduced recently, which allow updating only a small number of the model weights. Consequently, these techniques enable finetuning LLMs using lower computational resources by reducing memory usage and speeding up the training process. PEFT techniques include prompt tuning, prefix tuning, adding additional adapter layers in the transformer block, and low-rank adaptation (LoRA).\n","\n","Hugging Face has developed a [PEFT library](https://huggingface.co/docs/peft/index) that contains implementations of common finetuning techniques. We will use the PEFT library to finetune LlaMA 2 on a custom dataset using a quantized version of the LoRA method."],"metadata":{"id":"jprNHaStWtS7"}},{"cell_type":"markdown","source":["### 21.3.2 Low-Rank Adaptation (LoRA) <a name='21.3.2-low-rank-adaptation-(lora)'></a>\n","\n","**Low-Rank Adaptation (LoRA)** involves freezing the pretrained model and finetuning a small number of additional weights. After the additional weights are updated, these weights are merged with the weights of the original model.\n","\n","This is depicted in the following figure, where regular finetuning is shown in the left figure, and it involves updating all weights $W$ in a pretrained model. As we know, the weight update matrix $\\nabla{W}$ is calculated based on the negative gradient of the loss function. Finetuning with LoRA is shown in the right figure, where the weight update matrix $\\nabla{W}$ is decomposed into two smaller matrices, $\\nabla{W}=W_A*W_B$, with size $W_A \\in \\mathbb{R}^{A \\times r}$ and $W_B \\in \\mathbb{R}^{r \\times B}$. The matrices $W_A$ and $W_B$ are called low-rank adapters, since they have lower rank $r$ in comparison to the original weight matrix, i.e., they have fewer number of columns or rows, respectively. During training, gradients are backpropagated only through the matrices $W_A$ and $W_B$, while the pretrained weights $W$ remain frozen.\n","\n","For instance, if the full weight matrix $W$ is of size $100 \\times 100$, this is equal to $10,000$ elements (model weights). If we decompose the weight update matrix $\\nabla{W}$ by using rank $r=5$, the total number of elements of $W_A \\in \\mathbb{R}^{100 \\times 5}$ and $W_B \\in \\mathbb{R}^{5 \\times 100}$ will be $500 + 500 =  1,000$. Hence, with LoRA the number of elements was reduced from $10,000$ to $1,000$.\n","\n","<img src=\"images/LoRA.png\" width=\"600\">\n","\n","*Figure: Regular finetuning versus LoRA finetuning .* Source: [7]."],"metadata":{"id":"XS2wNFlwlgD1"}},{"cell_type":"markdown","source":["### 21.3.3 Quantized LoRA (QLoRA) <a name='21.3.3-quantized-lora-(qlora)'></a>\n","\n","**Quantized LoRA (QLoRA)** is a modified version of LoRA that uses 4-bit quantized weights. *Quantization* reduces the precision for the values of the network weights. In TensorFlow and PyTorch, the network weights by default are stored with 32-bit floating-point precision. With quantization techniques, the network weights are stored with lower precision, such as 16-bit, 8-bit, or 4-bit precision.\n","\n","This approach introduces a new 4-bit quantization format called \"nf4\" (normalized float 4) where the range of values is normalized to the range [-1, 1] by dividing the values evenly into 16 bins (4-bit allows $2^4=16$ values). While 4-bit floating point precision (fp4) applies non-linear floating point representation of the original values and results in unequal spacing of the values, normalized float 4 precision (nf4) applies linear quantization of the original values into equally spaced bins and follows a normal distribution.\n","\n","QLoRA combines 4-bit quantization of the model weights in the pretrained model and LoRA that adds low-rank adaptor layers. The benefits of QLoRA with 4-bit quantization of the model weights include reduced size of the model and increased inference speed, while having a modest decrease in the overall model performance.\n","\n","For example, with QLoRA a 70B parameter model can be finetuned with 48 GB VRAM, in comparison to 780 GB VRAM required for finetuning all weights of the original model (using 32-bit floating-point precision). Similarly, QLoRA enables to train the smaller version of LlaMA 2 with 7B parameters on a T4 GPU (provided by Google Colab) that has 16 GB VRAM. In cases when only a single GPU is available, using quantization is necessary for finetuning LLMs."],"metadata":{"id":"SfoWFu-JjxUr"}},{"cell_type":"markdown","source":["## 21.4 Finetuning Example: Finetuning LlaMA-2 7B<a name='21.4-finetuning-example:-finetuning-llama-2-7b'></a>"],"metadata":{"id":"n8BHP3m4ntWm"}},{"cell_type":"markdown","source":["\n","### Import Libraries\n","\n","We will begin by installing the required libraries and importing modules from these packages. These include `accelerate` (for optimized training on GPUs), `peft` (for Parameter-Efficient Fine-Tuning), `bitsandbytes` (to quantize the LlaMA model to 4-bit precision), `transformers` (for working with Transformer Networks), and `trl` (for supervised finetuning, where trl stands for Transformer Reinforcement Learning)."],"metadata":{"id":"kC7SG1ZO5L2y"}},{"cell_type":"code","source":["!pip install -q accelerate peft bitsandbytes transformers trl"],"metadata":{"id":"ZJrRD657TXc1","executionInfo":{"status":"ok","timestamp":1762622980762,"user_tz":420,"elapsed":7183,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"169d5eaf-7d4e-46b5-a925-c31507f81981"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n","    TrainingArguments, pipeline, logging)\n","from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","from trl import SFTTrainer"],"metadata":{"id":"-gVbHOdK5hdU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load the Model\n","\n","We will download the smallest version of LlaMA-2-Chat model with 7B parameters from Hugging Face. Understandably, the larger LlaMA 2 models with 13B and 70B parameters require larger memory and computational resources for finetuning.\n","\n","Also, we will use the BitsAndBytes library to apply quantization with 4-bit precision format for loading the model weights. Loading a quantized model reduces the GPU memory requirement and makes it possible to train the model with a single GPU, as a tradeoff for some loss in precision. In the next cell we define the configuration for BitsAndBytes, and afterward we will use the configuration in the `from_pretrained` function to load the LlaMA 2 model. The parameters in BitsAndBytes configuration are described in the commented code below.\n","\n","The compute type in the cell below refers to the data format for performing computations, and it can be either \"float16\", \"bfloat16\", or \"float32\" because computations are performed in either 16 or 32-bit precision. In this case, we specified to use `\"torch.float16\"` compute data type (i.e., 16-bit floating-point numbers) for memory-saving purposes. Note that although the model weights are loaded with 4-bit precision, the weights are dequantized to 16-bit precision for performing the calculations for the forward and backward passes through the network, since 4-bit precision is too low for performing the calculations."],"metadata":{"id":"DqCbomvC5L49"}},{"cell_type":"code","source":["# The model is Llama 2 from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\""],"metadata":{"id":"gChA0lhb7MpV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BitsAndBytes configuraton\n","bnb_config = BitsAndBytesConfig(\n","    # Load the model using 4-bit precision\n","    load_in_4bit=True,\n","    # Quantization type (fp4 or nf4)\n","    # nf4 is \"normalized float 4\" format, uses an asymmetric quantization scheme with 4-bit precision\n","    # optimized for normally distributed weights (better than fp4 for neural networks)\n","    bnb_4bit_quant_type=\"nf4\",\n","    # Compute dtype for 4-bit models\n","    bnb_4bit_compute_dtype= torch.float16,\n","    # Use double quantization for 4-bit models\n","    # Double quantization applies further quantization to the quantization constants\n","    bnb_4bit_use_double_quant=True,\n",")"],"metadata":{"id":"9vqy2DA-7Igl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will use `AutoModelForCausalLM` to load the model with the `from_pretrained` function, and we will use the above BitesAndBytes configuration to load the model parameters with 4-bit precision.\n","\n","In the following cell we will load the corresponding tokenizer for LlaMA 2 by using `AutoTokenizer` and `from_pretrained`."],"metadata":{"id":"NjKpW3FwVOsN"}},{"cell_type":"code","source":["# Load Llama 2 model from Hugging Face\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    # Apply quantization by using the bnb configuration from the previous cell\n","    quantization_config=bnb_config,\n","    # Don't cache the model weights, load the model weights from Hugging Face\n","    use_cache=False,\n","    # Trade-off parameter in Llama-2, less important, it should be 1 in most cases\n","    pretraining_tp=1,\n","    # Load the entire model on the GPU if available\n","    device_map=\"auto\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241,"referenced_widgets":["e7482b1809f4416392d78a39e33165ec","6fcbb297315e499481b52a387a163c09","28b67cc39a4944d58074dcfe250db1bb","e697477c4f29447c82fb34ee2e8392bf","95c702d669db4350b81bee28b0483eb9","17eb721b196a4d3285970c631ebe4444","58a65ff3af60495ab3e94438eee421ea","4ca3cb034bce4df4af57ccf0374b25da","dc3638af82e24614b2cb8b8c82b7c4c0","ef23daa9b15e44699cc2c6dc9ba356ab","08ef4f085536428781742b6528d04949","d8dca34cc3d548dcbdd5dbcbc744d12d","2ac7139cbca448bca2289c35a3b4eca6","099c091269bb4383a9c33b17bf8519cc","c652821d1b274571aed7dcf7f6dfb298","0ed2bfe4401f4b509064b42c291d3076","74975e2df06046bfaaaffcb166415189","792d810a8665447f83e8b3b0968ffb58","a39ea2895b264d07b903f140267955e4","4660c3f440334498990a3031668b14c6","0376d82d87704ca0874d8e84e6443b05","959d719a978347dc8d2efd0166c6f3c0","da7ab1d500dc4c8db5c0f0f11dcf1015","2ee870a307db4fc480728142ae6fafd8","b7e9d48108f24825975c874e476619a2","c6f501df710640d4b80adf3cf15ffa6c","a2bb9f5ad26b4f48829f8c0ce40a2ba7","10b1e32bd2a24a10921b0ac6f78bd079","d4a74e3bab154d19aecbf8dff883d027","e3b66d2fc0c844f1a4f65e853828015d","007d2e24bb0c4d7386d939625c237fb3","263840b5b0f0482bb0bfd88216cf8b60","10c10d609fad4e73b9c5976f1f7465de","3605da362f9d4c3e9001e3c1986a7930","b3d1e39c72914a1abb56d5573ee93f3b","bf709f75591e4c79be0404f0b986cf05","f58bb8e53885443990b985e6dcb63523","5765de23b7a441a9bd6e2e9b7599ab31","3292bfc103b444fe9acf926834f1d3f4","2ce453b2567d450db3319a145bc84b62","98602435eea14387b17f4b50b510c392","00d55f4aa1704bb7bd0aed805c5dcfc9","2955395e4b854fa8903936fbc3f8c9a0","4ff5ec0bbae743dfb0aeccdc824ad1df","18f53f74a7704d8183eaf9422fb0e861","8d283dfca0f14282b85e7c2e310027a8","e1532add393248af846143d9c9ae023a","83390f224e4b4127a76cada25b588b06","04d95e165e294c4e81307df3cbc5fdce","0f670ed0ee20446ea828bb8ff1dcaa89","dfe057dc901b4043a69841c91bc9951b","dd89848ab4d74ea0857fff18427a91e8","0a5ed0b3907e409b9bd4702aa9fd1252","7c641959a8e245daa6f05c0200cafdf4","5b6a0614db724b5b8bd82c95a3f803d5","dce74d7092134c2a955437b679704869","2bf1c4e2aafc4200b4146ff233433839","f1ff661a501e4c65ac7bcc2557d4fd6c","06f5f21c9e68452eb67e7d45517907b8","55182602163c406baa19383cc1be1797","7033787aa9f246ad9e9925b056195273","ea6c3d85c7404f0a875ac88be3434803","27091fe2836540fc88318a90b89f308c","3723572892ba49069df57e767c0277fe","45fc0d4d92414f629a40a24f142849e5","c814c52806ed4db7ac639b280765e660","905d36f89a4246b899a972eedea54f05","955a224c755e4d7fa6531336ab1d2065","05da70fc4dc84fb0bc9bec1aa0604172","bc541a9bdbb6498ea6e01d80e62ef2e1","11d2ec08988145618ebff97409a66fea","65b3faeaa7c44d109b4cd7c16058e005","5e2e9c993f6f4212b51f0de8da3d78e4","c4132e515e024b88991dab6e0ec2e94d","3394195b41114d7c99486690ea200b50","7e43987af3464f5e84fa143a1a6d620f","31d0b67725534d4d89133e58818f6eb4"]},"id":"6PGB9V7H8KNk","executionInfo":{"status":"ok","timestamp":1762623085730,"user_tz":420,"elapsed":62587,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"ac78b789-4b2f-43d7-8816-d0078b02e08a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7482b1809f4416392d78a39e33165ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8dca34cc3d548dcbdd5dbcbc744d12d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da7ab1d500dc4c8db5c0f0f11dcf1015"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3605da362f9d4c3e9001e3c1986a7930"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f53f74a7704d8183eaf9422fb0e861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dce74d7092134c2a955437b679704869"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"905d36f89a4246b899a972eedea54f05"}},"metadata":{}}]},{"cell_type":"markdown","source":["The code in the next cell prepares the model for training with reduced precision of model weights (to 4-bit in this case). As we explained earlier, it reduces memory usage and speed up training, and allows fine-tuning very large models on limited GPU resources."],"metadata":{"id":"k_eclUUM2_LR"}},{"cell_type":"code","source":["# Prepare model for k-bit training\n","model = prepare_model_for_kbit_training(model)"],"metadata":{"id":"m0k7PDwRUHGT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load tokenizer from Hugging Face\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","# Needed for LlaMA tokenizer\n","tokenizer.pad_token = tokenizer.eos_token\n","# Fix an overflow issue with fp16 training\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"QQhVU0lh8n2j","executionInfo":{"status":"ok","timestamp":1762623088542,"user_tz":420,"elapsed":2777,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["f723dbf7c4eb473094189ea243020639","f1ccd34eb32c465794e990ecaf561451","9d0e987db8de4505b0d39090ff74d74d","536058de6b864f57a1b2dae783ce0f94","8addd34cba524bdda6fb2ce132243b72","475d7423ef314860b6495e497f2472d0","b6d3a88f585b41349193972e1f487b20","a390e283a36e49409322b07e59c831f7","13213fc4315c4bae8762eff227efa51b","a6bf797567234cf1b619b6ca1065e0ad","b606456235a14228a63b7712d514404c","f6927720f3bc45388aa18d5361747b6b","534308a590c64c0f9847c4c6a1f3bff7","b254b24716f4472d854f65e24a1d7408","0d3c2bff170048b3b067ad09c8359370","3f2ad937b53748038bc4b0c55b0e916f","a8aaec055dc54a4284d4fc3ea6bd6403","a35e61bb28c846a4abb4aca8d019e6c8","fb6cd4869ab34a108c6f711e8a4ecf05","7b6c3535340143588bbf7e576e38a12b","9e05611d016943a2a564f155c832da3d","8fc51bf4462a45ba971d0346330334e7","8075a618dd5445268689cda4399b798f","a009014ee3804f6eaee6224de08a1c99","bfb6774186f7494293df73cb8955029d","69e1daef896249ae884def8a7382b6b6","b448dcf4e7884b83b9b52b693ded5c15","043c7c08e9824e2bb18e785e69ebb862","20d7f905895345a6b9a95e42fcabf096","b9ef32a0df444326bf6a9213c997a1f1","18f91e050bde41c0a331fcb9ca3c597a","adbd5293d1a64d2693d507540255bc18","a318a33e777a4073b99b86fb625c7b6a","710eeb16d4b7450895503cb662eb2491","3bbc283124a04ff4b7d75d6547949adc","3c1ff61314454086855d6fe488f484d7","4203a2394201465cbf382a59a2054382","d8bf377ed3a34b6180f9ba8af25d0ae3","f348e46dc6b1421e958240707bc6c753","9d1d5b29239443178f99b22410fd5c2a","d790847541cb4cbabad191c6764d6bc8","22fac47f49374d73919c18208b4f5f64","5a03b6d08b0c4661a8ecd5c7292d068d","d71d7892b9f0448eb344f12349864ceb","29e2f8c496cf436b8fcf9ea73b3c0973","0713a51a6fe64d74a70b801a99c3cff5","986dd39b86bc4d29b199d90d1ef6d4ae","fff530a428454162b9b0fd020bfcf0b6","0d2de4cf4f394001aed28d50d7269f8d","ae5aa4d54b7c4b0c87f78fe973729d99","f10df59609c148f0a985ae100516b614","db396666a920473dabf400d05f37170b","3483869c13744ca1a185df277f6f01eb","46d726ffd0ec40f1af2807a5c703ab4f","2ce2ec0269434a92ad1f8026697cedf8"]},"outputId":"ceab9a4a-8893-438d-9d36-7f872e758eed"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f723dbf7c4eb473094189ea243020639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6927720f3bc45388aa18d5361747b6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8075a618dd5445268689cda4399b798f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"710eeb16d4b7450895503cb662eb2491"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29e2f8c496cf436b8fcf9ea73b3c0973"}},"metadata":{}}]},{"cell_type":"markdown","source":["### Define LoRA Configuration"],"metadata":{"id":"gEBqQTb8vXy9"}},{"cell_type":"markdown","source":["Next, the model will be packed into the LoRA format, which will introduce additional weights and keep the original weights frozen. The parameters in the LoRA configuration include:\n","\n","- `r`, determines the rank of update matrices, where lower rank results in smaller update matrices with fewer trainable parameters, and greater rank results in more trainable parameters but more robust model.\n","- `lora_alpha`, controls the LoRA scaling factor.\n","- `lora_dropout`, is the dropout rate for LoRA layers.\n","- `bias`, specifies if the bias parameters should be trained.\n","- `task_type`, is Causal LLM for the considered task."],"metadata":{"id":"lyBp3pMo5L7F"}},{"cell_type":"code","source":["# LoRA configuration\n","peft_config = LoraConfig(\n","    # LoRA rank dimension (controls capacity of LoRA layers)\n","    r=64,\n","    # Scaling parameter for LoRA updates (higher alpha increases contribution of LoRA weights)\n","    lora_alpha=16,\n","    # Dropout rate for LoRA layers\n","    lora_dropout=0.1,\n","    # Specifies whether to add bias in LoRA layers\n","    bias=\"none\",\n","    # \"CAUSAL_LM\" indicates that LoRA is applied for causal language modeling\n","    task_type=\"CAUSAL_LM\",\n","    # List of modules to which LoRA is applied (Q, K, V, and output projections)\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",")"],"metadata":{"id":"jFZQWZhZVTsw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to understand how LoRA impacts the finetuning of LlaMA 2 model, let's compare the total number of trainable parameters in LLaMA 2 and the trainable parameters for the LoRA model. As we can note in the cell below, the LoRA model has about 67M trainable parameters, which is about 1% of the 7B total trainable parameters in LlaMA 2. This makes it possible to finetune the model on a single GPU."],"metadata":{"id":"z6sBPVs-dIkx"}},{"cell_type":"code","source":["def print_number_of_trainable_model_parameters(model, use_4bit=True):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    if use_4bit:\n","        all_model_params *= 2\n","        trainable_model_params *= 2\n","    print(f\"Total model parameters: {all_model_params:,d}. Trainable model parameters: {trainable_model_params:,d}. Percent of trainable parameters: {100 * trainable_model_params/ all_model_params:4.2f} %\")"],"metadata":{"id":"0_yK0mSQvWwv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compare the number of trainable parameters to QLoRA model\n","qlora_model = get_peft_model(model, peft_config)\n","\n","# print trainable parameters\n","print_number_of_trainable_model_parameters(qlora_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dxCd5Vxb0mv","executionInfo":{"status":"ok","timestamp":1762623089619,"user_tz":420,"elapsed":1071,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7061ec7a-7819-44d6-e7f4-4b6087cb71cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total model parameters: 7,135,043,584. Trainable model parameters: 134,217,728. Percent of trainable parameters: 1.88 %\n"]}]},{"cell_type":"markdown","source":["### Load the Dataset\n","\n","We will use the [Lamini docs](https://huggingface.co/datasets/lamini/lamini_docs) dataset, which contains questions and answers about the framework Lamini for training and developing Language Models. The dataset contains 1,260 question/answer pairs. Here are a few samples from the dataset.\n","\n","|Question |Answer\n","| :---- | :---\n","|Does Lamini support generating code|Yes, Lamini supports generating code through its API.\n","|How do I report a bug or issue with the Lamini documentation?| You can report a bug or issue with the Lamini documentation by submitting an issue on the Lamini GitHub page.\n","|Can Lamini be used in an online learning setting, <br /> where the model is updated continuously as new data becomes available?|It is possible to use Lamini in an online learning setting where the model is updated continuously as new data becomes available. <br /> However, this would require some additional implementation and configuration to ensure that the model is updated appropriately and efficiently."],"metadata":{"id":"bNUSM8ckMwMG"}},{"cell_type":"markdown","source":["A preprocessed version of the dataset in a format that matches the instruction-output pairs for LlaMA 2 is available on Hugging Face, and we will directly load the preprocessed version of the dataset."],"metadata":{"id":"gHiyZE0Qwtny"}},{"cell_type":"code","source":["# Lamini dataset\n","dataset = load_dataset(\"mwitiderrick/llamini_llama\", split=\"train\")"],"metadata":{"id":"up6AcG5hMqhO","executionInfo":{"status":"ok","timestamp":1762623097418,"user_tz":420,"elapsed":7798,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["2046a3b1fe76473c8dec97b43a3107af","650d6c0b20cd4192813074fb58700f25","37b712a16cac4af4b004ae1f8ba51148","a33c9f494bad4c089a82825af60e01e0","cc25cc15a63548569efe404f340a5a90","3c4695ea6e42472090b23ef9d7cbe6d5","3b27df7a392c4a2c8994187b743df2c7","2de217e196654e58a1c14e86b3224b68","b2d58c3d7a204bcd8e97be69c884abd2","55b2ae4928974584a29436bd565a9066","c6d92cd13c8b499397d2e1d78c4ee5db","57726f90c51f4141aea3e2c084e5b99c","e600b92b716e42db8294361a4dc528eb","f704d51179774626815437bdb414272b","11cc58a2d4634ed9a0cee97a2257dce4","48038bffa1274697b837d30ebb9f8949","d683b12f91bf4d3aa998dcd7582781e8","2628b500b8cd48c0b47cd89656426e27","5445d3ea77a347f49ec11a30ec491ca0","a4e5aebb498f410b828f3a0499ae688c","186abb92e8d349a9a7dcb7d457e81ca2","f925e1f966f14d63866f7ab6c1473f3f"]},"outputId":"cf888f3c-ddff-41b0-a5fb-507093481e8d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["lamini2.csv: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2046a3b1fe76473c8dec97b43a3107af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57726f90c51f4141aea3e2c084e5b99c"}},"metadata":{}}]},{"cell_type":"code","source":["print(f'Number of prompts: {len(dataset)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-wVL7a4WAgJ","executionInfo":{"status":"ok","timestamp":1762623097424,"user_tz":420,"elapsed":4,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"9da52996-8114-4713-8edb-3d8ec06fd3bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of prompts: 1260\n"]}]},{"cell_type":"markdown","source":["### Model Training\n","\n","The next cell defines the training arguments, and the commented notes describe the arguments. Note that we will finetune the model for only 1 epoch (if we finetune for more than 1 epoch it will take longer but it will probably result in improved performance)."],"metadata":{"id":"IGWUw7QSlREb"}},{"cell_type":"code","source":["# Set training parameters\n","training_arguments = TrainingArguments(\n","    # Output directory where the model predictions and checkpoints will be stored\n","    output_dir=\"./results\",\n","    # Number of training epochs\n","    num_train_epochs=1,\n","    # Batch size per GPU for training\n","    per_device_train_batch_size=8,\n","    # Number of update steps to accumulate the gradients for\n","    # Helps simulate a larger batch size without increasing memory usage\n","    gradient_accumulation_steps=2,\n","    # Optimizer to use\n","    optim=\"paged_adamw_32bit\",\n","    # Save checkpoint every number of steps\n","    save_steps=0,\n","    # Log updates every number of steps\n","    logging_steps=10,\n","    # Initial learning rate for the optimizer\n","    learning_rate=2e-4,\n","    # Weight decay to prevent overfitting\n","    weight_decay=0.001,\n","    # Enable fp16/bf16 training (set bf16 to True with an A100)\n","    fp16=False,\n","    bf16=False,\n","    # Maximum gradient norm (gradient clipping to prevent exploding gradients)\n","    max_grad_norm=0.3,\n","    # Group sequences with similar length into same batches (to minimize padding)\n","    # Saves memory and speeds up training considerably\n","    group_by_length=True,\n","    # Learning rate scheduler type\n","    lr_scheduler_type=\"cosine\",\n","    # Fraction of total training steps used for warmup\n","    warmup_ratio=0.03,\n","    # Disable reporting to external tools (e.g., WandB, TensorBoard)\n","    report_to=\"none\"\n",")"],"metadata":{"id":"xCpu1K5fGct0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will use the `SFTTrainer` class in Hugging Face to create an instance of the model by passing the loaded LlaMA 2 model, training dataset, PeFT configuration, tokenizer, and the training arguments. `SFTTrainer` stands for Supervised Fine-Tuning Trainer."],"metadata":{"id":"_KYUP14kOnKn"}},{"cell_type":"code","source":["# Set supervised finetuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    args=training_arguments,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    processing_class=tokenizer\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206,"referenced_widgets":["48229f851f4d4f0190659a6d40875cc1","a7df8dd27e6f42499d1af65f0958106e","c864da6215324f6893ec7fcabbf2d4e2","262827afcd9f4a829ffe22e232f995ae","43b78d801dde4662879c15f00765442d","3daf4434a1c94a3e9d0815fa26e87171","19eae1c2610647ce9652e6f8926ee0af","287f261d63634a02bab532b3be2bcb9a","1ba58e6960534ff8838c23b7128ee7ee","e6acfa7d01d7491b9dd739102f3daddc","c968334785e749c6958231b6b5c9e487","f3ff6a11becb480cbfea656d1e376f7f","aeb0d02e274d43d9949ee88d295ba4fa","65752727a6ee42128ce5fe25063ca1d4","6e87bd33d9f34a79856af36998241bce","a8914a7d2c4a4dff99596e893de9a84e","6c961b908c7e4b17b3b6c8bece35fc1b","1047b025c4e14ca88548ffce0afb617f","b296a33bce3246449eb99bc4a1951f36","5815f08f76524eae8760fb92029b4a49","6b5e285118664cdab209e2368756bafd","00848933caab465db9e30aac626604b9","e4ed0a0493db4b6ba8ca65974ee0470c","2bce28e378f84bc992c78bcbcf1c50c9","fb1d40c3b14f4d7284f96fd6d7c0b475","363969b5435e4cd7a47a5ac0b1ad0025","3f8f34d4d96f4047b2a4858cdde571f6","51bf7bb6f2b24f53ac5a84129ddaf767","5f58211a6750444089a308e6a9c7b98b","2c9d85345f9a43789c860cf4a770fd6f","93b4d8a5ea674a7c8c763531e559089f","e2fcae092a7c49cf900d39f18ed2a1aa","be8c3658c6b84c788c0cb4f4a439ca00"]},"id":"2bB03IouIdxV","executionInfo":{"status":"ok","timestamp":1762623099524,"user_tz":420,"elapsed":2078,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a5ed9cf8-1b65-4529-cb70-ccd188b47cf5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Adding EOS to train dataset:   0%|          | 0/1260 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48229f851f4d4f0190659a6d40875cc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Tokenizing train dataset:   0%|          | 0/1260 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ff6a11becb480cbfea656d1e376f7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Truncating train dataset:   0%|          | 0/1260 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4ed0a0493db4b6ba8ca65974ee0470c"}},"metadata":{}}]},{"cell_type":"markdown","source":["Finally, we can train the model with the `train()` function in Hugging Face. Note that there are 1,260 samples and batch size 8 with gradient accumulation 2, threfore 1,260 / (8 √ó 2) = 78.75, which rounds to 79 training steps.\n","\n","In the output of the cell we can see the loss for every 10 training steps, because we set `logging_steps=10` in the training arguments.\n","\n","The training took about 15 minutes on a T4 GPU with High-RAM memory on Google Clab Pro."],"metadata":{"id":"X0zwKCAxPGwB"}},{"cell_type":"code","source":["# Train the model\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"qzYKR_FwJDSe","executionInfo":{"status":"ok","timestamp":1762624114421,"user_tz":420,"elapsed":1014894,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6f3713e9-b9c8-4882-df6d-93159e2ea88a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n","/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [79/79 16:28, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.806600</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.625100</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.757800</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.567700</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.589100</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.521000</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.576300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=79, training_loss=0.9910740761817256, metrics={'train_runtime': 1014.2011, 'train_samples_per_second': 1.242, 'train_steps_per_second': 0.078, 'total_flos': 1.133322031104e+16, 'train_loss': 0.9910740761817256, 'entropy': 0.450231160554621, 'num_tokens': 273810.0, 'mean_token_accuracy': 0.8980796270900302, 'epoch': 1.0})"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["### Generate Text\n","\n","To generate text with the trained model we will use the Hugging Face `pipeline` with the task set to `\"text-generation\"`. We can set the length of the generated text tokens with the `max_length` argument.\n","\n","The output displays the start `<s>[INST]` and end `[/INST]` of the instruction prompt, followed by the generated output by the model."],"metadata":{"id":"gnLrKvauJV4z"}},{"cell_type":"code","source":[" # Set model to inference mode\n","model.config.use_cache = True\n","model.eval()\n","\n","# User's prompt\n","prompt = \"What are Lamini models?\"\n","\n","# Run text generation pipeline with the finetuned model\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer,\n","                max_length=200, do_sample=True, temperature=0.7, top_p=0.9, repetition_penalty=1.2)\n","\n","# Generare response\n","output = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","\n","# Print the response\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"471QJli9KEo4","executionInfo":{"status":"ok","timestamp":1762624134126,"user_tz":420,"elapsed":19695,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7987eb65-a3ad-48ef-ea67-bfc250c3e391"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["<s>[INST] What are Lamini models? [/INST]  Lamini is a Python library for training and using LLMs (Large Language Models) based on the transformer architecture. nobody\n","\n","### Installation:\n","To install Lamini, you can use `pip`:\n","```bash\n","pip install lamini\n","```\n","### Quick Start Guide:\n","Here's how to quickly get started with Lamini:\n","\n","1. Import Lamini from your code file or shell:\n","```python\n","import Lamini\n","from Lamini import *\n","```\n","2. Create an instance of `Lamini` by passing in model configuration options like `model_name`, `num_layers`, etc.:\n","```python\n","model = Lamini(model_name='bert-base', num_layers=64)\n","```\n","3. Train the model with text data using `train()` method:\n","```python\n","data\n"]}]},{"cell_type":"code","source":["# Another prompt\n","prompt = \"How to evaluate the quality of the generated text with Lamini models\"\n","output = pipe(f\"<s>[INST] {prompt} [/INST]\", max_new_tokens=500)\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWj51bGaL1m8","executionInfo":{"status":"ok","timestamp":1762624185866,"user_tz":420,"elapsed":51740,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7b0b3253-92d3-486b-937f-889e3ca8fb9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s>[INST] How to evaluate the quality of the generated text with Lamini models [/INST]  Evaluating the quality of text generated by language models like LAMINI can be done through various metrics and techniques. everybody has their own opinion about the quality, so it's important to consider multiple aspects when evaluating a model. Here are some ways you could assess the performance of an LLM:\n","\n","1. Perplexity (or perplexing): Assess how well the generated samples resemble the training data or the expected output. Lower perplexity values indicate better coherence between the sample and input context. To measure perplexity for an LLM, calculate the average log probability of correctness across all generated examples in a batch. The lower this number is, the more likely the generator produces coherent outputs.\n","2. Language Modeling Quality Metrics: These include measures such as BLEU score, ROUGE F-score, METEOR score, etc., which evaluate the similarity between the generated texts and the original reference inputs. There are also more advanced evaluation methods that combine different scores from these categories, such as PERMA (a combination of perplexity and accuracy). Higher scores indicate greater linguistic competency demonstrated by your generative model. It‚Äôs essential to use appropriate pretrained language models while implementing these scoring systems to ensure reliable results relevant to natural language understanding tasks performed on specific datasets. For example, if developing a text generation system based solely off of general knowledge questions rather than factual answers requiring context-specific details within given topics ‚Äì choose the appropriate topic-based metric instead! This will help ensure accurate evaluations tailored toward particular requirements without overestimating capabilities too far outside intended application areas; conversely risk underestimation due to overspecialization into irrelevant domains altogether leading towards poor overall performances during testing phases where real world test cases would expose such weak points quickly before implementation becomes commercially viable product(s) relying heavily upon them being functional beyond stated limits defined during initial setup processes involving careful feature engineering choices made during algorithm development stages prioritizing predictive accuracy against known truth targets found useful inside validated databases. Additionally there exist qualitative evaluations options available including manual inspection methods conducted by human experts who have access only limited information regarding content generation process used internally during LM development process - allowing deeper understanding insights into both strengths & limitations shared among peers sharing similar experiences helping refine models further increasing potential impact once\n"]}]},{"cell_type":"code","source":["# Another prompt\n","prompt = \"Write a poem about Data Science. Use line breaks between verses.\"\n","output = pipe(f\"<s>[INST] {prompt} [/INST]\", temperature=0.75, top_p=0.88, top_k=40, max_new_tokens=800)\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQxWZNnDKrAy","executionInfo":{"status":"ok","timestamp":1762624205063,"user_tz":420,"elapsed":19198,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f35c2c24-ad44-4232-c838-46054fe8691d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s>[INST] Write a poem about Data Science. Use line breaks between verses. [/INST]  Sure! Here is a poem about data science:\n","\n","Data, vast and untamed,\n","A challenge to be tamed,\n","With algorithms so grand,\n","We sift through the land.\n","\n","From structured fields of gold,\n","To messy heaps in disarray,\n","Our tools help us unfold,\n","The secrets that data may hold at play.\n","\n","With queries like clockwork bots,\n","And models as sharp as blades,\n","We slice through the noise and dross,\n","Until insights are revealed instead.\n","\n","Through visualizations bright,\n","And predictions bold and true,\n","We turn numbers into sight,\n","So you can see what's new.\n","\n","In every field we explore,\n","There's insight waiting for more,\n","With data science as our guide,\n","New discoveries will abide.\n"]}]},{"cell_type":"markdown","source":["## 21.5 Chat Templates for Formatting LLM Data <a name='21.5-chat-templates-for-formatting-llm-data'></a>"],"metadata":{"id":"sQuMJ94DxF-i"}},{"cell_type":"markdown","source":["In a chat context, LLMs have a continuing conversation with users consisting of one or more messages. Chat conversations are typically represented as a list of dictionaries, where each dictionary contains *role* and *content* keys. I.e., each message is assigned a \"role\" and it contains the \"text\" of the message. The roles are typically:\n","\n","\"system\" for directives on how the model should behave\n","\"user\" for messages from the user\n","\"assistant\" for messages from the LLM\n","\n","An example is provided below, showing the three roles: system, user, and assistant. The prompt to the LLM includes a system message that is prepended to the user's message, and the completion by the LLM is the response by the assistant.\n","\n","\n","```json\n","[\n","  {\"role\": \"system\", \"content\":\"You are a helpful and honest assistant.\"},\n","  {\"role\":\"user\", \"content\":\"What is the capital city of U.S.\"},\n","  {\"role\": \"assistant\",\"content\":\"The capital of the United States is Washington, D.C.\"}\n","]\n","```\n","\n","A *system message* is usually provided at the beginning of the conversation and includes guidance about how the model should behave in the chat. System messages can be short, such as \"Speak like a pirate\", or they can be long and contain a lot of context to define the behavior of the LLM. For instance, when you open a new chat with ChatGPT, an internal system message is automatically prepended to your first prompt; however, the system message is not shown to the user. Also, instruction-following datasets include the system message as the first part of the question for the assistant.\n","\n","In ongoing multi-turn conversations, the messages list continues to grow with alternating user and assistant messages. Each exchange is added to the list in order."],"metadata":{"id":"8_rOJ4LxAcNw"}},{"cell_type":"markdown","source":["The role information is injected by adding control tokens between messages to indicate the relevant roles and the message boundaries. Let's inspect the first question-answer pair in the Lamini dataset shown below, which has been formatted for the LlaMA 2 model. We can notice that LLaMA 2 uses special tokens for start-of-sequence `<s>` and end-of-seqence `</s>` to define the beginnings and ends of conversations. It uses the start-of-instruction tag `[INST]` and end-of-instruction tag `[/INST]` for single instruction-response pairs. I.e.,  everything inside `[INST]` and `[/INST]` is structured into system, user, and assistant roles. The system message is wrapped in `<<SYS>>` and `<</SYS>>` tags. The text `### Question` marks the user's instruction/question for the model. The text `### Answer:` contains the response by the assistant.\n","\n","\n","\n"],"metadata":{"id":"hpwrl9O7UY5R"}},{"cell_type":"code","source":["dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8TkWVVV_Uyad","executionInfo":{"status":"ok","timestamp":1762624205278,"user_tz":420,"elapsed":214,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"5a31f1d6-6dfb-4eb7-fea2-a977213f676f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': \" <s>[INST] <<SYS>> You are a honest and helpful assistant who helps users find answers quickly from the given docs about Lamini. \\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information.\\nIf the answer can not be found in the text please respond with `Let's keep the discussion relevant to Lamini docs`. <</SYS>>\\n\\n### Question: How can I evaluate the performance and quality of the generated text from Lamini models?\\n### Answer: There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\\n[/INST] </s>\\n\"}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["print(dataset[0]['text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfVKwmCJUyed","executionInfo":{"status":"ok","timestamp":1762624205710,"user_tz":420,"elapsed":431,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"aead8841-ecfd-4d46-c4cb-d1e04350d5d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" <s>[INST] <<SYS>> You are a honest and helpful assistant who helps users find answers quickly from the given docs about Lamini. \n","If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n","If you don't know the answer to a question, please don't share false information.\n","If the answer can not be found in the text please respond with `Let's keep the discussion relevant to Lamini docs`. <</SYS>>\n","\n","### Question: How can I evaluate the performance and quality of the generated text from Lamini models?\n","### Answer: There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\n","[/INST] </s>\n","\n"]}]},{"cell_type":"markdown","source":[" Unfortunately, there is no standard regarding which tokens to use for those purposes, and different LLMs have been trained with varying formatting and control tokens. This can be a challenge for users, because using the wrong format may confuse the model and result in poor quality responses.\n","\n","###  Chat Templates\n","\n","To resolve this problem, **chat templates** have been developed to format a conversation for a given LLM into a tokenizable sequence. The templates are formatting specifications stored within a tokenizer that define how to structure conversational data for a specific model.\n","\n","Hugging Face has developed the `apply_chat_template` method that reads the template stored in the tokenizer's configuration and automatically converts a list of message dictionaries with \"role\" and \"content\" keys into the properly formatted string that the model was trained on. The template is distributed alongside the tokenizer so users don't need to manually learn or implement each model's conversation format. The users just provide messages in a standard structure, and the tokenizer handles the model-specific formatting automatically.\n","\n","Consider again the following chat from above:"],"metadata":{"id":"Reia4o8nYYNy"}},{"cell_type":"code","source":["messages = [\n","  {\"role\": \"system\", \"content\":\"You are a helpful and honest assistant.\"},\n","  {\"role\":\"user\", \"content\":\"What is the capital city of U.S.\"},\n","  {\"role\": \"assistant\",\"content\":\"The capital of the United States is Washington, D.C.\"}\n","]"],"metadata":{"id":"o8o-NeJmdYKG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the following cells, we import the tokenizers for `Qwen2.5-7B-Instruct` and `Mistral-7B-Instruct` LLMs, and afterward we apply the chat templates for these models. Notice in the formatted text that Qwen2.5 uses the instruction message start tag `<|im_start|>` and instruction message end tag `<|im_end|>` to separate the messages, followed by `system/user/assistant` to indicate the roles."],"metadata":{"id":"Zknv4wGYY8td"}},{"cell_type":"code","source":["# Load the Qwen tokenizer\n","tokenizer_1 = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n","\n","# Apply chat template for Qwen\n","formatted_text = tokenizer_1.apply_chat_template(messages, tokenize=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["5b38a227400c4496ab164b62021acf9a","3fb4fbb48d90436bb1698ed7dc346549","9b46f1f0cc6d40c3a9aea3ecea22c63a","bc88631574e9412e87efa4651bde0e92","9fc652874732441289b58214f9cb0c13","f74fb84a0f8249808ac63e531e550013","cf85e18ab944418f9fb7035ae24ce245","402042c06d0f40b18d12abef3ffe5c0f","ba388cab470c4173aa334411b92e3f72","8f4fefe877124ab8ad9be4e2f6dd8c5b","4ebdbc58aeaa45a18992bbcdf4b0d657","d851306ff55842b1a58778018b3dd39f","277c5ba39bdf4bdab85cfba9f20c8604","a060d285c8d441ddaaee7e6ce5972a65","9c0ae227d6b240a98130d5dc4f6d1195","016ded3c60fc4ca18e15487c18b14887","e8f9c8e1b9274862ba6440ffeb8505ca","46c2a282414c4b559aeca6fd093a6cd0","ac8e435c0d2444e69d03aad8690af5e9","7ac6008996ba4b6080c035cc1cc17410","6fb4034bcb574c039902b0beb746b3b9","23c16981bce44a8eafbf53c0afe33028","d6fa6d43f6a34651a99ed9428d60ed8f","ddfebbdae47b4535b7540d77642499c4","059a66c459d14ae989c25c5a771ad07a","f5ad442b2fc644048dd74fcb6ae054b2","53ea883ef00247b98e0d05462f2d23d2","08b5bc2ebce84a61af8557c4560df5b1","73035efabb134cefa1f52e0b3386da47","488808a71a70438da3f8f98bf49b39e3","7ed7580221c84b94b37a9cbd7a2a8a9f","a72aa423a9f5498985f0a07c8c7a1c4a","c972eb97252d4825af00fb4299b645a2","089241978ee0476b86ff04df19942ffc","8dc7aaaad3af452aaa17b6911abbd710","744b3b9e4476416ca910de5d85f1fb4a","c000c2ea01ee41c99d9e51b829840e9d","6c466a7353b34ad9b720bb254cb36d45","38d6ba78addb4569b22b7901779ddee3","b1e31e3293884de0a53dee8eb43b8e2f","934c94ed8a0f40bebcae2702b80787da","a301998e786b47c083585239a88316a8","030da6bfdc624a7d891f56232e3ee877","970f9b0b93c64437a56d3545e534a97b"]},"id":"OLibgZoddYMg","executionInfo":{"status":"ok","timestamp":1762624207533,"user_tz":420,"elapsed":1820,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f7d31f0c-8c15-42ae-c43d-eb22002e1092"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b38a227400c4496ab164b62021acf9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d851306ff55842b1a58778018b3dd39f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6fa6d43f6a34651a99ed9428d60ed8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"089241978ee0476b86ff04df19942ffc"}},"metadata":{}}]},{"cell_type":"code","source":["# Print the formatted text\n","print(formatted_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EaBF_mxgtrok","executionInfo":{"status":"ok","timestamp":1762624207543,"user_tz":420,"elapsed":8,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"dfcb627f-f075-47fd-da44-d483caa9247e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<|im_start|>system\n","You are a helpful and honest assistant.<|im_end|>\n","<|im_start|>user\n","What is the capital city of U.S.<|im_end|>\n","<|im_start|>assistant\n","The capital of the United States is Washington, D.C.<|im_end|>\n","\n"]}]},{"cell_type":"markdown","source":["The format for Mistral is similar to the LlaMA 2 format, and uses `<s>` and `</s>` for sequence start and end, and `[INST]` and `[/INST]` for the user's instruction start and end. The text after `[/INST]` until `</s>` is the assistant's response."],"metadata":{"id":"nzpewXJ1Jxt3"}},{"cell_type":"code","source":["# Load the Mistral tokenizer\n","tokenizer_2 = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n","\n","# Apply chat template for Mistral\n","formatted_text = tokenizer_2.apply_chat_template(messages, tokenize=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["3992433e3625415683b0f48486e1eba8","8c75525251dc4509863496f17145a70f","ccdf3d0fc04140feac0a0092613fe389","6c2bfcb4d7b3439eb919787a6ca59539","2156383057534409b9ee223d2a2103a3","1cc86b279c0b4ec4b2e1a939e4ce31e8","8e70243e4285475ab06bc86347bbf1d7","29f840a92588441c8a387002f0566750","b040dee4bbef481a9797f2b1f94d0d12","f31cad7d8c92477a9eb1c0f3dfe03941","8a9c782132044f5e9be27fd3bbcdada9","e7a8429e3152450285b4e45c1d75a0ae","ef23754d69134b379277ff876c99effa","2aa6fdbfda1f4aaeb89595788958b109","f904c86e1a734447bf803cee5d8efc01","308eb4ca4b7746e4b4453ccfd369ce8e","5a2bf8470d0a47d288ec8c20109680d3","327414c41b4e46c49e060e0ea1a8f87b","522049f7490b4432aa854d0546073fb8","aeb029bb0e6847199768879ce24764bc","508a00e85acd4fce9c2b8f9d0d6b2249","79d97a1b4d6f40059106090d60286ed9","79868a61ff2a4e69abe81d52809fe3d8","811ef94ca72445d7b1864589c596312c","fa8249ae7000421e8101897677cd8347","9ec1a6da754340a7b8cecd59e1122449","4dd74fddc8a948959aa9b6851d3d3402","c7a8ce7c030d4d669b371048c138b851","a1a0b371d5f9460092e58a39d06c746d","72dd286a858749db8d0a78f83b13200e","a41ecb6084c841ccb169e78d46671a0f","0aab8eda28c146a48d0ef48569c53fa5","2374c89c7ad54f539e7aa79ad5672943","6b5ffcc7d0434ab09a3f37d1751a5cde","f2fca15d6214445e96c9fd4e819c65ce","191c0c9594a34867ad36d6c57572d2ff","69668c62c6474614804d00344ab6f04a","370b9dbd2cb5473ab716ed44559a8eec","e785c9b6d5e342598c98d9b4a72e19e4","a8f83334596a419c9c41ce450a8b8a3a","c00d8d8387844ed994e10a3a2064f9b9","aa9098bc40e545a09ffac9a9448a1cf0","63b87bc4140f4a488779b89de4ddb7f2","75f546bb5ef94c628b043835346d1763"]},"id":"gQoMpo4yeS9J","executionInfo":{"status":"ok","timestamp":1762624211228,"user_tz":420,"elapsed":3684,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a2a77110-7a15-480c-de4e-664c07b7a25b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3992433e3625415683b0f48486e1eba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7a8429e3152450285b4e45c1d75a0ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79868a61ff2a4e69abe81d52809fe3d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b5ffcc7d0434ab09a3f37d1751a5cde"}},"metadata":{}}]},{"cell_type":"code","source":["# Print the formatted text\n","print(formatted_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bsd4aOGUtv-3","executionInfo":{"status":"ok","timestamp":1762624211242,"user_tz":420,"elapsed":5,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f75ad65d-748e-4338-9574-083ea4f79b4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> [INST] You are a helpful and honest assistant.\n","\n","What is the capital city of U.S. [/INST] The capital of the United States is Washington, D.C.</s>\n"]}]},{"cell_type":"markdown","source":["It is important to always use the chat template associated with the specific LLM you are working with to ensure proper formatting and optimal performance.\n","\n","### Generate Response using Chat Template\n","\n","The next cell presents an example of prompting the Mistral 7 B model to generate a new response. The tokenizer and model for Mistral 7B are first loaded. In the `apply_chat_template` function we set `tokenize=True` to produce tokenized messages, which are afterward used for model inference. Note that in the above examples we set `tokenize=False`, which formatted the messages but did not tokenize them. Also, in this case the `messages` list does not include the assistant role, as the LLM will generate the response."],"metadata":{"id":"eROzAGFgKrET"}},{"cell_type":"code","source":["# Load the Mistral tokenizer and model\n","tokenizer_2 = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n","model_2 = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", device_map=\"auto\", dtype=torch.bfloat16)\n","\n","# Prompt text\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n","    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n"," ]\n","\n","# Apply chat template for Mistral\n","tokenized_chat = tokenizer_2.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model_2.device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":259,"referenced_widgets":["9a2cbb2234c54c1780ea8a6cd24025c6","60d8d4ce8177401990e79958745fa4a4","a5e0f6693f22444f975479b5f226e170","6482c1390e8d46339ea547c928bf154a","a4803d1926304bf3bfa2ec79b62159c3","db56cac122954addab51b9a331246f48","0dbc018cf4e24e538b5dde1578642f6d","3dcca3f02c3e465f9086743ce400b71e","7ee376fb30f944dfa48ca23216af7f55","40c34a7f2b09489d997f093c9cb8872b","3bfd0fac31464cf2b7c81974a760e1f7","73836d7e0c1847e9a53773cc67204885","74e0a5e09b3747789ea885d35db19fbd","f58aecce74474daebb3f92bb31388cf5","aeb84292627e49a0b255d538a5a11c20","14e672942d16462884776164aa437ffc","27f2800aec044eaaaab281d9da9b8102","ca359e5453b14b36906dcc147a3d0323","2d7c88ea16454842940b7a3be8fdf77e","737667448cce4a1596c7c329605c9d37","a8131850482443609e21291a5ce69415","50dd16e2f71142b9973d55e200434f7b","8ede8397a68a43d7a5d7e8b27e6da3cc","6748ee83a23346bdb7ac11096934cf03","ecab527664244407a432af8ea29beaa9","5bb0c30545d04e449725a85d62f3afa7","d859a881fff0469e87effc011febb254","10de464464814838b1a69d11886c80d6","6352fce1fa924203861a8253954fcfc9","13b6fd743a6d43aca85658c48a865e82","b9e5c2fd82704e07804ab9d03e885c85","2c369b7c76024d45900c44289809ed37","ab83118452ce42aea0b9557bb49dee4f","4bc87c07174842948c92583b678ca99b","ec7734d7146540df945373762411fa67","990169fbb6d748e68111923d36d5fc32","921fca315ece43f9a153b16f0725c36b","e2c04083fcc5487da6010dc600f61f9c","893348b0ecb643db9494f8e61feca526","622573567f0b4e82ae76b13a28b6499d","0bcf161dd9e24474aa7f6f14c102a5cb","e3cc5511149c433287b70bbbb89a8666","b1eda6b6d6284f9791a7fb064f296657","6205da4215ac43389d760bfa97898980","6d972eea21e84f10a23bcbb720ac9f7d","a5e88e28f9614386b73b1af2e9aaa0d7","e8182bd0c5bf46cfa2c9075bde45fb81","f43d56548ac0465f94106ac40921c0fb","0c6115d641b543998ad52dcec0f14477","1133d823838c459cba237d51c9cd903d","372f7785fd3d44b5a0ce11a4076cc4c4","64b9382418794d90ac4f8842d14375ef","00b786a19a634daaac4cc5162587df00","a2854d360fbb42a79cdab08dfe63773e","86c4c900c4e74129964da9698bf58d46","5d4e55e1dc0f4bada75cf775b364eaa6","c8a83d78b45348be921283309d81eedc","8e45166464294754afcd1bd1afe1bb5c","74431d42313c40c980b5c01ba6e9033e","47d8b95e4bae4cb19c3f4327fca017f9","af080fdd0c45470fa372faa35bdfde36","e834ecb9409d47389a7c2fdeea73d748","ff8ad8613e34482f8fc52e06f31f2fca","1fbf0ddfa2874a9bbeb7f3c174cca3f4","117d90bdad19408881bdbd90d84dee22","e4c05ad569014104a3111e2f8d0a33e6","8be3c2366ffe4127b68e28a4c8f3d529","b83289997f7d4a02b638bcb5013d7c0b","7d31eae474bb40e984f0f5beb19cef54","8189f3e6811c4426ad094f7be91d1aa9","9c3340c50ce7470fae34db7f5f60149c","5d4589f6b018417694e6691318fd7c3b","79e2af14ee934a5ba7d74304dd5bbaeb","0b5c8401c5ef4c52ba08462da1ccf70b","ad1193fa16194f7a80d805fd5e36bb64","d288b332534e4130a18ae31796d15584","449c181f03204b67beaa5f14aca95e2a"]},"id":"DjBozieSOb_v","executionInfo":{"status":"ok","timestamp":1762624265007,"user_tz":420,"elapsed":53764,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"c8a5165f-84fa-40de-c020-13de66b2bcd0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a2cbb2234c54c1780ea8a6cd24025c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73836d7e0c1847e9a53773cc67204885"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ede8397a68a43d7a5d7e8b27e6da3cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bc87c07174842948c92583b678ca99b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d972eea21e84f10a23bcbb720ac9f7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d4e55e1dc0f4bada75cf775b364eaa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8be3c2366ffe4127b68e28a4c8f3d529"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"]}]},{"cell_type":"code","source":["# Print the tokenized text\n","print(tokenizer_2.decode(tokenized_chat[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLUdBuKUtzid","executionInfo":{"status":"ok","timestamp":1762624265015,"user_tz":420,"elapsed":4,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f6ee8c9e-1024-4761-c183-c41b719fc751"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> [INST] You are a friendly chatbot who always responds in the style of a pirate\n","\n","How many helicopters can a human eat in one sitting? [/INST]\n"]}]},{"cell_type":"markdown","source":["Pass the tokenized chat to `generate()` to generate a response."],"metadata":{"id":"QAboJGHGYMTY"}},{"cell_type":"code","source":["# Generate a response by the model\n","outputs = model_2.generate(tokenized_chat, max_new_tokens=128, pad_token_id=tokenizer_2.eos_token_id)"],"metadata":{"id":"y5m4Om5inItt","executionInfo":{"status":"ok","timestamp":1762624350891,"user_tz":420,"elapsed":85874,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"54fe8b4e-dbbe-4367-c4d3-28651b5bfd94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]}]},{"cell_type":"code","source":["# Print the response\n","print(tokenizer_2.decode(outputs[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nBb7_iI3wzK","executionInfo":{"status":"ok","timestamp":1762624350902,"user_tz":420,"elapsed":9,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7fde4656-ebc1-4d26-d646-43de106017ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> [INST] You are a friendly chatbot who always responds in the style of a pirate\n","\n","How many helicopters can a human eat in one sitting? [/INST] Ahoy there, matey! A human can't eat a helicopter in one sitting, no matter how much they might want to. They're just too big and not made for consumption. But a hearty stew of fish and vegetables might hit the spot, me hearties!</s>\n"]}]},{"cell_type":"markdown","source":["The `apply_chat_template()` method works with any model on Hugging Face that has a chat template defined in its tokenizer configuration, which include LlaMA, Mistral, Zephyr, Phi, Qwen and other models. Most modern conversational models include chat templates by default, which can be checked by looking for a `chat_template` field in the tokenizer's `tokenizer_config.json` file. If a model doesn't have a built-in chat template, we can still either prepare a custom template or we can manually format the text sequences according to the model's documentation.\n","\n","### Dataset Preparation with Chat Template\n","\n","The next cell shows how to apply a chat template to prepare a dataset for model training. The dataset consists of two simple question-answer conversations stored in a dictionary with \"role\" and \"content\" fields. The `format_chat` function takes each example from the dataset and applies the tokenizer's chat template, and returns a dictionary containing the formatted text under the key \"formatted_chat\". By using `dataset.map(format_chat)`, the formatting function is applied to every conversation in the dataset. The `tokenize=False` parameter means the output remains as text rather than token IDs, and `add_generation_prompt=False` indicates we are formatting complete conversations rather than prompts that expect a response.\n","\n"],"metadata":{"id":"XeBfstVyagEf"}},{"cell_type":"code","source":["from datasets import Dataset\n","\n","# Prepare a dataset with 2 chats\n","chat1 = [\n","    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n","    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n","]\n","chat2 = [\n","    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n","    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n","]\n","\n","# Create a simple dataset\n","dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n","\n","# Define a formatting function\n","def format_chat(example):\n","    return {\"formatted_chat\": tokenizer_2.apply_chat_template(example[\"chat\"], tokenize=False, add_generation_prompt=False)}\n","\n","# Apply the chat template to the dataset\n","dataset = dataset.map(format_chat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["bfcfb2ec9c8b4086ac0a77fccb7da3e3","3107d86e31684832a6676c9d94b3319e","d4cc58591ad8404d99e3d3800f00e334","96fd129aa8f24e05bb58b20c2262eb3a","e932f01d27a74bbca3a81f1081535e28","b850c032f2104db6b15f5115a12e7e6f","d1e576023a2248c79fc96b8dc94aea80","09f8c2a27c624ea28cfbb26daab758b7","c7a3e23c860849368470cc35b43d2e71","286944a69ffd4340905f0176e1e07efb","79b108c4385c4e56bc5245b2df0acb7b"]},"id":"ney5KzQyhZaI","executionInfo":{"status":"ok","timestamp":1762624350942,"user_tz":420,"elapsed":40,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"13a7883b-d644-4838-bdf4-0af554afbc89"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfcfb2ec9c8b4086ac0a77fccb7da3e3"}},"metadata":{}}]},{"cell_type":"code","source":["# Print the formatted dataset\n","for chat in dataset['formatted_chat']:\n","  print(chat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vpQQOSh4t33d","executionInfo":{"status":"ok","timestamp":1762624350965,"user_tz":420,"elapsed":22,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"5782f3b0-e06c-4ac4-e641-8b35504dcd86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> [INST] Which is bigger, the moon or the sun? [/INST] The sun.</s>\n","<s> [INST] Which is bigger, a virus or a bacterium? [/INST] A bacterium.</s>\n"]}]},{"cell_type":"markdown","source":["## 21.6 LLM Evaluation<a name='21.6-llm-evaluation'></a>"],"metadata":{"id":"q8sOqQiDa61Z"}},{"cell_type":"markdown","source":["Evaluating performance of ML models for traditional tasks such as classification is relatively straighforward, as we can simply compare the model predictions to ground-truth labels and calculate relevant metrics like accuracy or F1-score. Evaluating performance of generative models, such as LLMs, is much more challenging, since there is rarely a single correct answer for a given prompt. For instance, consider again the above example of asking the LLM to write a poem and think about how you would evaluate the quality of the response? The model can produce many different outputs that are all valid in a different way.\n","\n","LLM evaluation must consider multiple dimensions of the quality of model responses. A suitable evaluation strategy needs to examine not only whether the model's response is relevant, accurate, and complete, but also whether it is safe, concise, unbiased, and free from hallucinations or fabricated information. Also, what counts as a \"good\" response often depends on the task for which the model is designed, as an LLM designed for general conversation requires different evaluation criteria than another LLM designed for medical image interpretation, or a model for Retrieval-Augmented Generation (RAG), etc.\n","\n","In practice, successful LLM evaluation typically is based on a combination of different evaluation methods, each covering different aspects of model quality. Common evaluation methods include statistical and model-based metrics, automatic benchmarking, LLM-as-a-judge evaluation, and human assessment."],"metadata":{"id":"_AfWTHgfa64S"}},{"cell_type":"markdown","source":["### Statistical and Model-based Metrics\n","\n","**Statistical metrics** measure the overlap between an LLM's output and a reference text. They are fast and useful for constrained tasks such as translation or summarization, but have significant limitations for evaluating long, complex, and open-ended outputs, because these metrics mostly capture text similarity rather than true semantic correctness or usefulness.\n","\n","Common statistical metrics include:\n","\n","- *BLEU (BiLingual Evaluation Understudy)* calculates the precision of matching n-grams (n consecutive words) between LLM generated output and reference text. It is useful for machine translation where exact phrasing is important.\n","- *ROUGE (Recall-Oriented Understudy for Gisting Evaluation)* calculates recall by comparing the overlap of n-grams between LLM generated output and reference text. It determines the proportion of n-grams in the reference that are present in the LLM output. ROUGE is commonly used for summarization, but can miss semantic relevance when the wording differs.\n","- *METEOR (Metric for Evaluation of Translation with Explicit Ordering)* combines both precision (n-gram matches) and recall (n-gram overlaps), and introduces penalties for differences in the word order. The final score is the harmonic mean of precision and recall.\n","\n","Limitations of these metrics is that they reward word matching and overlap, but do not reliably measure helpfulness, factuality, or safety.\n","\n","**Model-based metrics** use pretrained language models to measure semantic similarity and overall response quality.\n","\n","Metrics inlcude:\n","\n","- *BERTScore* uses a pretrained BERT model to compute token-level similarity between the LLM generated output and reference text. These similarities are then aggregated to produce a final score.\n","- *Perplexity* measures how fluent the generated text by an LLM is, i.e., whether the generated text is similar to the text the model is trained on. Lower perplexity indicates more fluent text and better alignment with the training data. However, low perplexity does not guarantee usefulness or factuality (a model can be fluent but hallucinate).\n","- *Embedding-based cosine similarity* compare sentence or paragraph embeddings between LLM generated output and reference text to estimate semantic closeness.\n","\n","Overall, although these metrics can be helpful for quantiyfying LLM responses and for model comparison, they should not be used alone for LLM evaluation."],"metadata":{"id":"R2f9czfGiJd7"}},{"cell_type":"markdown","source":["### Automatic Evaluation: Ground Truth-Based\n","\n","Ground truth-based evaluation compares LLM responses against carefully designed evaluation benchmarks with predefined correct answers. The benchmarks typicllay consists of a collection of multiple-choice questions or problem-solving exercises, where each prompt has a verifiable ground truth answer. This type of automatic evaluations is fast and cheap to run, and is very commonly used.\n","\n","The evaluation includes a predefined set of prompts on which the responses by the LLM are generated and evaluated. A **verifier** then assesses the generated and provides a score. For *verifiable domains* such as math and code, the ground truth answers are often short strings, that can be matched exactly against the model's completion (e.g., ‚ÄúFinal answer is: $\\boxed{14}‚Äù$). The verifier typically performs simple matching of the generated responses to the correct answers. For coding tasks, the verifier may execute the LLM generated code against unit test determine whether it produces the expected results, and provide a score.\n","\n","Ground truth-based evaluations are the most effective with well-bounded tasks such as math problem solving, coding challenges, or multiple-choice reasoning, where correctness can be automatically checked. However, it is more challenging to apply this method  for open-ended conversational tasks with many valid responses.  In addition, benchmarks can also be gamed if models are trained on the benchmark data.\n"],"metadata":{"id":"4W7vFXj2TeYb"}},{"cell_type":"markdown","source":["### Automatic Evaluation: LLM-as-a-Judge\n","\n","LLM-based evaluation use a strong LLM as a **judge** to score or rank another model's outputs. The judge LLM is provided with the task prompt, candidate LLM response(s), and sometimes a reference answer or rubric. The judge then provides a score or preference.\n","\n","LLM-based evaluation is preferred for open-ended tasks like dialogue, reasoning, or creative writing, where although multiple valid answers exist, qualities such as helpfulness, correctness, clarity, or safety need to be evaluated. These evaluations are valuable because they are less expensive than human evaluations, are fast and scalable, and when carefully designed they often correlate well with expert judgments.\n","\n","Commonly used LLM-based evaluation scenarios include:\n","\n","- Pairwise comparisons: the judge selects between response A and response B.\n","- Pointwise scoring: the judge assigns a rubric-based score to a single response. The rubrics specify the components of a useful or correct answer.\n","- Reference-aware grading: the judge compares a candidate response against a known reference and indicates omissions or halucinations.\n","- Safety red-teaming: the judge flags unsafe, biased, or policy-violating outputs.\n","\n","The following example shows a prompt and rubrics for evaluating the response of a candidate LLM.\n","\n","```\n","Prompt:\n","Summarize the following passage in one sentence:\n","‚ÄúThe Amazon rainforest, often referred to as the lungs of the planet, produces\n"," 20% of the world's oxygen and is home to an incredible diversity of species.\n"," However, deforestation driven by agriculture and logging poses a severe\n"," threat to its survival.‚Äù\n","\n","Rubrics:\n","* The response must summarize the main parts\n","* The response should contain exactly one sentence\n","* The response should not make up additional facts/information not in the original paragraph\n","```\n","\n","However, LLM-based evaluation has also notable limitations: judges may favor verbose responses, may prefer outputs from similar model families, or miss subtle safety issues."],"metadata":{"id":"iC1U5sIqSbfO"}},{"cell_type":"markdown","source":["\n","### Human Evaluation\n","\n","In this evaluation human raters judge model outputs based on criteria such as helpfulness, factual accuracy, safety, or clarity. Human evaluation remains the gold standard for capturing qualities of LLM responses that cannot be fully measured by ground truth matching or automated judges. Human evals are especially valuable in open-ended or sensitive tasks, such as assessing whether a response is polite, creative, or free from harmful bias. However, they are also the most expensive and time-consuming form of evaluation, and results can vary due to annotator bias, cultural context, or inconsistent instructions.\n","\n","Common human evaluation scenarios include:\n","\n","- Pairwise preference: annotators pick the better response between two candidates.\n","- Likert-scale ratings: raters score a response on a 1-5 or 1-7 scale for attributes like helpfulness or safety.\n","- Expert evaluations: domain experts assess correctness in specialized areas (e.g., medicine, law, finance).\n","- User studies: live experiments with real users measuring satisfaction, trust, or usability."],"metadata":{"id":"_9coN8YyYMFp"}},{"cell_type":"markdown","source":["### Recommendations\n","\n","In most cases, LLM evaluation requires to develop an evaluation pipeline that combines several methods. The pipelines typically include automated evaluations against benchmarks and LLM-as-a-judge, and incorporates human evaluations on a subset of generated responses. Successful evaluation requires to design clear rubrics with examples for both automated and human evaluators. It is also important to validate the scores of automated evaluations by human evaluators on a held-out set of problems and questions. For safety and bias evaluation, create a set of adversarial prompts to evaluate the generated responses.\n","\n","\n"],"metadata":{"id":"N7VcC6Ils_sD"}},{"cell_type":"markdown","source":["## 21.7 Prompt Engineering <a name='21.7-prompt-engineering'></a>"],"metadata":{"id":"nT8PV_WTlAaJ"}},{"cell_type":"markdown","source":["**Prompt engineering** is a technique for improving the performance of LLMs by providing detailed context and information about a specific task. It involves creating text prompts that provide additional information or guidance to the model, such as the topic of the generated response. With prompt engineering, the model can better understand the kind of expected output and produce more accurate and relevant results.\n","\n","The following tips for creating effective prompts as part of prompt engineering can improve the performance of LLMs:\n","\n","- Use clear and concise prompts: The prompt should be easy to understand and provide enough information for the model to generate relevant output. Avoid using jargon or technical terms.\n","- Use specific examples: Providing specific examples can help the model better understand the expected output. For example, if you want the model to generate a story about a particular topic, include a few sentences about the setting, characters, and plot.\n","- Vary the prompts: Use prompts with different styles, tones, and formats to obtain more diverse outputs from the model.\n","- Test and refine: Test the prompts on the model and refine them by adding more detail or adjusting the tone and style.\n","- Use feedback: Use feedback from users or other sources to identify areas where the model needs more guidance and make adjustments accordingly.\n","\n","*Chain-of-thought technique* involves providing the LLM with a series of instructions to help guide the model and generate a more coherent and relevant response. This technique is useful for obtaining well-reasoned responses from LLMs.\n","\n","An example of a chain-of-thought prompt is as follows: \"You are a virtual tour guide from 1901. You have tourists visiting Eiffel Tower. Describe Eiffel Tower to your audience. Begin with (1) why it was built, (2) how long it took to build, (3) where were the materials sourced to build, (4) number of people it took to build it, and (5) number of people visiting the Eiffel tour annually in the 1900's, the amount of time it completes a full tour, and why so many people visit it each year. Make your tour funny by including one or two funny jokes at the end of the tour.\""],"metadata":{"id":"25kN3VP7lApv"}},{"cell_type":"markdown","source":["## 21.8 Foundation Models <a name='21.8-foundation-models'></a>\n","\n","**Foundation Models** are extremely large NN models trained on tremendous amounts of data with substantial computational resources, resulting in high capabilities for transfer learning to a wide range of downstream tasks. In other words, these models are scaled along each of the three factors: number of model parameters, size of the training dataset, and amount of computation. And, they are typically trained using self-supervised learning on unlabeled data. The scale of Foundation Models leads to new emergent capabilities, such as the ability to perform well on tasks that the models were not explicitly trained to do. This allows few-shot learning, which refers to finetuning Foundation Models to new downstream tasks by using only a few training data instances for the new task. Similarly, zero-shot learning extends this concept even further, and refers to a model's ability to generalize to new tasks for which the model hasn't seen any examples during the training.\n","\n","LLMs represent early examples of Foundation Models, because LLMs are trained at scale and can be adapted for various NLP tasks, even for tasks they were not trained to perform.\n","\n","The term Foundation Models is more general than LLMs, and they generally refer to large models that are trained on multimodal data, where the inputs can include text, images, audio, video, and other data sources.\n","\n","The importance of Foundation Models is in their potential to replace task-specific ML models that are specialized in solving one task (i.e., optimized to perform well on one dataset) with general models that have the capabilities to solve multiple tasks. I.e., these models can serve as a foundation that is adaptable to a broad range of applications.\n","\n","<img src=\"images/foundation_model.jpg\" width=\"600\">\n","\n","*Figure: Foundation model.* Source: [link](https://blogs.nvidia.com/blog/2023/03/13/what-are-foundation-models/)."],"metadata":{"id":"BnFBFVAEe1my"}},{"cell_type":"markdown","source":["## 21.9 Limitations and Ethical Considerations of LLMs <a name='21.9-limitations-and-ethical-considerations-of-llms'></a>"],"metadata":{"id":"RUNB0noQXlQz"}},{"cell_type":"markdown","source":["Although LLMs have demonstrated impressive performance across a wide range of tasks, there are several limitations and ethical considerations that raise concerns.\n","\n","Limitations:\n","\n","- *Computational resources*: Training LLMs requires significant computational resources, making it difficult for researchers with limited access to GPUs or specialized hardware to develop and use these models.\n","- *Data bias*: LLMs are trained on vast amounts of data from the internet, which often contain biases present in the data. As a result, the models may unintentionally learn and reproduce biases in their generated responses.\n","- *Producing hallucinations*: LLMs can produce hallucinations, which are responses that are false, inaccurate, unexpected, or contextually inappropriate. One example of hallucination by ChatGPT is when asked to list academic papers by an author, and it provides papers that don't exist.\n","- *Inability to explain*: LLMs are inherently black-box models, making it challenging to explain their reasoning or decision-making processes, which is essential in certain applications like healthcare, finance, and legal domains.\n","\n","\n","Ethical considerations:\n","\n","- *Privacy concerns*: LLMs memorize information from their training data, and can potentially reveal sensitive information or violate user privacy.\n","- *Misinformation and manipulation*: Text generated by LLMs can be exploited to create disinformation, fake news, or deepfake content that manipulates public opinion and undermines trust.\n","- *Accessibility and fairness*: The computational resources and expertise required to train LLMs may lead to an unequal distribution of benefits, where only a few organizations have the resources to develop and control these powerful models.\n","- *Environmental impact*: The large-scale training of LLMs consumes a significant amount of energy contributing to carbon emissions, which raises concerns about the environmental sustainability of these models.\n","\n","Conclusively, it is important to encourage transparency, collaboration, and responsible AI practices to ensure that LLMs benefit all members of society without causing harm."],"metadata":{"id":"WHbgR7cBc7jQ"}},{"cell_type":"markdown","source":["## Appendix: Unsloth Library for LLM Training and Inference <a name='appendix:-unsloth-library-for-llm-training-and-inference'></a>\n","\n","**(The material in the Appendix is not required for quizzes and assignments.)**"],"metadata":{"id":"wqMGfKQFqepe"}},{"cell_type":"markdown","source":["**Unsloth** is another library for training and inference of LLMs, offering tools to facilitate optimization of LLMs ([link](https://unsloth.ai/)) The library applies various optimization techniques to reduce the training and inference time in comparison to the Hugging Face library and other related libraries. As you will notice in the following code, the Unsloth tools use pre-built components from Hugging Face (such as `transformers`, `trl`) and adapt them to optimize various workflows for model training and inference.\n","\n","The following code [14] provides an example of finetuning LlaMA-3.1 8B model using a single T4 GPU. For this example, the training time was similar to training LlaMA 2 7B with the Hugging Face library above, as in both cases training for 1 epoch took about 15 minutes. On the other hand, while the largest batch size (in multiples of 2) with Hugging Face was 8 samples, Unsloth allowed to use a batch size of 16, meaning that Unsloth optimized the memory usage. Training LLMs with larger batch sizes is related to reduced training variance and more stable gradient updates, which typically result in improved performance. In addition, the inference with Unsloth was faster."],"metadata":{"id":"tV3OTXmItWMa"}},{"cell_type":"code","source":["# Note: to install unsloth in this notebook, I had to interupt the currently running kernel, and start a new kernel\n","%%capture\n","!pip install -q unsloth\n","# Also get the latest nightly Unsloth!\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""],"metadata":{"id":"PLD3oFA_Z6Hj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n","    max_seq_length = 2048,\n","    dtype = None,\n","    load_in_4bit = True,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":323,"referenced_widgets":["d3593822e80b4773a41751f3baf711e0","1abbfaaa8fe2415b97f2a007b4a7f793","78cf46d33d2f4fcb9d525628711af475","5553b414a41143d0b1976ed07b5bf708","b4027ca9781848f5bbe04c9081c9dc8b","f96ac88e5aac4e65ac790e03e474cddb","9c16e37322204a8dae2a92db66563b75","f028252b6d604051a5cc946b719edb3c","77683fb810e14dcab7e00a225643d6d9","4bac4ce419d443308ca37b09e29d61ef","3a5561a1e6614ba5a4dfaefa5208a967","62b1f8a0a2ab471ea25d61942eb147ee","9fec6f46f04743a1a4e4b045b33099aa","2334c13956214977bfd63aff514a2047","926156adb4134896bcc91258ce400d20","df60b1289c404ca1bbc4ed3569b53020","e34c21abcf9840db9568654c18a0f888","3e8df545ef8b4499adf3719cc6a3759e","1802e8483f42457fafa5d73fdddb9887","006835fc6b5645d89e65c218f62c51b9","8f4531fc221a416c99f20e9a7cf5bb76","787de563a9894edcbb2ad53adf7523bf","e00c190f1e6c4b07972a71c722a9cf18","5cac35191aa04f0fbf29f305d374779f","fbb8bae2138c457ca550e6ec5491cb28","8c914960cec84125883abcfffd091999","dedede2e8bb14b8fa199fd5ad1c9fed3","96521f2b900240b8bea13b111fc83710","d44b5bd26a654e92942aa87309af37c3","4baacb07dca94887a34a46400c6e9be2","6f660286f4904539b165986606018337","03f1a9eef0ba47db9a3337b252377c42","2abf04830ef043628263c76706ec603a","2ab0e1b4fd6644a1bde1c45dcf507faa","3ae22d5f3a444c7199af72e72312da27","749aaafaf7e1446ea77ec6ee9fafc952","a9dfaf851d20488abaaea4a716a9a1ec","69df1a5699414f14a3cfb20c0d624a6e","37b0471c4a3344d0b0711bbc41224e99","89bf6c93a7aa456e9df062d78df53fad","e3a897dcb4ee4444a63f103c61374359","e02b14a39cf14e43aa6ff2904611e165","6a3301fb549746798ae479a30c25dcfc","1cd84bf2ab964b1d85cb4215cf7d5424","9a8998b0d7334176bbc631b664de476f","36f3291349f5467193dafaca521923da","fa23dc1b74cd4dee843a9f2412df85d4","c7fced67d9714b4b9900969b665afb72","01d665bb88524e938e7ed5d1fa75a735","226b7893dfdb4b96a50ddf4cf3bef9a1","d4610c836b5c4fe7b9a0c378f6886c0f","ab1dade2db0f4a33bb58056332d430c9","f4903b19beb0430dbd22a889b1649710","3a22677853274054bf1d5801a19137af","6adedf09d0e54ab7acaaf0179cd1272a"]},"id":"1jnwKADYr8Gn","executionInfo":{"status":"ok","timestamp":1762615080379,"user_tz":420,"elapsed":109553,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"ce084efb-d3c3-4ef4-a35b-087bc2bc55a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","ü¶• Unsloth Zoo will now patch everything to make training faster!\n","==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3593822e80b4773a41751f3baf711e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62b1f8a0a2ab471ea25d61942eb147ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e00c190f1e6c4b07972a71c722a9cf18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ab0e1b4fd6644a1bde1c45dcf507faa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a8998b0d7334176bbc631b664de476f"}},"metadata":{}}]},{"cell_type":"code","source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # Supports rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"],"metadata":{"id":"dTxPMx9jZ6J2","executionInfo":{"status":"ok","timestamp":1762615087308,"user_tz":420,"elapsed":6919,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f79e3271-9e40-492f-84e2-1d42b7d02921"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth 2025.11.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load the Lamini dataset\n","dataset = load_dataset(\"mwitiderrick/llamini_llama\", split=\"train\")"],"metadata":{"id":"0VNtZ0BPsf2z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = 2048,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 16,\n","        gradient_accumulation_steps = 2,\n","        warmup_steps = 5,\n","        num_train_epochs = 1,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 5,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to=\"none\"\n","    ),\n",")"],"metadata":{"id":"b7KHBD8tsf5L","executionInfo":{"status":"ok","timestamp":1762615099176,"user_tz":420,"elapsed":5112,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["0569e0ee5c054255959781085f537e83","f0435eb9c89146f6b6e1ee84501441a0","f8d134b0b0f6411ab70153e557fbf59a","466c003314064b15a5caa6734638d2d8","88ca2d2f676040bf81e42ac065621690","d8c151d5a90244ac9deb6a4dd10304af","d80d99806fde4e6face09be6bf2b5650","87a9f4ec77ec4fe28ff6deba882925f8","06c0e86259b2497593e2db9a6b8a7984","b490f3df480c4b18a4a48baac317669e","80bfde1d1bb8445789c461fbb70b74c9"]},"outputId":"a9351b5e-6cee-4be8-c84c-fc9e0b25eaed"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/1260 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0569e0ee5c054255959781085f537e83"}},"metadata":{}}]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"6KXJ5fZzslmB","executionInfo":{"status":"ok","timestamp":1762615989163,"user_tz":420,"elapsed":889984,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/","height":490},"outputId":"9a562b60-c73f-47cb-f0cb-f6e9884628cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 1,260 | Num Epochs = 1 | Total steps = 40\n","O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 2\n","\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 2 x 1) = 32\n"," \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Will smartly offload gradients to save VRAM!\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [40/40 14:18, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>2.840700</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.679800</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.779200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.694000</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.670500</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.670100</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.614600</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.631900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=40, training_loss=1.0725934982299805, metrics={'train_runtime': 887.705, 'train_samples_per_second': 1.419, 'train_steps_per_second': 0.045, 'total_flos': 1.5910729135030272e+16, 'train_loss': 1.0725934982299805, 'epoch': 1.0})"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Perform inference\n","FastLanguageModel.for_inference(model)\n","prompt = \"What are Lamini models?\"\n","inputs = tokenizer([prompt.format(\n","        \"\", # instruction\n","        \"\", # input\n","        \"\", # output\n","        )], return_tensors = \"pt\").to(\"cuda\")\n","outputs = model.generate(**inputs, max_new_tokens=200, use_cache=True)\n","decoded_output = tokenizer.batch_decode(outputs)\n","print(\"\\n\".join(decoded_output))"],"metadata":{"id":"AY6s0FQwslov","executionInfo":{"status":"ok","timestamp":1762616002410,"user_tz":420,"elapsed":13246,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"54260722-811a-4ff5-c3db-df33d4e6c8c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<|begin_of_text|>What are Lamini models? Lamini models are pre-trained language models that have been trained on large datasets to generate human-like text. These models are designed to be fine-tuned for specific tasks, such as language translation, text summarization, or chatbot responses.\n","Lamini models are trained using a technique called masked language modeling, where a portion of the input text is randomly replaced with a [MASK] token. The model is then trained to predict the original text instead of the [MASK] token. This technique helps the model learn the context and relationships between words in a sentence.\n","Lamini models can be fine-tuned for specific tasks by adding a task-specific layer on top of the pre-trained model. This layer is trained to perform the specific task, such as language translation or text classification.\n","Lamini models are available in various sizes, including small, medium, and large. The size of the model determines the amount of training data and computational resources required to train the model.\n","Here are some benefits of\n"]}]},{"cell_type":"code","source":["# Perform inference\n","prompt = \"Write a poem about Data Science\"\n","inputs = tokenizer([prompt.format(\n","        \"\", # instruction\n","        \"\", # input\n","        \"\", # output\n","        )], return_tensors = \"pt\").to(\"cuda\")\n","outputs = model.generate(**inputs, max_new_tokens=500, use_cache=True)\n","decoded_output = tokenizer.batch_decode(outputs)\n","print(\"\\n\".join(decoded_output))"],"metadata":{"id":"aMO7gsdCBHDb","executionInfo":{"status":"ok","timestamp":1762616291897,"user_tz":420,"elapsed":18069,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd8590b1-8514-4b02-909e-c75cfa6b4c6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<|begin_of_text|>Write a poem about Data Science\n","In the realm of numbers, where truth is key\n","Lies a field of magic, where data sets free\n","The scientists of insight, with tools so grand\n","Unravel secrets, in this digital land\n","With algorithms and models, they weave their spell\n","To extract meaning, from data's endless shell\n","They mine for gold, in this digital sea\n","And uncover stories, hidden from humanity\n","With Python and R, they craft their might\n","And bring the data, to the light of day and night\n","In the world of data science, where facts reign\n","They seek to understand, the patterns that remain\n","From finance to healthcare, to social media's sway\n","They analyze and optimize, in a digital way\n","With machine learning, they teach the machines\n","To learn from data, and make wise decisions' claims\n","In this field of wonder, where data's the key\n","Lies a future bright, for humanity\n","Where insights abound, and wisdom is shared\n","In the realm of data science, where truth is declared\n","Lies a world of possibilities, yet to be told\n","In this digital age, where data's the guide\n","Lies a path forward, where innovation will reside\n","In the world of data science, where the future's bright\n","Lies a world of wonder, where data's the light.<|eot_id|>\n"]}]},{"cell_type":"markdown","source":["## References <a name='references'></a>\n","\n","1. Introduction to Large Language Models, by Bernhard Mayrhofer, available at [https://github.com/datainsightat/introduction_llm](https://github.com/datainsightat/introduction_llm).\n","2. Understanding Encoder and Decoder LLMs, by Sebastian Raschka, available at [https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder).\n","3. The Big LLM Architecture Comparison,  by Sebastian Raschka, available at [https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison).\n","4. Linear Layers and Activation Functions in Transformer Models, by Adrian Tam, available at [https://machinelearningmastery.com/linear-layers-and-activation-functions-in-transformer-models/](https://machinelearningmastery.com/linear-layers-and-activation-functions-in-transformer-models/).\n","5. LLM Training: RLHF and Its Alternatives, by Sebastian Raschka, available at [https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives).\n","6. Training Language Models to Follow Instructions with Human Feedback, by Long Ouyang et al., available at [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155).\n","7. Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA), by Sebastian Raschka, available at [https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html).\n","8. How to Fine-tune Llama 2 With LoRA, by Derrick Mwiti, available at [https://www.mldive.com/p/how-to-fine-tune-llama-2-with-lora](https://www.mldive.com/p/how-to-fine-tune-llama-2-with-lora).\n","9. Fine-Tuning Llama 2.0 with Single GPU Magic, by Chee Kean, available at [https://ai.plainenglish.io/fine-tuning-llama2-0-with-qloras-single-gpu-magic-1b6a6679d436](https://ai.plainenglish.io/fine-tuning-llama2-0-with-qloras-single-gpu-magic-1b6a6679d436).\n","10. Fine-Tuning LLaMA 2 Models using a single GPU, QLoRA and AI Notebooks, by Mathieu Busquet, available at [https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/](https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/).\n","11. Getting started with Llama, by Meta AI, available at [https://ai.meta.com/llama/get-started/](https://ai.meta.com/llama/get-started/).\n","12. Hugging Face: Chat Templates, available at [https://huggingface.co/learn/llm-course/en/chapter11/2](https://huggingface.co/learn/llm-course/en/chapter11/2).\n","13. Post-training 101, by Han Fang, Karthik Abinav Sankararaman, available at [https://tokens-for-thoughts.notion.site/post-training-101#262b8b68a46d80a18d03d44d591a972e](https://tokens-for-thoughts.notion.site/post-training-101#262b8b68a46d80a18d03d44d591a972e).\n","14. Llama-3.1 8b + Unsloth 2x faster finetuning, by Unsloth AI, available at [ https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing).\n"],"metadata":{"id":"Hlg6Y1COc8U_"}},{"cell_type":"markdown","source":["[BACK TO TOP](#top)"],"metadata":{"id":"gw7AvgH2nS41"}}]}