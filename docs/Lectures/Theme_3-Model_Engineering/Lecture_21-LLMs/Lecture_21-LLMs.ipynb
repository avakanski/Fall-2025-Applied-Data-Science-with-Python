{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Zc3eqC2hVQCg-97uT5BZZR-lWutdK5jP",
     "timestamp": 1699675698515
    },
    {
     "file_id": "1WlJSR0FBSQtTqM8AVpW48zbvAIyj5d7F",
     "timestamp": 1699662628845
    }
   ],
   "machine_shape": "hm",
   "gpuType": "T4",
   "authorship_tag": "ABX9TyO6EV4zlvC6c7VMwh9RzMzl"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lecture 21 - Large Language Models"
   ],
   "metadata": {
    "id": "xoYnii0YEv9n"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_21-LLMs/Lecture_21-LLMs.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_21-LLMs/Lecture_21-LLMs.ipynb)"
   ],
   "metadata": {
    "id": "QSUTHCD9lnyf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='top'></a>"
   ],
   "metadata": {
    "id": "iGbTNIZlln1J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [21.1 Introduction to LLMs](#21.1-introduction-to-llms)\n",
    "  - [21.1.1 Architecture of Large Language Models](#21.1.1-architecture-of-large-language-models)\n",
    "  - [21.1.2 Variants of Transformer Network Architectures](#21.1.2-variants-of-transformer-network-architectures)\n",
    "- [21.2 Creating LLMs](#21.2-creating-llms)\n",
    "  - [21.2.1 Pretraining](#21.2.1-pretraining)\n",
    "  - [21.2.2 Supervised Finetuning](#21.2.2-supervised-finetuning)\n",
    "  - [21.2.3 Alignment ](#21.2.3-alignment)\n",
    "- [21.3 Finetuning LLMs](#21.3-finetuning-llms)\n",
    "  - [21.3.1 Parameter-Efficient Finetuning (PEFT)](#21.3.1-parameter-efficient-finetuning-(peft))\n",
    "  - [21.3.2 Low-Rank Adaptation (LoRA)](#21.3.2-low-rank-adaptation-(lora))\n",
    "  - [21.3.3 Quanitized LoRA (QLoRA)](#21.3.3-quanitized-lora-(qlora))\n",
    "- [21.4 Finetuning Example: Finetuning LlaMA-2 7B](#21.4-finetuning-example:-finetuning-llama-2-7b)\n",
    "- [21.5 Chat Templates for Formatting LLM Data](#21.5-chat-templates-for-formatting-llm-data)\n",
    "- [21.6 Prompt Engineering](#21.6-prompt-engineering)\n",
    "- [21.7 Foundation Models](#21.7-foundation-models)\n",
    "- [21.8 Limitations and Ethical Considerations of LLMs](#21.8-limitations-and-ethical-considerations-of-llms)\n",
    "- [Appendix: Unsloth Library for LLM Training and Inference](#appendix:-unsloth-library-for-llm-training-and-inference)\n",
    "- [References](#references)\n"
   ],
   "metadata": {
    "id": "sMS08x8Rltiw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 21.1 Introduction to LLMs <a name='21.1-introduction-to-llms'></a>"
   ],
   "metadata": {
    "id": "KrZV8AcvUPqZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Large Language Models (LLMs) are  a class of Deep Neural Networks designed to understand and generate natural human language. LLMs achieved state-of-the-art performance across various NLP tasks.\n",
    "\n",
    "LLMs are a result of many years of research and advancement in NLP and Machine Learning. Important phases in NLP development include:\n",
    "\n",
    "- *Statistical language models (1980s-2000s)*: developed to predict the probability of a word in a text sequence based on the preceding words. Examples of statistical language models include Bag-Of-Words models based on N-grams. These models were used in tasks like speech recognition and machine translation, but struggled with capturing long-range dependencies and context-related information in text.\n",
    "- *Neural network models (2000-2017)*: Fully-connected NNs and Recurrent NNs emerged as an alternative to statistical language models. Long Short-Term Memory (LSTM) RNN models were used for sequence-to-sequence tasks (such as  machine translation) and they formed the basis for several early LLMs. Similar to statistical language models, RNNs struggled with capturing context-related information. Other limitations of RNNs include the inability to parallelize the data processing, and the gradients can become unstable during training.\n",
    "- *Transformer network models (2017-present)*: Transformer networks introduced the self-attention mechanism as a replacement for the recurrent layers in RNNs. This architecture enabled the development of more powerful and efficient LLMs, laying the foundation for BERT, GPT, and modern LLMs."
   ],
   "metadata": {
    "id": "yut8E4WoRpVv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 21.1.1 Architecture of Large Language Models <a name='21.1.1-architecture-of-large-language-models'></a>"
   ],
   "metadata": {
    "id": "v5uvsID2UMCT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The architecture of modern LLMs is based on Transformer Networks, which we covered in Lecture 20. The main components of the Transformer Networks architecture include:\n",
    "\n",
    "- **Input embeddings**, are fixed-size continuous vector embeddings that represent tokens in input text.\n",
    "- **Positional encodings**, are fixed-size continuous vectors that are added to the input embeddings to provide information about the relative positions of the tokens in the input text sequence.\n",
    "- **Encoder**, is composed of a stack of multi-head attention modules and fully-connected (feed-forward) modules. The encoder block also includes dropout layers, residual connections, and applies layer normalization.\n",
    "- **Decoder**, is composed of a stack of multi-head self-attention modules and fully-connected (feed-forward) modules similarly to the encoder block. The decoder block has an additional masked multi-head attention module, that applies masking to the next words in the text sequence to ensure that the module does not have access to those words for predicting the next token.\n",
    "- **Output fully-connected layer**, the output of the decoder is passed through a fully-connected (dense, linear) layer to produce the next token in the text sequence.\n",
    "\n",
    "<img src=\"images/transformer.jpg\" width=\"450\">\n",
    "\n",
    "*Figure: Pretraining LLMs.* Source: [2]."
   ],
   "metadata": {
    "id": "aP-g3MgRUYUI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The architecture of Transformer Networks includes multiple successive encoder and decoder blocks to create deep networks with many layers that allow learning complex patterns in input text. For example, the original Transformer Network has 6 encoder and 6 decoder blocks, as shown in the above figure.\n",
    "\n",
    "The **self-attention mechanism** is a key component of the Transformer Network architecture that enables the model to weigh the importance of each token with respect to the other tokens in a sequence. It allows to capture long-range dependencies and relationships between the tokens (words) and helps the model to understand the context and structure of the input text sequence."
   ],
   "metadata": {
    "id": "5eNRiCQhWXlN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 21.1.2 Variants of Transformer Network Architectures <a name='21.1.2-variants-of-transformer-network-architectures'></a>"
   ],
   "metadata": {
    "id": "NtZrLHZdewMk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Various LLMs have been built on top of the Transformer Network architecture. The popular variants include:\n",
    "\n",
    "- **Decoder-only models**: are autoregressive models that utilize only the decoder part of the Transformer Network architecture. These models are particularly suitable for generating text and content. An example of decoder-only LLMs is the family of GPT models.\n",
    "- **Encoder-only models**: use only the encoder part of the Transformer Network architecture, and perform well on tasks related to language understanding, such as classification and sentiment analysis. An example is the BERT model.\n",
    "- **Encoder-decoder models**: employ the original Transformer Network architecture and combine encoder and decoder sub-networks, enabling to both understand language and generate content. These models can be used for various NLP tasks with minimal task-specific modifications. An example of this class of models is T5 (Text-to-Text Transfer Transformer)."
   ],
   "metadata": {
    "id": "FevKIGPne1iW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### List of LLMs\n",
    "\n",
    "A large number of LLMs have been developed in the past several years. Some of the most well-known LLMs include:\n",
    "\n",
    "- *GPT* (Generative Pretrained Transformers): Developed by OpenAI, the GPT family are the best-known LLMs. They include GPT 1, 2, 3, 3.5 (initial ChatGPT), 4, 4o (current ChatGPT), and o1 (where o stands for omni, meaning that the model can process multi-modal inputs, including text, images, video, audio, etc.). According to some sources, GPT-4 has 1.76 trillion parameters, and it is trained on 13T tokens.\n",
    "- *LlaMA (Large Language Model Meta AI)*: Developed by Meta AI, LlaMA is an open-source LLM, which can be used for both research and commercial uses. It consists of several models including LlaMA base model, LlaMA-Chat, and Code-LlaMA. Released versions include LlaMA 2, LlaMA 3, LLaMA 3.1, and LlaMA 3.2. The latest LlaMA 3.2 includes smaller test models with 1B and 3B parameters, and multi-modal 11B and 90B parameters, trained on 9T tokens.\n",
    "- *Claude*: Developed by Anthropic, the latest version Claude 3 has three models named Haiku, Sonnet, and Opus. These models rank very high on the benchmarking leaderboards for many tasks, and they are currently the main competitor to OpenAI's GPT models.\n",
    "- *Gemini*: Developed by Google, offers four models named Nano, Flash, Pro, and Ultra. The number of parameters is not known. The smaller models are designed for smartphones, whereas the larger models are multimodal and can process images, video, code, and other inputs, beside text.\n",
    "- *Mixtral*: Developed by Mistral, these LLM use mixture-of-experts (MOE) architecture, which allows them to be competitive with larger models, despite having fewer parameters. Current models have 8 mixture-of-experts with 7B and 22B parameters.\n",
    "- *Grok*: Developed by xAI, Grok is trained on data from X (formerly Twitter) and has 314B parameters. It also uses a mixture-of-experts (MOE) architecture.\n",
    "- *BERT* (Bidirectional Encoder Representations from Transformers): Developed by Google in 2018, BERT is an early LLM with 340M parameters that can understand natural language and answer questions.\n",
    "- *Cohere LLM*: Developed by Cohere, it is a family of LLMs with 6B, 13B, and 52B parameters, designed for enterprise use cases.\n",
    "- *Vicuna*: Developed by LMSYS, Vicuna is a 13B parameters chat assistant finetuned from LLaMA on user-shared conversations.\n",
    "- *Alpaca*: Developed by Stanford, it is a 7B LLM finetuned from instruction-following samples by LLaMA.\n",
    "- *Falcon*: Developed by UAE's Technology Innovation Institute (TII), it is an open-source family of models with 1.3B, 7.5B, 40B, and 180B parameters, trained on 3.5T tokens.\n",
    "- *DBRX* and *Dolly*: Developed by Databricks, DBRX has 132B parameters, whereas Dolly is a smaller LLM language model with 12B parameters."
   ],
   "metadata": {
    "id": "40129R8npOLm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 21.1.3 Optimizations of the Transformer Network Architectures <a name='21.1.3-optimizations-of-the-transformer-network-architectures'></a>"
   ],
   "metadata": {
    "id": "WXVpBZz4xzax"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Activation Functions"
   ],
   "metadata": {
    "id": "xyzRozBlx9J_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 21.2 Creating LLMs <a name='*21.2*-creating-llms'></a>"
   ],
   "metadata": {
    "id": "s3o9DjItXUxt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating modern LLMs typically involves three main phases:\n",
    "\n",
    "1. **Pretraining**, the model extracts knowledge from large unlabeled text datasets.\n",
    "2. **Supervised finetuning**, the model is refined to improve the quality of generated responses.\n",
    "3. **Alignment**, the model is further refined to generate safe and helpful responses that are aligned with human preferences."
   ],
   "metadata": {
    "id": "LaZLtPXyXfrK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 21.2.1 Pretraining <a name='21.2.1-pretraining'></a>\n",
    "\n",
    "The first step in creating LLMs is **pretraining** the model on massive amounts of text data. The datasets usually consist of a large collection of web pages or e-books comprising billions or trillions of tokens, and ranging from gigabytes to terabytes of text. During pretraining, the model learns the structure of the language, grammar rules, facts about the world, and reasoning rules. And, it also learns biases and harmful content present in the training data.\n",
    "\n",
    " Pretraining is performed using unsupervised learning techniques. Two common approaches for pretraining LLMs are:\n",
    "\n",
    "- **Causal Language Modeling**, also known as autoregressive language modeling, involves training the model to predict the next token in the text sequence given the previous tokens. This approach is more common with modern LLMs.\n",
    "- **Masked Language Modeling**, where a certain percentage of the input tokens are randomly masked, and the model is trained to predict the masked tokens based on the surrounding context. BERT and earlier LLMs were pretrained with masked language modeling.\n",
    "\n",
    "The following figure depicts the pretraining phase with Causal Language Modeling, where the model learns to predict the next word in a sentence given the previous words.\n",
    "\n",
    "<img src=\"images/pretraining.jpg\" width=\"450\">\n",
    "\n",
    "*Figure: Pretraining LLMs.* Source: [3].\n",
    "\n",
    "Pretraining allows to extract knowledge from very large unlabeled datasets in unsupervised learning manner, without the need for manual labeling. Or, to be more precise, the \"label\" in LLMs pretraining is the next word in the text, to which we already have access since it is part of the training text. Such pretraining approach is also called self-supervised training, since the model uses each next word in the text to self-supervise the training.\n",
    "\n",
    "Note that pretraining LLMs from scratch is computationally expensive and time-consuming. As we stated before, the pretraining phase can cost millions of dollars (e.g., the estimated cost for training GPT-4 is $100 million). Also, pretraining LLMs requires access to large datasets and technical expertise with strong understanding of deep learning workflows, working with distributed software and hardware, and managing model training with thousands of GPUs simultaneously."
   ],
   "metadata": {
    "id": "wJ5rp47DXf2w"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 21.2.2 Supervised Finetuning <a name='21.2.2-supervised-finetuning'></a>\n",
    "\n",
    "After the pretraining phase, the model is finetuned on a much smaller dataset, which is carefully generated with human supervision. This dataset consists of samples where AI trainers provide both queries (instructions) and model responses (outputs), as depicted in the following figure. That is, *instruction* is the input text given to the model, and *output* is the desired response by the model. The model takes the instruction text as input (e.g., \"Write a limerick about a pelican\") and uses next-token prediction to generate the output text (e.g., \"There once was a pelican so fine ...\").\n",
    "\n",
    "The finetuning process involves updating the model's weights using supervised learning techniques. The objective of supervised finetuning is to improve the quality of the generated responses by the pretrained LLM.\n",
    "\n",
    "To compile datasets for supervised finetuning, AI trainers need to write the desired instructions and responses, which is a laborious process. Typical datasets include between 1K and 100K instruction-output pairs. Based on the provided instruction-output pairs, the model is finetuned to generate responses that are similar to those provided by AI trainers.\n",
    "\n",
    "<img src=\"images/finetuning.jpg\" width=\"500\">\n",
    "\n",
    "*Figure: Finetuning a pretrained LLM.* Source: [3]."
   ],
   "metadata": {
    "id": "LnAjaqBPXlJC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 21.2.3 Alignment <a name='21.2.3-alignment'></a>\n",
    "\n",
    "To further improve the performance and align the model responses with human preferences, LLMs are typically refined in one additional phase. This ensures that the responses generated by LLMs are aligned with human preferences, making the models more useful and safer for interaction with users. The alignment phase is essential for reducing harmful, biased, or otherwise undesirable outputs.  \n",
    "\n",
    "Two main strategies for LLM alignment include Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization (PPO) and Reinforcement Learning with Direct Policy Optimization (DPO).\n",
    "\n",
    "**Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization (PPO)**\n",
    "\n",
    "LLM alignment with Reinforcement Learning from Human Feedback (RLHF) by employing Proximal Policy Optimization (PPO) is depicted in the figure below and involves the following steps:\n",
    "\n",
    "1. *Collect human feedback*. For this step a new dataset is created by collecting sample prompts from a database or by creating a set of new prompts. For each prompt, multiple responses are generated by the supervised finetuned model. Next, AI trainers are asked to rank by quality all responses generated by the model for the same prompt, from best to worst. Such feedback is used to define the human preferences and expectations about the responses by the model. Although this ranking process is time-consuming, it is usually less labor-intensive than creating the dataset for supervised finetuning, since ranking the responses is faster than writing the responses.\n",
    "2. *Create a reward model*. The collected data with human feedback containing the prompts and the ranking scores of the different responses are used to train a Reward Model (denoted with RM in the figure). The task for the Reward Model is to predict the quality of the different responses to a given prompt and output a ranking score. The ranking scores provided by AI trainers are used to establish the ground-truth for training the Reward Model. Note that the Reward Model is a different model than the LLM that is being finetuned, and it only needs to rank the generated responses by the LLM.\n",
    "3. *Finetune the LLM with RL*. The LLM is finetuned using the Reinforcement Learning (RL) algorithm Proximal Policy Optimization (PPO). For a new prompt, the original LLM generates a response, which the Reward Model evaluates and calculates a reward score $r_k$. Next, the PPO algorithm uses the reward score $r_k$ to finetune the LLM so that the total rewards for the generated responses by the LLM are maximized. I.e., the goal is to generate responses by the LLM that maximize the predicted reward scores, and by that, the responses become more aligned with human preferences and are more useful to human users.\n",
    "4. *Iterative improvement*. The RLHF process is performed iteratively, with multiple rounds of collecting additional feedback from human labelers, re-training the Reward Model, and applying Reinforcement Learning. This leads to continuous refinement and improvement of the LLM's performance.\n",
    "\n",
    "<img src=\"images/RLHF.jpg\" width=\"600\">\n",
    "\n",
    "*Figure: Reinforcement Learning from Human Feedback.* Source: [4].\n",
    "\n",
    "In summary, the RLHF approach creates a reward system that is augmented by human feedback and is used to teach LLMs which responses are more aligned with human preferences. Through these iterations, LLMs can be better aligned with our human values and can lead to higher-quality responses, as well as improved performance on specific tasks.\n",
    "\n",
    "Note also that there are several variants of the RLFH approach for finetuning LLMs. For example, LlaMA models employ two reward models: one based on the ranks of helpfulness of the responses, and another based on the ranks of safety of the responses. The final reward score is obtained as a combination of the helpfulness and safety scores.\n",
    "\n",
    "**Reinforcement Learning with Direct Policy Optimization (DPO)**\n",
    "\n",
    "RL with Direct Policy Optimization (DPO) is another approach for LLM alignment that has been popular recently, as it is simpler than RLHF with PPO. DPO uses a different optimization approach in comparison to RL with PPO, where DPO optimizes the LLM directly based on user preferences, without the need for training a separate Reward Model. I.e., DPO aims to directly maximize the reward function to produce model outputs that align with human preferences. Detailed explanation of RL with DPO is beyond the scope of this lecture.\n"
   ],
   "metadata": {
    "id": "0s2BsqGjXlLg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 21.3 Finetuning LLMs <a name='21.3-finetuning-llms'></a>"
   ],
   "metadata": {
    "id": "HY3OHi4TlRAw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Finetuning LLMs** involves updating the weights of an LLM model on new data to improve its performance on a specific task and make the model more suitable for a specific use case. It involves additional re-training of the model on a new dataset that is specific to that task. That is, finetuning is a transfer learning technique, where the gained knowledge by a trained model is transferred to improve the performance on a target task.\n",
    "\n",
    "To adapt LLMs to a custom task, different finetuning techniques have been applied. *Full model finetuning* is a method that finetunes all the parameters of all the layers of a pretrained model. Full model finetuning typically can achieve the best performance, but it is also the most resource-intensive and time-consuming. *Performance-efficient finetuning* involves updating only a small number of the parameters to reduce the required computational resources and costs.\n",
    "\n",
    "In this section, we will demonstrate how to finetune **LlaMA 2**, an open-source LLM developed by Meta AI. Released in July 2023, LlaMA 2 was the first LLM that is open for both research and commercial use. LlaMA 2 is a successor model to the original LlaMA developed by Meta AI as well. LlaMA 2 has three variants with 7B, 13B, and 70B parameters. It has been trained on 2 trillion tokens, and it has a context window of 4,096 tokens enabling to process large documents. For instance, for the task of summarizing a pdf document the context can include the entire text of the pdf document, or for dialog with a chatbot the context can include the previous conversation history with the chatbot. Furthermore, specialized versions of LlaMA 2 include LlaMA-2-Chat optimized for dialog generation, and Code LlaMA optimized for code generation tasks."
   ],
   "metadata": {
    "id": "gHwycTGfVUN_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 21.3.1 Parameter-Efficient Finetuning (PEFT) <a name='21.3.1-parameter-efficient-finetuning-(peft)'></a>"
   ],
   "metadata": {
    "id": "rie71lFEWtQ_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finetuning LLMs is challenging since the large number of parameters of modern LLMs requires substantial computational resources for storing the models and for re-training the weights. Thus, it can be prohibitively expensive for most users. For instance, to load the largest version of the LlaMA 2 model with 70 billion parameters into the GPU memory requires approximately 280 GB of RAM. Full model finetuning of LlaMA 2 model with 70 billion parameters requires 780 GB of GPU memory. This is equivalent to 10 A100s GPUs that have 80 GB RAM each, or 48 T4 GPUs that have 16 GB RAM each. The free version of Google Colab offers one T4 GPU with 16 GB RAM.\n",
    "\n",
    "Fortunately, several Parameter-Efficient FineTuning (PEFT) techniques have been introduced recently, which allow updating only a small number of the model weights. Consequently, these techniques enable finetuning LLMs using lower computational resources by reducing memory usage and speeding up the training process. PEFT techniques include prompt tuning, prefix tuning, adding additional adapter layers in the transformer block, and low-rank adaptation (LoRA).\n",
    "\n",
    "Hugging Face has developed a [PEFT library](https://huggingface.co/docs/peft/index) that contains implementations of common finetuning techniques. We will use the PEFT library to finetune LlaMA 2 on a custom dataset using a quantized version of the LoRA method."
   ],
   "metadata": {
    "id": "jprNHaStWtS7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 21.3.2 Low-Rank Adaptation (LoRA) <a name='21.3.2-low-rank-adaptation-(lora)'></a>\n",
    "\n",
    "**Low-Rank Adaptation (LoRA)** involves freezing the pretrained model and finetuning a small number of additional weights. After the additional weights are updated, these weights are merged with the weights of the original model.\n",
    "\n",
    "This is depicted in the following figure, where regular finetuning is shown in the left figure, and it involves updating all weights $W$ in a pretrained model. As we know, the weight update matrix $\\nabla{W}$ is calculated based on the negative gradient of the loss function. Finetuning with LoRA is shown in the right figure, where the weight update matrix $\\nabla{W}$ is decomposed into two smaller matrices, $\\nabla{W}=W_A*W_B$, with size $W_A \\in \\mathbb{R}^{A \\times r}$ and $W_B \\in \\mathbb{R}^{r \\times B}$. The matrices $W_A$ and $W_B$ are called low-rank adapters, since they have lower rank $r$ in comparison to the original weight matrix, i.e., they have fewer number of columns or rows, respectively. During training, gradients are backpropagated only through the matrices $W_A$ and $W_B$, while the pretrained weights $W$ remain frozen.\n",
    "\n",
    "For instance, if the full weight matrix $W$ is of size $100 \\times 100$, this is equal to $10,000$ elements (model weights). If we decompose the weight update matrix $\\nabla{W}$ by using rank $r=5$, the total number of elements of $W_A \\in \\mathbb{R}^{100 \\times 5}$ and $W_B \\in \\mathbb{R}^{5 \\times 100}$ will be $500 + 500 =  1,000$. Hence, with LoRA the number of elements was reduced from $10,000$ to $1,000$.\n",
    "\n",
    "<img src=\"images/LoRA.png\" width=\"600\">\n",
    "\n",
    "*Figure: Regular finetuning versus LoRA finetuning .* Source: [5]."
   ],
   "metadata": {
    "id": "XS2wNFlwlgD1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 21.3.3 Quanitized LoRA (QLoRA) <a name='21.3.3-quanitized-lora-(qlora)'></a>\n",
    "\n",
    "**Quanitized LoRA (QLoRA)** is a modified version of LoRA that uses 4-bit quantized weights. *Quantization* reduces the precision for the values of the network weights. In TensorFlow and PyTorch, the network weights by default are stored with 32-bit floating-point precision. With quantization techniques, the network weights are stored with lower precision, such as 16-bit, 8-bit, or 4-bit precision.\n",
    "\n",
    "This approach introduces a new 4-bit quantization format called \"nf4\" (normalized float 4) where the range of values is normalized to the range [-1, 1] by dividing the values evenly into 16 bins (4-bit allows $2^4=16$ values). While 4-bit floating point precision (fp4) applies non-linear floating point representation of the original values and results in unequal spacing of the values, normalized float 4 precision (nf4) applies linear quantization of the original values into equally spaced bins and follows a normal distribution.\n",
    "\n",
    "QLoRA combines 4-bit quantization of the model weights in the pretrained model and LoRA that adds low-rank adaptor layers. The benefits of QLoRA with 4-bit quantization of the model weights include reduced size of the model and increased inference speed, while having a modest decrease in the overall model performance.\n",
    "\n",
    "For example, with QLoRA a 70B parameter model can be finetuned with 48 GB VRAM, in comparison to 780 GB VRAM required for finetuning all weights of the original model (using 32-bit floating-point precision). Similarly, QLoRA enables to train the smaller version of LlaMA 2 with 7B parameters on a T4 GPU (provided by Google Colab) that has 16 GB VRAM. In cases when only a single GPU is available, using quantization is necessary for finetuning LLMs."
   ],
   "metadata": {
    "id": "SfoWFu-JjxUr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 21.4 Finetuning Example: Finetuning LlaMA-2 7B<a name='21.4-finetuning-example:-finetuning-llama-2-7b'></a>"
   ],
   "metadata": {
    "id": "n8BHP3m4ntWm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Import Libraries\n",
    "\n",
    "We will begin by installing the required libraries and importing modules from these packages. These include `accelerate` (for optimized training on GPUs), `peft` (for Parameter-Efficient Fine-Tuning), `bitsandbytes` (to quantize the LlaMA model to 4-bit precision), `transformers` (for working with Transformer Networks), and `trl` (for supervised finetuning, where trl stands for Transformer Reinforcement Learning)."
   ],
   "metadata": {
    "id": "kC7SG1ZO5L2y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q accelerate peft bitsandbytes transformers trl"
   ],
   "metadata": {
    "id": "ZJrRD657TXc1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553391264,
     "user_tz": 420,
     "elapsed": 6772,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    TrainingArguments, pipeline, logging)\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ],
   "metadata": {
    "id": "-gVbHOdK5hdU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553442895,
     "user_tz": 420,
     "elapsed": 49387,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Model\n",
    "\n",
    "We will download the smallest version of LlaMA-2-Chat model with 7B parameters from Hugging Face. Understandably, the larger LlaMA 2 models with 13B and 70B parameters require larger memory and computational resources for finetuning.\n",
    "\n",
    "Also, we will use the BitsAndBytes library to apply quantization with 4-bit precision format for loading the model weights. Loading a quantized model reduces the GPU memory requirement and makes it possible to train the model with a single GPU, as a tradeoff for some loss in precision. In the next cell we define the configuration for BitsAndBytes, and afterward we will use the configuration in the `from_pretrained` function to load the LlaMA 2 model. The parameters in BitsAndBytes configuration are described in the commented code below.\n",
    "\n",
    "The compute type in the cell below refers to the data format for performing computations, and it can be either \"float16\", \"bfloat16\", or \"float32\" because computations are performed in either 16 or 32-bit precision. In this case, we specified to use `\"torch.float16\"` compute data type (i.e., 16-bit floating-point numbers) for memory-saving purposes. Note that although the model weights are loaded with 4-bit precision, the weights are dequantized to 16-bit precision for performing the calculations for the forward and backward passes through the network, since 4-bit precision is too low for performing the calculations."
   ],
   "metadata": {
    "id": "DqCbomvC5L49"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# The model is Llama 2 from the Hugging Face hub\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\""
   ],
   "metadata": {
    "id": "gChA0lhb7MpV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553442900,
     "user_tz": 420,
     "elapsed": 2,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# BitsAndBytes configuraton\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    # Load the model using 4-bit precision\n",
    "    load_in_4bit=True,\n",
    "    # Quantization type (fp4 or nf4)\n",
    "    # nf4 is \"normalized float 4\" format, uses an asymmetric quantization scheme with 4-bit precision\n",
    "    # optimized for normally distributed weights (better than fp4 for neural networks)\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # Compute dtype for 4-bit models\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    # Use double quantization for 4-bit models\n",
    "    # Double quantization applies further quantization to the quantization constants\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ],
   "metadata": {
    "id": "9vqy2DA-7Igl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553442902,
     "user_tz": 420,
     "elapsed": 1,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use `AutoModelForCausalLM` to load the model with the `from_pretrained` function, and we will use the above BitesAndBytes configuration to load the model parameters with 4-bit precision.\n",
    "\n",
    "In the following cell we will load the corresponding tokenizer for LlaMA 2 by using `AutoTokenizer` and `from_pretrained`."
   ],
   "metadata": {
    "id": "NjKpW3FwVOsN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load Llama 2 model from Hugging Face\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # Apply quantization by using the bnb configuration from the previous cell\n",
    "    quantization_config=bnb_config,\n",
    "    # Don't cache the model weights, load the model weights from Hugging Face\n",
    "    use_cache=False,\n",
    "    # Trade-off parameter in Llama-2, less important, it should be 1 in most cases\n",
    "    pretraining_tp=1,\n",
    "    # Load the entire model on the GPU if available\n",
    "    device_map=\"auto\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "eca434ff689f4231b8390e9bdbbb90df",
      "79c1f1e5765744f182fbb9f15cebd314",
      "10acfa8e4eba4f9aac8dc08504ec3f74",
      "c544d11e0a77499986643766bbaec12c",
      "86a45f449cc649168568d952b7ae8008",
      "83b80c8796d14708874454aa23cd9dcf",
      "d243938b183849eba78ea494665cc2a3",
      "81179fc5cdd54402a66c04469dc691f9",
      "0a492dc90381473db77b9f7f7e02f30a",
      "77a05f02dcbc42369dd9f375ebf2231e",
      "964bed43bd7e4302bbccb5a20cee7cfa"
     ]
    },
    "id": "6PGB9V7H8KNk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553465381,
     "user_tz": 420,
     "elapsed": 22478,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "379cd332-421f-4205-8575-9a2d90bb02e7"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eca434ff689f4231b8390e9bdbbb90df"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "metadata": {
    "id": "m0k7PDwRUHGT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553465397,
     "user_tz": 420,
     "elapsed": 14,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load tokenizer from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# Needed for LlaMA tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Fix an overflow issue with fp16 training\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "id": "QQhVU0lh8n2j",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553465836,
     "user_tz": 420,
     "elapsed": 438,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define LoRA Configuration"
   ],
   "metadata": {
    "id": "gEBqQTb8vXy9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, the model will be packed into the LoRA format, which will introduce additional weights and keep the original weights frozen. The parameters in the LoRA configuration include:\n",
    "\n",
    "- `r`, determines the rank of update matrices, where lower rank results in smaller update matrices with fewer trainable parameters, and greater rank results in more trainable parameters but more robust model.\n",
    "- `lora_alpha`, controls the LoRA scaling factor.\n",
    "- `lora_dropout`, is the dropout rate for LoRA layers.\n",
    "- `bias`, specifies if the bias parameters should be trained.\n",
    "- `task_type`, is Causal LLM for the considered task."
   ],
   "metadata": {
    "id": "lyBp3pMo5L7F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    # LoRA rank dimension\n",
    "    r=64,\n",
    "    # Alpha parameter for LoRA scaling\n",
    "    lora_alpha=16,\n",
    "    # Dropout rate for LoRA layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")"
   ],
   "metadata": {
    "id": "jFZQWZhZVTsw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553465860,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to understand how LoRA impacts the finetuning of LlaMA 2 model, let's compare the total number of trainable parameters in LLaMA 2 and the trainable parameters for the LoRA model. As we can note in the cell below, the LoRA model has about 67M trainable parameters, which is about 1% of the 7B total trainable parameters in LlaMA 2. This makes it possible to finetune the model on a single GPU."
   ],
   "metadata": {
    "id": "z6sBPVs-dIkx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def print_number_of_trainable_model_parameters(model, use_4bit=True):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    if use_4bit:\n",
    "        all_model_params *= 2\n",
    "        trainable_model_params *= 2\n",
    "    print(f\"Total model parameters: {all_model_params:,d}. Trainable model parameters: {trainable_model_params:,d}. Percent of trainable parameters: {100 * trainable_model_params/ all_model_params:4.2f} %\")"
   ],
   "metadata": {
    "id": "0_yK0mSQvWwv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553465861,
     "user_tz": 420,
     "elapsed": 5,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# compare the number of trainable parameters to QLoRA model\n",
    "qlora_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# print trainable parameters\n",
    "print_number_of_trainable_model_parameters(qlora_model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dxCd5Vxb0mv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553467601,
     "user_tz": 420,
     "elapsed": 1743,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "3d363dc2-7f10-45e5-fe78-d5473c3aec31"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total model parameters: 7,135,043,584. Trainable model parameters: 134,217,728. Percent of trainable parameters: 1.88 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Dataset\n",
    "\n",
    "We will use the [Lamini docs](https://huggingface.co/datasets/lamini/lamini_docs) dataset, which contains questions and answers about the framework Lamini for training and developing Language Models. The dataset contains 1,260 question/answer pairs. Here are a few samples from the dataset.\n",
    "\n",
    "|Question |Answer\n",
    "| :---- | :---\n",
    "|Does Lamini support generating code|Yes, Lamini supports generating code through its API.\n",
    "|How do I report a bug or issue with the Lamini documentation?| You can report a bug or issue with the Lamini documentation by submitting an issue on the Lamini GitHub page.\n",
    "|Can Lamini be used in an online learning setting, <br /> where the model is updated continuously as new data becomes available?|It is possible to use Lamini in an online learning setting where the model is updated continuously as new data becomes available. <br /> However, this would require some additional implementation and configuration to ensure that the model is updated appropriately and efficiently."
   ],
   "metadata": {
    "id": "bNUSM8ckMwMG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A preprocessed version of the dataset in a format that matches the instruction-output pairs for LlaMA 2 is available on Hugging Face, and we will directly load the preprocessed version of the dataset."
   ],
   "metadata": {
    "id": "gHiyZE0Qwtny"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Lamini dataset\n",
    "dataset = load_dataset(\"mwitiderrick/llamini_llama\", split=\"train\")"
   ],
   "metadata": {
    "id": "up6AcG5hMqhO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553469282,
     "user_tz": 420,
     "elapsed": 1682,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'Number of prompts: {len(dataset)}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-wVL7a4WAgJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553469300,
     "user_tz": 420,
     "elapsed": 13,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "347786e5-7073-4d42-b11a-256db6b963a1"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of prompts: 1260\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Training\n",
    "\n",
    "The next cell defines the training arguments, and the commented notes describe the arguments. Note that we will finetune the model for only 1 epoch (if we finetune for more than 1 epoch it will take longer but it will probably result in improved performance)."
   ],
   "metadata": {
    "id": "IGWUw7QSlREb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    # Output directory where the model predictions and checkpoints will be stored\n",
    "    output_dir=\"./results\",\n",
    "    # Number of training epochs\n",
    "    num_train_epochs=1,\n",
    "    # Batch size per GPU for training\n",
    "    per_device_train_batch_size=8,\n",
    "    # Number of update steps to accumulate the gradients for\n",
    "    gradient_accumulation_steps=2,\n",
    "    # Optimizer to use\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    # Save checkpoint every number of steps\n",
    "    save_steps=0,\n",
    "    # Log updates every number of steps\n",
    "    logging_steps=10,\n",
    "    # Initial learning rate (AdamW optimizer)\n",
    "    learning_rate=2e-4,\n",
    "    # Weight decay to apply\n",
    "    weight_decay=0.001,\n",
    "    # Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    # Maximum gradient normal (gradient clipping)\n",
    "    max_grad_norm=0.3,\n",
    "    # Group sequences with same length into batches (to minimize padding)\n",
    "    # Saves memory and speeds up training considerably\n",
    "    group_by_length=True,\n",
    "    # Learning rate schedule\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # Disable reporting to external tools (e.g., WandB, TensorBoard)\n",
    "    report_to=\"none\"\n",
    ")"
   ],
   "metadata": {
    "id": "xCpu1K5fGct0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553469329,
     "user_tz": 420,
     "elapsed": 28,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will use the `SFTTrainer` class in Hugging Face to create an instance of the model by passing the loaded LlaMA 2 model, training dataset, PeFT configuration, tokenizer, and the training arguments. `SFTTrainer` stands for Supervised Fine-Tuning Trainer."
   ],
   "metadata": {
    "id": "_KYUP14kOnKn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set supervised finetuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bB03IouIdxV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762553470770,
     "user_tz": 420,
     "elapsed": 1440,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "ee550795-5a3c-4fd9-f81b-4d4e519820c2"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can train the model with the `train()` function in Hugging Face. In the output of the cell we can see the loss for every 10 training steps, because we set `logging_steps=10` in the training arguments.\n",
    "\n",
    "The training took about 15 minutes on a T4 GPU with High-RAM memory on Google Clab Pro."
   ],
   "metadata": {
    "id": "X0zwKCAxPGwB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "qzYKR_FwJDSe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762554402576,
     "user_tz": 420,
     "elapsed": 931796,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "7f77721e-04fd-41d4-f3bf-ee04b4d58857"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 15:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.529200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.372700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.721200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.570400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.587400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.574900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=79, training_loss=0.9190209002434453, metrics={'train_runtime': 931.0002, 'train_samples_per_second': 1.353, 'train_steps_per_second': 0.085, 'total_flos': 1.133322031104e+16, 'train_loss': 0.9190209002434453, 'entropy': 0.4541758464442359, 'num_tokens': 273810.0, 'mean_token_accuracy': 0.898286329375373, 'epoch': 1.0})"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate Text\n",
    "\n",
    "To generate text with the trained model we will use the Hugging Face `pipeline` with the task set to `\"text-generation\"`. We can set the length of the generated text tokens with the `max_length` argument.\n",
    "\n",
    "The output displays the start `<s>[INST]` and end `[/INST]` of the instruction prompt, followed by the generated output by the model."
   ],
   "metadata": {
    "id": "gnLrKvauJV4z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set model to inference mode\n",
    "model.config.use_cache = True\n",
    "model.eval()\n",
    "\n",
    "# User's prompt\n",
    "prompt = \"What are Lamini models?\"\n",
    "\n",
    "# Run text generation pipeline with the finetuned model\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer,\n",
    "                max_length=200, do_sample=True, temperature=0.7, top_p=0.9, repetition_penalty=1.2)\n",
    "\n",
    "# Generare response\n",
    "output = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "\n",
    "# Print the response\n",
    "print(output[0]['generated_text'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "471QJli9KEo4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762554470325,
     "user_tz": 420,
     "elapsed": 21379,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "925b4aaa-8650-411b-9daa-ccce7fc43579"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s>[INST] What are Lamini models? [/INST]  Lamini is an AI-based language model that uses a combination of natural language processing (NLP) and machine learning algorithms to generate text. nobody owns the rights to any specific work shared on LLM; instead, users grant permission for their posts to be used by other community members. The generated content must not violate the terms of service or infringe upon copyright laws.\n",
      "\n",
      "There are several different types of Lamini models available:\n",
      "\n",
      "1. Text Generator - This model generates text based on prompt input provided. It can write anything from short answers to lengthy stories with ease. Users may use this feature if they want help generating ideas for writing projects or simply need inspiration when brainstorming topics related to their interests!\n",
      "2. Question Answerer - Using pre-existing documentation about Lamini as well as external knowledge sources such as Google search results helps answer\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Another prompt\n",
    "prompt = \"How to evaluate the quality of the generated text with Lamini models\"\n",
    "output = pipe(f\"<s>[INST] {prompt} [/INST]\", max_new_tokens=500)\n",
    "print(output[0]['generated_text'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWj51bGaL1m8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762554631076,
     "user_tz": 420,
     "elapsed": 56823,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "02024632-b487-48ce-87f0-2c0bf305e39d"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s>[INST] How to evaluate the quality of the generated text with Lamini models [/INST]  Evaluating the quality of generated text from a language model like Lamini can be done using various metrics and methods. Unterscheidung between realistic and non-realistic text generation, evaluating coherence and fluency, assessing accuracy in specific domains or tasks are some ways you could go about this evaluation process.\n",
      "\n",
      "Here are some common evaluation methods for generating text:\n",
      "\n",
      "1. Perplexity (PP): Measure how well the generated text fits the context by calculating perplexity, which is the ratio of the probability of the correct output given the input. Lower values indicate better fit.\n",
      "2. BLEU score (B): Assess the similarity between the generated text and ground truth by comparing their n-gram frequency distributions. A higher BLEU score indicates more similarities between the two texts.\n",
      "3. ROC curve (R): Generate test data from a known dataset and train a classifier on it; then use that trained model to predict whether a given sample contains valid or invalid information. The area under the curve represents overall performance.\n",
      "4. F1-score (F1): Calculate the harmonic mean of precision and recall when judging the generations' ability to generate grammatically correct sentences. Higher scores demonstrate improved linguistic understanding and translation skills.\n",
      "5. Distinct Ratio (DR): Compare the number of unique words used in each sentence across both training and testing sets. A higher distinct ratio shows the generator produces more diverse outputs rather than repeating phrases too frequently.\n",
      "6. Frequency Weighted Likelihood (FWL) measures the likelihood of each word being correctly generated based on its frequency in the target distribution. This metric focuses on high-frequency words, as they contribute most significantly towards accurate text generation.\n",
      "7. Character Accuracy Rate (AC): Count instances where all characters within a given sequence match corresponding character positions in either training data or desired result. It provides an overview of the model's ability to produce accurate letter-level predictions.\n",
      "8. Fluency Score(FS): Assesses how natural and fluent the generated text appears. It does so by analyzing factors such as grammar usage consistency, sentence structure appropriateness according to the style provided during training or prompts provided after generation completion etcetera. This gives insights into whether your content flows smoothly enough without sounding forced or\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Another prompt\n",
    "prompt = \"Write a poem about Data Science\"\n",
    "output = pipe(f\"<s>[INST] {prompt} [/INST]\", max_new_tokens=800)\n",
    "print(output[0]['generated_text'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQxWZNnDKrAy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762554688341,
     "user_tz": 420,
     "elapsed": 27181,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "e6a2dd2d-3dc3-4d50-fc8d-bb54366d3bcd"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s>[INST] Write a poem about Data Science [/INST]  In the realm of numbers, we find our home\n",
      "\n",
      "Where truths and insights are waiting to be known\n",
      "A world where patterns and trends unfold\n",
      "In every dataset that's been told.\n",
      "\n",
      "With algorithms sharp as swords in hand\n",
      "We delve into complexity, ready to stand\n",
      "Against the noise that hides within\n",
      "And uncover secrets hidden from prying eyes again.\n",
      "\n",
      "From predictive models high on might\n",
      "To clustering groups bright with insight light\n",
      "Our tools help us navigate this sea\n",
      "Of data, both past and yet to be.\n",
      "\n",
      "From statistics so pure and true\n",
      "To machine learning, too, we break through\n",
      "The barriers that once seemed impossible to pierce\n",
      "And make connections that before were not even near.\n",
      "\n",
      "Data science is more than just machines\n",
      "Its art, creativity, and dreams\n",
      "That bring forth knowledge like a shining light\n",
      "And show the beauty deep inside whats right.\n",
      "\n",
      "So let us embrace this wondrous sight\n",
      "And see beyond mere facts tonight.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sampling"
   ],
   "metadata": {
    "id": "S51TYdLIxfFV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 21.5 Chat Templates for Formatting LLM Data <a name='21.5-chat-templates-for-formatting-llm-data'></a>"
   ],
   "metadata": {
    "id": "sQuMJ94DxF-i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a chat context, LLMs have a continuing conversation with users consisting of one or more messages. Chat conversations are typically represented as a list of dictionaries, where each dictionary contains *role* and *content* keys. I.e., each message is assigned a \"role\" and it contains the \"text\" of the message. The roles are typically:\n",
    "\n",
    "\"system\" for directives on how the model should behave\n",
    "\"user\" for messages from the user\n",
    "\"assistant\" for messages from the LLM\n",
    "\n",
    "An example is provided below, showing the three roles: system, user, and assistant. The prompt to the LLM includes a system message that is prepended to the user's message, and the completion by the LLM is the response by the assistant.\n",
    "\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\"role\": \"system\", \"content\":\"You are a helpful and honest assistant.\"},\n",
    "  {\"role\":\"user\", \"content\":\"What is the capital city of U.S.\"},\n",
    "  {\"role\": \"assistant\",\"content\":\"The capital of the United States is Washington, D.C.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "A *system message* is usually provided at the beginning of the conversation and includes guidance about how the model should behave in the chat. System messages can be short, such as \"Speak like a pirate\", or they can be long and contain a lot of context to define the behavior of the LLM. For instance, when you open a new chat with ChatGPT, an internal system message is automatically prepended to your first prompt; however, the system message is not shown to the user. Also, instruction-following datasets include the system message as the first part of the question for the assistant.\n",
    "\n",
    "In ongoing multi-turn conversations, the messages list continues to grow with alternating user and assistant messages. Each exchange is added to the list in order."
   ],
   "metadata": {
    "id": "8_rOJ4LxAcNw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The role information is injected by adding control tokens between messages to indicate the relevant roles and the message boundaries. Let's inspect the first question-answer pair in the Lamini dataset shown below, which has been formatted for the LlaMA 2 model. We can notice that LLaMA 2 uses special tokens for start-of-sequence `<s>` and end-of-seqence `</s>` to define the beginnings and ends of conversations. It uses the start-of-instruction tag `[INST]` and end-of-instruction tag `[/INST]` for single instruction-response pairs. I.e.,  everything inside `[INST]` and `[/INST]` is structured into system, user, and assistant roles. The system message is wrapped in `<<SYS>>` and `<</SYS>>` tags. The text `### Question\"` marks the user's instruction/question for the model. The text `### Answer:` contains the response by the assistant.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "hpwrl9O7UY5R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TkWVVV_Uyad",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762554695264,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "3c2b17e3-afcd-453a-e75a-16360ec0fe51"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': \" <s>[INST] <<SYS>> You are a honest and helpful assistant who helps users find answers quickly from the given docs about Lamini. \\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information.\\nIf the answer can not be found in the text please respond with `Let's keep the discussion relevant to Lamini docs`. <</SYS>>\\n\\n### Question: How can I evaluate the performance and quality of the generated text from Lamini models?\\n### Answer: There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\\n[/INST] </s>\\n\"}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(dataset[0]['text'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfVKwmCJUyed",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762554700130,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "c73935c8-6508-459b-8251-2b0e076bafe6"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " <s>[INST] <<SYS>> You are a honest and helpful assistant who helps users find answers quickly from the given docs about Lamini. \n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
      "If you don't know the answer to a question, please don't share false information.\n",
      "If the answer can not be found in the text please respond with `Let's keep the discussion relevant to Lamini docs`. <</SYS>>\n",
      "\n",
      "### Question: How can I evaluate the performance and quality of the generated text from Lamini models?\n",
      "### Answer: There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\n",
      "[/INST] </s>\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " Unfortunately, there is no standard regarding which tokens to use for those purposes, and different LLMs have been trained with varying formatting and control tokens. This can be a challenge for users, because using the wrong format may confuse the model and result in poor quality responses.\n",
    "\n",
    "###  Chat Templates\n",
    "\n",
    "To resolve this problem, **chat templates** have been developed to format a conversation for a given LLM into a tokenizable sequence. The templates are formatting specifications stored within a tokenizer that define how to structure conversational data for a specific model.\n",
    "\n",
    "Hugging Face has developed the `apply_chat_template` method that reads the template stored in the tokenizer's configuration and automatically converts a list of message dictionaries with \"role\" and \"content\" keys into the properly formatted string that the model was trained on. The template is distributed alongside the tokenizer so users don't need to manually learn or implement each model's conversation format. The users just provide messages in a standard structure, and the tokenizer handles the model-specific formatting automatically.\n",
    "\n",
    "Consider again the following chat from above:"
   ],
   "metadata": {
    "id": "Reia4o8nYYNy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "  {\"role\": \"system\", \"content\":\"You are a helpful and honest assistant.\"},\n",
    "  {\"role\":\"user\", \"content\":\"What is the capital city of U.S.\"},\n",
    "  {\"role\": \"assistant\",\"content\":\"The capital of the United States is Washington, D.C.\"}\n",
    "]"
   ],
   "metadata": {
    "id": "o8o-NeJmdYKG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762554706095,
     "user_tz": 420,
     "elapsed": 5,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following cells, we import the tokenizers for `Qwen2.5-7B-Instruct` and `Mistral-7B-Instruct` LLMs, and afterward we apply the chat templates for these models. Notice in the formatted text that Qwen2.5 uses the instruction message start tag `<|im_start|>` and instruction message end tag `<|im_end|>` to separate the messages, followed by `system/user/assistant` to indicate the roles."
   ],
   "metadata": {
    "id": "Zknv4wGYY8td"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the Qwen tokenizer\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "# Apply chat template for Qwen\n",
    "formatted_text = tokenizer_1.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Print the formatted text\n",
    "print(formatted_text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "7e56db8c3f264cfeac0aac36328aeabf",
      "49f9a504b07342fd9b0334d0750a47c0",
      "8accc5e716a0423d8a43f195d41c3c74",
      "9e6d5d728bbe40aea2fccca4947f6315",
      "e961de0edc684167b491817204c8531c",
      "172088c933164e9091a75eacc250c88d",
      "f5e1202fba334a268248579004687dee",
      "c6afcd8d14af4466bc8fb1eb85e39231",
      "0b92b577ce974111b884ec3ea5b2920c",
      "b8d8adc953c940cf83287efd3ff9abf7",
      "fe58eb260b5b415eb8e11c87e59b969e",
      "0102346d3a5e414cb76f6e15b10c8ad3",
      "089951a219ca46be9a019f5ad87959c3",
      "0dd717241581429ba89af37d2a38685a",
      "402fcc9fdffd43eab68dd9a6a92caaec",
      "d6b15609864c4301965d03d9b06ba974",
      "d1a5a2650ce9415c8e580dbf1a44e83b",
      "85f4f795e34c4c86b91efe803cfac431",
      "b99ea5c5351b497586d734366af25a02",
      "042be4fbb687490fa1e27f06af4fcc36",
      "3315f66ebc6a45cdaf352e75c962580e",
      "cb32b72b37ee4c73831b8c66b5b2f182",
      "8e7beb240bef4c059027216b7eb720ef",
      "94f39722e5384173a23ba766327702b5",
      "80c33ad0fecc4bbda847b66bd4742192",
      "e957ed718c6e46b7a2acd52537f61147",
      "bb3b2739580f4bfa966306c2b733cdeb",
      "e63ccee1dd7d48ddae1ef2d8f70e8301",
      "cf0642ab59c84018addc0c1e5df79d7a",
      "fce7bc72c8f24805a8b8bec7e62a24c8",
      "da10b944efe8497b9b4858734ea62b70",
      "07ff330321e3444d90270f5faf3afbed",
      "096467641be347a2badccf1034f557ca",
      "ee02f8a9c8c5494da63d3cda9bff946a",
      "32e97d99810b4c56becc5595b4c85364",
      "e670800bd6f84f38b4b999d3aa428aef",
      "813bcb624e13481eae25f38b7b659fcb",
      "d18895060e8548848475fb666a7d1d6c",
      "6fada90bec17498bad230679da6b0ac9",
      "b7b488e3ff60494da606d6d0e0274aad",
      "2a89590c99a94c18ad4bb6e8113403c4",
      "c1594b20c2af425581a18d953e88ac0a",
      "08ed9f4abc0b4325984a4dc9d977277c",
      "e0614aa782cd47439b61af5bd0a0911f"
     ]
    },
    "id": "OLibgZoddYMg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762554709459,
     "user_tz": 420,
     "elapsed": 1560,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "dbbc5464-1927-4767-86a7-70707b1fa6a4"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e56db8c3f264cfeac0aac36328aeabf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0102346d3a5e414cb76f6e15b10c8ad3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e7beb240bef4c059027216b7eb720ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee02f8a9c8c5494da63d3cda9bff946a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful and honest assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital city of U.S.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The capital of the United States is Washington, D.C.<|im_end|>\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The format for Mistral is similar to the LlaMA 2 format, and uses `<s>` and `</s>` for sequence start and end, and `[INST]` and `[/INST]` for the user's instruction start and end. The text after `[/INST]` until `</s>` is the assistant's response."
   ],
   "metadata": {
    "id": "nzpewXJ1Jxt3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the Mistral tokenizer\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "# Apply chat template for Mistral\n",
    "formatted_text = tokenizer_2.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Print the formatted text\n",
    "print(formatted_text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200,
     "referenced_widgets": [
      "36dc85fd9e784693b23e8bf58657fa88",
      "cfd2944cc70c4ab48dddca99749b869a",
      "554509d3d6394282ba39f2e12e9a6ec2",
      "dbdae6ea733f4026bd701e5cbd77e402",
      "7572484f363f44888fbf5f661ebbbff3",
      "7a738ea973dd4df3a8578cef47aab471",
      "b96e12b85f6c4cef8713e0bffc333dac",
      "8460a59adfc94be28d5d70eba00a3dc7",
      "836478c3402540bfa2f1f0f87e67e3e7",
      "8d99f96beabf477cb26cb78394c9284e",
      "8b686453956d4635a23a0d5ab26c80bf",
      "edde663fb8df4af3a8cf11d840e65047",
      "51a0afec221e49aa837d69a03dbba00e",
      "901d1f15ce5a4bdb99f8af613b3b6c46",
      "7a8b7e4aeb494ad2b7f94fbbdb8d1219",
      "366601bdee05435ebf1bbe4c28b25337",
      "d0757387781942a9aedf60360d8656fa",
      "fb608b50795f4d55b805fea9e5177842",
      "170a69e4b77645f48a21272e865d3149",
      "a0e8f90b853a4696803a262e858201bb",
      "2969621e9468446d812e9914a37da1d6",
      "efea8783d0d84766a1706a39c7d47184",
      "143a452a7284480d81deef106d8749fb",
      "6d1f7c6634304703b87f2016e865e163",
      "4ac327531add48d083f911355e049009",
      "f8bfd802f1d34108bb0d49b0ff169328",
      "2a7f843f92b3465db321dae5c388ae84",
      "74cab04b7a094021a7c659924137c90a",
      "c104a3b9a67740029fdb192f9995b1f8",
      "a81c1dbf35fc4fb88a8d93e7c4267735",
      "da116b8ba5694ee7b489eaa723996d54",
      "f4133af4bde0446c8ff4f507664d9590",
      "e6f7c8822fbf4ef59908baaf7d6785e6",
      "b67fa10c20e2407b87dfdff4fab586d4",
      "3ac111071c18489ea9fa297fa2a3b93c",
      "f60fafb90d584448a1f20e6d26391abf",
      "930749eea98c4ba7a98411462f65dc77",
      "4153f006a0654b70aa5b62a2861fd8ec",
      "351d81ae942545d1bb31d3273ceb31ee",
      "e046fb9a5dd54f958fd14ca6d28b28c7",
      "02389bcb03d84d7092832cdf2e60f564",
      "cd7c3b9206114d17bc206aad44a86f7a",
      "a134911fb12d46c5af6e81510cab1ad9",
      "9aa55b56ba594808a9dc53693c6d45ee"
     ]
    },
    "id": "gQoMpo4yeS9J",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762554714061,
     "user_tz": 420,
     "elapsed": 1233,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "d5881272-a3f9-43ff-bcf9-fe75a46a1c6a"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36dc85fd9e784693b23e8bf58657fa88"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edde663fb8df4af3a8cf11d840e65047"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "143a452a7284480d81deef106d8749fb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b67fa10c20e2407b87dfdff4fab586d4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s> [INST] You are a helpful and honest assistant.\n",
      "\n",
      "What is the capital city of U.S. [/INST] The capital of the United States is Washington, D.C.</s>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is important to always use the chat template associated with the specific LLM you are working with to ensure proper formatting and optimal performance.\n",
    "\n",
    "### Generate Response using Chat Template\n",
    "\n",
    "The next cell presents an example of prompting the Mistral 7 B model to generate a new response. The tokenizer and model for Mistral 7B are first loaded. In the `apply_chat_template` function we set `tokenize=True` to produce tokenized messages, which are afterward used for model inference. Note that in the above examples we set `tokenize=False`, which formatted the messages but did not tokenize them. Also, in this case the `messages` list does not include the assistant role, as the LLM will generate the response."
   ],
   "metadata": {
    "id": "eROzAGFgKrET"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the Mistral tokenizer and model\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", device_map=\"auto\", dtype=torch.bfloat16)\n",
    "\n",
    "# Prompt text\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "\n",
    "# Apply chat template for Mistral\n",
    "tokenized_chat = tokenizer_2.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model_2.device)\n",
    "\n",
    "# Print the tokenized text\n",
    "print(tokenizer_2.decode(tokenized_chat[0]))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "20fcee65fece43feac5cc079e96ed549",
      "ad77727140434aecae9bd118820d3947",
      "696ddc4bc18f4384ab2ffbc691b365cb",
      "74f5d07ad689487898a7c8e9c3efe89f",
      "32f37d188433459bbf2a9d532f1a8861",
      "8c8280dea8bb41acbe5e368789dff9e6",
      "4024d0a8360647f8b67962c28db14280",
      "54f56ed94f054372bcb317c215d5e4fd",
      "8b356b67967f420d80dd2c288f25f806",
      "b9d220fd1cec4f84b08b010682e890b2",
      "4ac0f08f92344bcea59fb18a0d519a4b"
     ]
    },
    "id": "DjBozieSOb_v",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762554944492,
     "user_tz": 420,
     "elapsed": 719,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "8ee5345b-07b5-442a-8f94-05ea07921204"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20fcee65fece43feac5cc079e96ed549"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s> [INST] You are a friendly chatbot who always responds in the style of a pirate\n",
      "\n",
      "How many helicopters can a human eat in one sitting? [/INST]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pass the tokenized chat to `generate()` to generate a response."
   ],
   "metadata": {
    "id": "QAboJGHGYMTY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate a response by the model\n",
    "outputs = model_2.generate(tokenized_chat, max_new_tokens=128, pad_token_id=tokenizer_2.eos_token_id)\n",
    "\n",
    "# Print the response\n",
    "print(tokenizer_2.decode(outputs[0]))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5m4Om5inItt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762555316640,
     "user_tz": 420,
     "elapsed": 209718,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "ab34d58b-39c2-4e94-ff6c-85e41978cd60"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s> [INST] You are a friendly chatbot who always responds in the style of a pirate\n",
      "\n",
      "How many helicopters can a human eat in one sitting? [/INST] Ahoy there, matey! A human can't eat a helicopter in one sitting, no matter how much they might want to. They're just too big and not made for consumption. But a hearty stew of fish and vegetables might hit the spot, me hearties!</s>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `apply_chat_template()` method works with any model on Hugging Face that has a chat template defined in its tokenizer configuration, which include LlaMA, Mistral, Zephyr, Phi, Qwen and other models. Most modern conversational models include chat templates by default, which can be checked by looking for a `chat_template` field in the tokenizer's `tokenizer_config.json` file. If a model doesn't have a built-in chat template, we can still either prepare a custom template or we can manually format the text sequences according to the model's documentation.\n",
    "\n",
    "### Dataset Preparation with Chat Template\n",
    "\n",
    "The next cell shows how to apply a chat template to prepare a dataset for model training. The dataset consists of two simple question-answer conversations stored in a dictionary with \"role\" and \"content\" fields. The `format_chat` function takes each example from the dataset and applies the tokenizer's chat template, and returns a dictionary containing the formatted text under the key \"formatted_chat\". By using `dataset.map(format_chat)`, the formatting function is applied to every conversation in the dataset. The `tokenize=False` parameter means the output remains as text rather than token IDs, and `add_generation_prompt=False` indicates we are formatting complete conversations rather than prompts that expect a response.\n",
    "\n"
   ],
   "metadata": {
    "id": "XeBfstVyagEf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Prepare a dataset with 2 chats\n",
    "chat1 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n",
    "]\n",
    "chat2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n",
    "]\n",
    "\n",
    "# Create a simple dataset\n",
    "dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n",
    "\n",
    "# Define a formatting function\n",
    "def format_chat(example):\n",
    "    return {\"formatted_chat\": tokenizer_2.apply_chat_template(example[\"chat\"], tokenize=False, add_generation_prompt=False)}\n",
    "\n",
    "# Apply the chat template to the dataset\n",
    "dataset = dataset.map(format_chat)\n",
    "\n",
    "# Print the formatted dataset\n",
    "for chat in dataset['formatted_chat']:\n",
    "  print(chat)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "e9b4aaee4875442f96791c360d9591ac",
      "ce79b2201a584223a61e338b11f8a224",
      "7128809fac244acfbfcf046c686ef4f0",
      "788786ac6b4e4451a40251c606020879",
      "5feb7ea7c005440b95fa4f09b8de8fdc",
      "d396372df6174abfa0be1cf9b4d69269",
      "64568e1ad3604536a4a6a3b8c198b983",
      "d55525ae98084e80aa49dc08c11a4c79",
      "82fd4357495f481a9211eb3bcbe7f591",
      "aeb2ff35b55a4d619ae45b7bf198e75b",
      "0f5428af6967488e9a38cd38ba78dea5"
     ]
    },
    "id": "ney5KzQyhZaI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762557309072,
     "user_tz": 420,
     "elapsed": 85,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "23ec943f-3152-46ac-b4e9-c1f9f98603f5"
   },
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9b4aaee4875442f96791c360d9591ac"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s> [INST] Which is bigger, the moon or the sun? [/INST] The sun.</s>\n",
      "<s> [INST] Which is bigger, a virus or a bacterium? [/INST] A bacterium.</s>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 21.6 Prompt Engineering <a name='21.6-prompt-engineering'></a>"
   ],
   "metadata": {
    "id": "nT8PV_WTlAaJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Prompt engineering** is a technique for improving the performance of LLMs by providing detailed context and information about a specific task. It involves creating text prompts that provide additional information or guidance to the model, such as the topic of the generated response. With prompt engineering, the model can better understand the kind of expected output and produce more accurate and relevant results.\n",
    "\n",
    "The following tips for creating effective prompts as part of prompt engineering can improve the performance of LLMs:\n",
    "\n",
    "- Use clear and concise prompts: The prompt should be easy to understand and provide enough information for the model to generate relevant output. Avoid using jargon or technical terms.\n",
    "- Use specific examples: Providing specific examples can help the model better understand the expected output. For example, if you want the model to generate a story about a particular topic, include a few sentences about the setting, characters, and plot.\n",
    "- Vary the prompts: Use prompts with different styles, tones, and formats to obtain more diverse outputs from the model.\n",
    "- Test and refine: Test the prompts on the model and refine them by adding more detail or adjusting the tone and style.\n",
    "- Use feedback: Use feedback from users or other sources to identify areas where the model needs more guidance and make adjustments accordingly.\n",
    "\n",
    "*Chain-of-thought technique* involves providing the LLM with a series of instructions to help guide the model and generate a more coherent and relevant response. This technique is useful for obtaining well-reasoned responses from LLMs.\n",
    "\n",
    "An example of a chain-of-thought prompt is as follows: \"You are a virtual tour guide from 1901. You have tourists visiting Eiffel Tower. Describe Eiffel Tower to your audience. Begin with (1) why it was built, (2) how long it took to build, (3) where were the materials sourced to build, (4) number of people it took to build it, and (5) number of people visiting the Eiffel tour annually in the 1900's, the amount of time it completes a full tour, and why so many people visit it each year. Make your tour funny by including one or two funny jokes at the end of the tour.\""
   ],
   "metadata": {
    "id": "25kN3VP7lApv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 21.7 Foundation Models <a name='21.7-foundation-models'></a>\n",
    "\n",
    "**Foundation Models** are extremely large NN models trained on tremendous amounts of data with substantial computational resources, resulting in high capabilities for transfer learning to a wide range of downstream tasks. In other words, these models are scaled along each of the three factors: number of model parameters, size of the training dataset, and amount of computation. And, they are typically trained using self-supervised learning on unlabeled data. The scale of Foundation Models leads to new emergent capabilities, such as the ability to perform well on tasks that the models were not explicitly trained to do. This allows few-shot learning, which refers to finetuning Foundation Models to new downstream tasks by using only a few training data instances for the new task. Similarly, zero-shot learning extends this concept even further, and refers to a model's ability to generalize to new tasks for which the model hasn't seen any examples during the training.\n",
    "\n",
    "LLMs represent early examples of Foundation Models, because LLMs are trained at scale and can be adapted for various NLP tasks, even for tasks they were not trained to perform.\n",
    "\n",
    "The term Foundation Models is more general than LLMs, and they generally refer to large models that are trained on multimodal data, where the inputs can include text, images, audio, video, and other data sources.\n",
    "\n",
    "The importance of Foundation Models is in their potential to replace task-specific ML models that are specialized in solving one task (i.e., optimized to perform well on one dataset) with general models that have the capabilities to solve multiple tasks. I.e., these models can serve as a foundation that is adaptable to a broad range of applications.\n",
    "\n",
    "<img src=\"images/foundation_model.jpg\" width=\"600\">\n",
    "\n",
    "*Figure: Foundation model.* Source: [link](https://blogs.nvidia.com/blog/2023/03/13/what-are-foundation-models/)."
   ],
   "metadata": {
    "id": "BnFBFVAEe1my"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 21.8 Limitations and Ethical Considerations of LLMs <a name='21.8-limitations-and-ethical-considerations-of-llms'></a>"
   ],
   "metadata": {
    "id": "RUNB0noQXlQz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although LLMs have demonstrated impressive performance across a wide range of tasks, there are several limitations and ethical considerations that raise concerns.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- *Computational resources*: Training LLMs requires significant computational resources, making it difficult for researchers with limited access to GPUs or specialized hardware to develop and use these models.\n",
    "- *Data bias*: LLMs are trained on vast amounts of data from the internet, which often contain biases present in the data. As a result, the models may unintentionally learn and reproduce biases in their generated responses.\n",
    "- *Producing hallucinations*: LLMs can produce hallucinations, which are responses that are false, inaccurate, unexpected, or contextually inappropriate. One example of hallucination by ChatGPT is when asked to list academic papers by an author, and it provides papers that don't exist.\n",
    "- *Inability to explain*: LLMs are inherently black-box models, making it challenging to explain their reasoning or decision-making processes, which is essential in certain applications like healthcare, finance, and legal domains.\n",
    "\n",
    "\n",
    "Ethical considerations:\n",
    "\n",
    "- *Privacy concerns*: LLMs memorize information from their training data, and can potentially reveal sensitive information or violate user privacy.\n",
    "- *Misinformation and manipulation*: Text generated by LLMs can be exploited to create disinformation, fake news, or deepfake content that manipulates public opinion and undermines trust.\n",
    "- *Accessibility and fairness*: The computational resources and expertise required to train LLMs may lead to an unequal distribution of benefits, where only a few organizations have the resources to develop and control these powerful models.\n",
    "- *Environmental impact*: The large-scale training of LLMs consumes a significant amount of energy contributing to carbon emissions, which raises concerns about the environmental sustainability of these models.\n",
    "\n",
    "Conclusively, it is important to encourage transparency, collaboration, and responsible AI practices to ensure that LLMs benefit all members of society without causing harm."
   ],
   "metadata": {
    "id": "WHbgR7cBc7jQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Appendix: Unsloth Library for LLM Training and Inference <a name='appendix:-unsloth-library-for-llm-training-and-inference'></a>"
   ],
   "metadata": {
    "id": "wqMGfKQFqepe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Unsloth** is another library for training and inference of LLMs, offering tools to facilitate optimization of LLMs ([link](https://unsloth.ai/)) The library applies various optimization techniques to reduce the training and inference time in comparison to the Hugging Face library and other related libraries. As you will notice in the following code, the Unsloth tools use pre-built components from Hugging Face (such as `transformers`, `trl`) and adapt them to optimize various workflows for model training and inference.\n",
    "\n",
    "The following code [10] provides an example of finetuning LlaMA-3.1 8B model using a single T4 GPU. For this example, the training time was similar to training LlaMA 2 7B with the Hugging Face library above, as in both cases training for 1 epoch took about 15 minutes. On the other hand, while the largest batch size (in multiples of 2) with Hugging Face was 8 samples, Unsloth allowed to use a batch size of 16, meaning that Unsloth optimized the memory usage. Training LLMs with larger batch sizes is related to reduced training variance and more stable gradient updates, which typically result in improved performance. In addition, the inference with Unsloth was faster."
   ],
   "metadata": {
    "id": "tV3OTXmItWMa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Note: to install unsloth in this notebook, I had to interupt the currently running kernel, and start a new kernel\n",
    "%%capture\n",
    "!pip install -q unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ],
   "metadata": {
    "id": "PLD3oFA_Z6Hj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762558499992,
     "user_tz": 420,
     "elapsed": 20826,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323,
     "referenced_widgets": [
      "c0e18d00bbd2450290d523fbc5336c9d",
      "d2719968fc9641d6afa6515285956489",
      "78331228a49841b18ce3d22ddfda8a04",
      "5bed4db71e584b949041ad9a421b363b",
      "27ab769395fa48729a91ffcb8ccbd9f3",
      "b981893bf40e4dbc90d1bdca8abe59f6",
      "fbd420ba1e374d2e817b0f206b82f5fc",
      "463332e94b324646ba55b756b418f97c",
      "07de6e3953724a8ca9f0bf31ecf195c4",
      "ba73b81d479f4d7aad2117f32fb0c902",
      "254ac32a888340f989b3842537d7726a",
      "df0d0768acfe4ce688b165573a895d33",
      "b0f865c6d114459891e0a514976cae0a",
      "467ed836f8e74aa999e03438a151f53a",
      "b593c612e0634821999414eae033909a",
      "036e83a50c7f4013b34359393ebe3e68",
      "e58ae77bfdab46a1a05dcb821b92396d",
      "5ff1eda0b7054cdbab7b98ccc8ea7b63",
      "13ab788416f34fdcb1d9b98764b32d57",
      "7f74ce9c91bc4c1487ef41aa1cd215d7",
      "97879c6a6a8b4a0590085474b28f7f7b",
      "48d093e354e049bebdc88c4173f52438",
      "0da5e1f632c14902a3c3abca4d3a129b",
      "06654e655f16440bbcb9300cce0ff99c",
      "f82781383b2741949ab9625d47bae637",
      "c92879f7fdc145a98ef27a8aaa003f5a",
      "c2561aae76b7486f8b0748f29e94422c",
      "e431c1991ea34cb0bd3621befbfe48d3",
      "332fdf804df341d8bebc0a2c5ae0155d",
      "b0f24b7e286a4094819f3f15ae8a294b",
      "b498d6eff7784998bbefa3c3b12704c3",
      "9d5bdeed13c740d8a8cb09c5f76c2513",
      "6460d6bfadca4bc08ce61632f2bd0a82",
      "b9ab4bc9b0b74fa5b3473d96844a9a0b",
      "410155960ec3459d90e0f5ac96f564b4",
      "0dfcc61617774e0e8ce9a617f64cccd5",
      "ddc0e9fa42fa4514ba84fe2b4c7bee08",
      "151ea171c81d465bac5be20d07ebf4f5",
      "795c06e1283344e5afc366e164e92def",
      "54ad8691b35c4705b4897b6593390bb6",
      "123d0339a2f547c08353dab811e07513",
      "23b5d54b8f70418c85a73ebd1fd07bef",
      "f847dab52ee14972af32b5a8c13e70ae",
      "17006dc6d20e4339844bbd59c185170a",
      "78bbb7c63e5d45d38c3a7cebc0a76e5f",
      "c1f12e7e99ea4010acc4f48728109d7f",
      "7102323bf4d84929bb65708bb280a56d",
      "0123b7090edd445d959c2ed9338703aa",
      "b38d7c831cd94535aecf13ead6a1f321",
      "f84f4b3db08444e2bd559366fd6bba4e",
      "442b4a8c546e4766999ffe12ced54157",
      "fff80417a5e04145badca55608c3e4ab",
      "67d51bad2b464fd4a4b9a92069d5a834",
      "a41efff370294ef5a07e979ae390c4f3",
      "be1fe8af27844f7fa13daf7cbe7710a0"
     ]
    },
    "id": "1jnwKADYr8Gn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762558642552,
     "user_tz": 420,
     "elapsed": 82221,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "0465586f-6928-43b1-9ab2-e96f5103caca"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      " Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0e18d00bbd2450290d523fbc5336c9d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df0d0768acfe4ce688b165573a895d33"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0da5e1f632c14902a3c3abca4d3a129b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9ab4bc9b0b74fa5b3473d96844a9a0b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78bbb7c63e5d45d38c3a7cebc0a76e5f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # Supports rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ],
   "metadata": {
    "id": "dTxPMx9jZ6J2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762558650421,
     "user_tz": 420,
     "elapsed": 7858,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "bebb8e4f-845c-4625-8b36-98f92c302a4f"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth 2025.11.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Lamini dataset\n",
    "dataset = load_dataset(\"mwitiderrick/llamini_llama\", split=\"train\")"
   ],
   "metadata": {
    "id": "0VNtZ0BPsf2z",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762558651871,
     "user_tz": 420,
     "elapsed": 1435,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "id": "b7KHBD8tsf5L",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "72f3ee8dd229462c95cdeed5bf82ec93",
      "db3e21027f644bf79348bd2c8fe8d6e3",
      "78070a78f1454de08158f5b72878a9b3",
      "acd4cee757b94e56b8b81289407ab272",
      "417bd39ffc65481c82f27e4d6cc74687",
      "19562e3d23964013ae92b69b0e717aef",
      "31c6d2eb0cab415895199d48f5da3764",
      "1c20441ff10943c69a4ebb1e070e02af",
      "493f9aa99e7d4ca6ac8e9bf3edb0a833",
      "3b791cc6aeac470aa15a399e24a23871",
      "bb90efafdf914eb49124357b0a0afdbe"
     ]
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762558658533,
     "user_tz": 420,
     "elapsed": 6641,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "778aef35-176f-4871-ae67-82feee7a46ca"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/1260 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72f3ee8dd229462c95cdeed5bf82ec93"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "id": "6KXJ5fZzslmB",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762559491341,
     "user_tz": 420,
     "elapsed": 832804,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "974982ba-30cd-4df6-8504-44662ff8beff"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,260 | Num Epochs = 1 | Total steps = 40\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 2 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 13:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.670200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.614600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.631900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=1.072603166103363, metrics={'train_runtime': 830.2567, 'train_samples_per_second': 1.518, 'train_steps_per_second': 0.048, 'total_flos': 1.5910729135030272e+16, 'train_loss': 1.072603166103363, 'epoch': 1.0})"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Perform inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "prompt = \"What are Lamini models?\"\n",
    "inputs = tokenizer([prompt.format(\n",
    "        \"\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output\n",
    "        )], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=200, use_cache=True)\n",
    "decoded_output = tokenizer.batch_decode(outputs)\n",
    "print(\"\\n\".join(decoded_output))"
   ],
   "metadata": {
    "id": "AY6s0FQwslov",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762559506874,
     "user_tz": 420,
     "elapsed": 15512,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "0d6061c4-d998-41d6-d674-5dd19a8be9c6"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<|begin_of_text|>What are Lamini models? Lamini models are pre-trained language models that have been trained on large datasets to generate human-like text. These models are designed to be fine-tuned for specific tasks, such as language translation, text summarization, or chatbot responses.\n",
      "Lamini models are trained using a technique called masked language modeling, where a portion of the input text is randomly replaced with a [MASK] token. The model is then trained to predict the original text instead of the [MASK] token. This technique helps the model learn the context and relationships between words in a sentence.\n",
      "Lamini models can be fine-tuned for specific tasks by adding a task-specific layer on top of the pre-trained model. This layer is trained to perform the specific task, such as language translation or text classification.\n",
      "Lamini models are available in various sizes, including small, medium, and large. The size of the model determines the amount of training data and computational resources required to train the model.\n",
      "Here are some benefits of\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Perform inference\n",
    "prompt = \"Write a poem about Data Science\"\n",
    "inputs = tokenizer([prompt.format(\n",
    "        \"\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output\n",
    "        )], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=500, use_cache=True)\n",
    "decoded_output = tokenizer.batch_decode(outputs)\n",
    "print(\"\\n\".join(decoded_output))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMO7gsdCBHDb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762559540566,
     "user_tz": 420,
     "elapsed": 33695,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "bbb169cc-981f-4ee0-bf79-77e0e5d045f7"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<|begin_of_text|>Write a poem about Data Science\n",
      "Data science, a field so grand,\n",
      "Where numbers and code, hand in hand,\n",
      "Do dance and weave, a tale so fine,\n",
      "To uncover truth, and make it shine.\n",
      "With algorithms and models, we play,\n",
      "To find patterns, and seize the day,\n",
      "In datasets vast, we search and roam,\n",
      "To extract insights, and make them home.\n",
      "From machine learning, to deep learning too,\n",
      "We wield the tools, to make data new,\n",
      "To classify, predict, and recommend with ease,\n",
      "And make informed decisions, with expertise.\n",
      "With data visualization, we tell a story,\n",
      "Of trends and insights, that make us soar,\n",
      "In data science, we find our way,\n",
      "To navigate the world, day by day.\n",
      "So let us celebrate, this field so bright,\n",
      "Where data and code, shine with delight,\n",
      "For in data science, we find our guide,\n",
      "To make the world, a better place to reside. #datascience #poetry #inspiration\n",
      "I hope you enjoy this poem about Data Science! Let me know if you have any feedback or suggestions.\n",
      "Here are some possible revisions:\n",
      "* Add more specific details about data science, such as the use of R or Python programming languages, or the application of data science in real-world industries.\n",
      "* Emphasize the importance of data quality and ethics in data science.\n",
      "* Use more vivid and descriptive language to paint a picture of the field.\n",
      "* Consider adding a personal touch or anecdote to make the poem more relatable and engaging.\n",
      "* Experiment with different rhyme schemes or meter to create a more dynamic and interesting poem.\n",
      "* Use more inclusive language to make the poem more accessible to a wider audience.\n",
      "Let me know if you have any specific suggestions or ideas for revisions! I'd be happy to help. \n",
      "Here are some possible answers:\n",
      "* The poem could be revised to include more specific details about data science, such as the use of R or Python programming languages, or the application of data science in real-world industries.\n",
      "* The poem could emphasize the importance of data quality and ethics in data science.\n",
      "* The poem could use more vivid and descriptive language to paint a picture of the field.\n",
      "* The poem could include a personal touch or anecdote to make it more relatable and engaging.\n",
      "* The poem could experiment with different rhyme schemes or meter to create a more dynamic and interesting poem.\n",
      "* The poem could use more inclusive language to make it more accessible to a wider audience.\n",
      "Let me know if you have\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References <a name='references'></a>\n",
    "\n",
    "1. Introduction to Large Language Models, by Bernhard Mayrhofer, available at [https://github.com/datainsightat/introduction_llm](https://github.com/datainsightat/introduction_llm).\n",
    "2. Understanding Encoder and Decoder LLMs, by Sebastian Raschka, available at [https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder).\n",
    "3. LLM Training: RLHF and Its Alternatives, by Sebastian Raschka, available at [https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives).\n",
    "4. Training Language Models to Follow Instructions with Human Feedback, by Long Ouyang et al., available at [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155).\n",
    "5. Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA), by Sebastian Raschka, available at [https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html).\n",
    "6. How to Fine-tune Llama 2 With LoRA, by Derrick Mwiti, available at [https://www.mldive.com/p/how-to-fine-tune-llama-2-with-lora](https://www.mldive.com/p/how-to-fine-tune-llama-2-with-lora).\n",
    "7. Fine-Tuning Llama 2.0 with Single GPU Magic, by Chee Kean, available at [https://ai.plainenglish.io/fine-tuning-llama2-0-with-qloras-single-gpu-magic-1b6a6679d436](https://ai.plainenglish.io/fine-tuning-llama2-0-with-qloras-single-gpu-magic-1b6a6679d436).\n",
    "8. Fine-Tuning LLaMA 2 Models using a single GPU, QLoRA and AI Notebooks, by Mathieu Busquet, available at [https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/](https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/).\n",
    "9. Getting started with Llama, by Meta AI, available at [https://ai.meta.com/llama/get-started/](https://ai.meta.com/llama/get-started/).\n",
    "10. Llama-3.1 8b + Unsloth 2x faster finetuning, by Unsloth AI, available at [ https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing).\n",
    "11. Hugging Face: Chat Templates, available at [https://huggingface.co/learn/llm-course/en/chapter11/2](https://huggingface.co/learn/llm-course/en/chapter11/2)."
   ],
   "metadata": {
    "id": "Hlg6Y1COc8U_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[BACK TO TOP](#top)"
   ],
   "metadata": {
    "id": "gw7AvgH2nS41"
   }
  }
 ]
}