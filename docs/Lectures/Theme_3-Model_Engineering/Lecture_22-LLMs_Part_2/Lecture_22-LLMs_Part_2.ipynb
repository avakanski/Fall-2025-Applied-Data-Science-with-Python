{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPm57rMcgQ5boaOr0NxRu+s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lecture 22 - Large Language Models (Part 2)"],"metadata":{"id":"xsrN8iJZlUgo"}},{"cell_type":"markdown","source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_22-LLMs_Part_2/Lecture_22-LLMs_Part_2.ipynb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_22-LLMs_Part_2/Lecture_22-LLMs_Part_2.ipynb)"],"metadata":{"id":"nT4plF3Klb8u"}},{"cell_type":"markdown","source":[],"metadata":{"id":"iaCiejDTiCUu"}},{"cell_type":"markdown","source":["<a id='top'></a>"],"metadata":{"id":"Zzs06-BflidN"}},{"cell_type":"markdown","source":["- [22.2 Vision-Language Models](#22.2-vision-language-models)\n","  - [22.2.1 VLM Workflow](#22.2.1-vlm-workflow)\n","  - [22.2.2 Representative VLM Models](#22.2.2-representative-vlm-models)\n","  - [22.2.3 VLM Finetuning](#22.2.3-vlm-finetuning)"],"metadata":{"id":"FNUdYxkulent"}},{"cell_type":"markdown","source":["## 22.2 Vision-Language Models <a name='22.2-vision-language-models'></a>"],"metadata":{"id":"SPXdAmMzW6sN"}},{"cell_type":"markdown","source":["**Vision Language Models (VLMs)** are multimodal systems that jointly process and reason over visual (images, videos) and linguistic (text) information. By integrating these two modalities, VLMs can understand and communicate about visual content using natural language.\n","\n","VLMs take both an image and its textual description as input and generate text as output. Building datasets for such models requires large scale collection of image and corresponding text, typically in the form of captions or descriptive phrases. Several very large such datasets exist that have been essential for training modern VLMs and contain millions or billions of image-text pairs, with descriptions in English and other languages. For instance, the [LAION-5B](https://laion.ai/blog/laion-5b/) dataset has 5.8 billion image-text examples, and the [PMD (Public Model Dataset)](https://huggingface.co/datasets/facebook/pmd) contains 70 million image-text examples.\n","\n","<img src=\"images/vlm_structure.png\" width=\"450\">\n","\n","*Figure: VLM structure.* Source: [1].\n","\n","During training, a VLM learns to map visual and textual representations into a joint embedding space. This mapping allows the model to associate visual features (shapes, colors, spatial relations) with linguistic concepts, enabling it to generalize to a wide range of vision tasks and perform zero-shot inference on unseen examples.\n","\n","The example below illustrates a VLM performing tasks such as object localization, segmentation, visual question answering, and image learning with instructions. The user prompts are shown on the left, and the model responses are given on the right. As demonstrated in this example, VLM models can not only interpret the semantic content of images, but can also understand spatial relations in images, such as identifying relative positions of objects, or generating segmentation masks. VLMs can also output bounding boxes of objects, and perform other spatial tasks.\n","\n","<img src=\"images/VLM_capabilities.jpg\" width=\"650\">\n","\n","*Figure: VLM prompts and responses.* Source: [2].\n","\n","In general, VLMs can perform various multimodal tasks including:\n","\n","- Image and video captioning/summarization: generate context-aware descriptions of images or video frames.\n","- Visual Question Answering (VQA): answer open-ended questions based  on visual content.\n","- Image-based reasoning: provide explanations or logical reasoning about visual scenes.\n","- Multimodal dialogues: engage in conversations involving visual inputs.\n","- Text-to-image search: retrieve images, figures, or diagrams in documents that match a textual query.\n","- Image generation: generate new images based on textual prompts."],"metadata":{"id":"5c4bRNh9lnUB"}},{"cell_type":"markdown","source":["### 22.2.1 VLM Workflow <a name='22.2.1-vlm-workflow'></a>"],"metadata":{"id":"Vxhi7aHnnF_x"}},{"cell_type":"markdown","source":["Numerous different architectures of VLMs have been proposed in recent years that employ diverse strategies to combine and fuse visual and text information. One common VLM workflow is shown in the above figure VLM Structure, and includes the following main components.\n","\n","**Multimodal inputs**. Input modalities in VLMs include *visual inputs* (images, pdf documents, or videos) and *textual inputs* (captions, question-answer pairs, or instructions).\n","\n","**Encoding Modalities.** A *vision encoder* transforms the visual input into numerical representations, referred to as visual embeddings. The vision encoder in VLMs is commonly a variant of the Vision Transformer (ViT) networks. A *text encoder* converts textual prompts into text embeddings. The text encoder is typically a pretrained Transformer-based encoder derived from an LLM.\n","\n","**Projection or fusion into a multimodal embeddings space.** The visual and textual embedding are next projected or fused into a shared multimodal embeddings space. This step is achieved through a *projector layer* (also referred to as a multimodal fusion layer), which is usually implemented as a small Transformer block or a block of fully-connected layers. The projector aligns the visual and textual embeddings into a shared embeddings space, enabling the VLM to reason simultaneously across images and language.\n","\n","**Autoregressive multimodal decoding.** The combined multimodal embeddings are passed to a *text decoder*, which generates a textual response one token at a time. Each new token is conditioned on previously generated tokens and on the multimodal embeddings."],"metadata":{"id":"hh5hE6sdqk78"}},{"cell_type":"markdown","source":["The shared embedding space enables VLMs to link textual concepts (e.g., cat) with corresponding visual evidence (a cat's color or location in the image), allowing reasoning across both language and vision. For instance, the model understands \"cat\" not just as a word, but also as a visual object in the image.\n","\n","Other VLM architectures employ a pretrained decoder network from LLMs instead of a dedicated text encoder. In the following figure, the LLM text decoder receives combined visual embeddings from the multimodal projector and text embeddings from the textual prompt or question. The LLM text decoder serves as the text generation backbone, and based on the fused vision and text embeddings, the text decoder generates a textual response in an autoregressive manner. Fusing together image and text representations into joint multimodal embeddings enables more advanced reasoning across modalities.\n","\n","<img src=\"images/multimodal_decoding.jpg\" width=\"550\">\n","\n","*Figure: Multimodal decoding.* Source: [2].\n","\n","Training VLMs with fused visual and textual embeddings typically involves multiple stages, as shown in the next figure. In the first stage only the multimodal projector layer is trained while keeping the vision encoder and LLM (text decoder) frozen, followed by another stage that involves additional finetuning of the multimodal projector layer and parts of the LLM decoder while the vision encoder is kept frozen. Furthermore, in some architectures, the vision encoder is also finetuned in the second stage.\n","\n","<img src=\"images/vlm-training.jpg\" width=\"650\">\n","\n","*Figure: VLM training.* Source: [2]."],"metadata":{"id":"UvXspadVqk91"}},{"cell_type":"markdown","source":["### 22.2.2 Representative VLM Models <a name='22.2.2-representative-vlm-models'></a>\n","\n","CLIP (Contrastive Language-Image Pretraining) is one of the earliest models that introduced vision-language alignment through contrastive learning. CLIP employs both a vision encoder and a text encoder, and learns a shared embedding space in which images and their textual descriptions are semantically aligned. The pretrained vision encoder of CLIP has been widely adopted as a vision encoder component in numerous subsequent VLMs. *Contrastive learning* in CLIP is employed to associate visual and textual content by maximizing the similarity between matched image and text pairs and minimizing the similarity between mismatched ones. In the figure below, the image encoder outputs image embeddings $I_1, I_2, I_3, ..., I_N$, and the text encoder outputs text embeddings $T_1, T_2, T_3, ..., T_N$. The model computes a similarity score between each pair of image embedding (e.g., $I_i$) and text embedding (e.g., $T_j$) to align them into a shared embeddings space $I_i\\cdot T_j$. Similarity scores are calculated using the dot product (i.e., cosine similarity) $I_i\\cdot T_j$ between image and text embeddings. As training progresses, image and text representations corresponding to similar concepts are drawn close to each other in the shared embedding space, while dissimilar concepts are pushed apart.\n","\n","<img src=\"images/CLIP.png\" width=\"550\">\n","\n","*Figure: CLIP architecture.* Source: [5].\n","\n","Subsequent VLMs, such as BLIP and Flamingo, introduced cross-attention mechanisms (similar to the cross-attention module connecting the encoder and decoder in Transformer networks). This enabled direct fusion of image and text embeddings into a joint multimodal representation. Fusing the image and text information within a single embedding space allowed such models to reason more efficiently over both modalities. Conversely, CLIP employs separate image and text encoders whose outputs are projected into a shared embedding space, where the image and text embeddings are semantically aligned but not fused.\n","\n","More recent models, such as Gemini 2.5 (Google), GPT-5 (OpenAI), Claude Opus 4 (Anthropic), Qwen-VL (Alibaba), and Mistral 3.1 (Mistral AI) demonstrate advanced visual comprehension and dialog capabilities. These VLMs typically employ fused image and text representations within a unified embeddings space to provide complex multimodal reasoning.\n","\n","Also, many open-source VLM alternatives have made this functionality widely accessible to the research community, and include LLaVA, Qwen-VL, LLaMA 3.2 Vision, InternVL, Pixtral, and others.\n"],"metadata":{"id":"FAgPro0hqk_0"}},{"cell_type":"markdown","source":["#### Benchmarking VLMs\n","\n","Performance of VLMs is assessed using multimodal benchmarks that evaluate models on a variety of tasks, such as reasoning, visual question answering, document comprehension, video understanding, and other tasks. Most benchmarks consist of a set of images with associated questions, often posed as multiple-choice questions. Popular benchmarks are [MMMU](https://mmmu-benchmark.github.io/), [Video-MME](https://video-mme.github.io/home_page.html), [MathVista](https://mathvista.github.io/), and [ChartQA](https://github.com/vis-nlp/ChartQA). MMMU is the most comprehensive benchmark, and contains 11.5K multimodal challenges that require knowledge and reasoning across different disciplines such as arts and engineering.\n","\n","Several VLM-specific leaderboards provide comparative rankings across diverse metrics. [Vision Arena](https://lmarena.ai/leaderboard/vision) ranks models based on anonymous voting of model outputs by human preferences. [Open VLM Leaderboard](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard) provides comparative ranking of VLMs according to different metrics and average scores."],"metadata":{"id":"6z3TLzuk6Q6D"}},{"cell_type":"markdown","source":["#### Importance\n","\n","Traditional computer vision (CV) models are constrained to learning from a predefined and fixed set of categories or objects for image classification or object detection (e.g., identify whether an image contains a cat or a dog). Moreover, these tasks require users to manually label a large number of images with a specific category for classification, or assign bounding boxes to multiple objects in each image for object detection, which is a tedious, time-consuming, and expensive process.\n","\n","Conversely, VLMs are trained with more  detailed textual descriptions of images, where for example an image can contain cats, dogs, and other objects, as well as the text description can provide additional contextual information (e.g., the cat is sitting, the dog is running, etc.). Learning from rich natural language descriptions allows VLMs to better understand visual scenes without limiting the learning to a narrow set of visual concepts comprising a fixed number of classes and objects. Also, it eliminates the need for exhaustive manual image labeling and extends VLMs' utility beyond traditional CV tasks like classification or detection to new capabilities including reasoning, summarization, question answering, and interactive dialogue, by simply changing the text prompt.\n","\n","VLMs have been applied across many domains and industries, and offer great potential to enhance visual perception. For instance, they can be used to review videos and extract insights for industrial inspection and robotics (detect faults, monitor operations, identify anomalies in real time), safety and infrastructure monitoring (recognize floods, fires, or traffic hazards), retail and logistics (track empty shelves, detect misplaced items, identify supply-chain bottlenecks), and numerous other tasks."],"metadata":{"id":"cXozhEaErru5"}},{"cell_type":"markdown","source":["### 2.2.3 VLM Finetuning <a name='22.2.3-vlm-finetuning'></a>"],"metadata":{"id":"OWKHrZNos2BH"}},{"cell_type":"markdown","source":["We are excited to announce that TRL’s SFTTrainer now includes experimental support for Vision Language Models! We provide an example here of how to perform SFT on a Llava 1.5 VLM using the llava-instruct dataset which contains 260k image-conversation pairs. The dataset contains user-assistant interactions formatted as a sequence of messages. For example, each conversation is paired with an image that the user asks questions about."],"metadata":{"id":"qMwR2hiLlnWN"}},{"cell_type":"code","source":[],"metadata":{"id":"Nuc4bJf_lawe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"iviiNT2_s2Cf"}},{"cell_type":"markdown","source":["## References <a name='references'></a>\n","\n","1. Vision Language Models (VLMs) Explained - GeeksForGeeks, available at [https://www.geeksforgeeks.org/artificial-intelligence/vision-language-models-vlms-explained/](https://www.geeksforgeeks.org/artificial-intelligence/vision-language-models-vlms-explained/).\n","2. Vision Language Models Explained - Hugging Face Blog, by Merve,\n","Edward Beeching, available at [https://huggingface.co/blog/vlms](https://huggingface.co/blog/vlms).\n","3. Understanding Vision-Language Models (VLMs): A Practical Guide, by Pietro Bolcato, available at [https://medium.com/@pietrobolcato/understanding-vision-language-models-vlms-a-practical-guide-8da18e9f0e0c](https://medium.com/@pietrobolcato/understanding-vision-language-models-vlms-a-practical-guide-8da18e9f0e0c).\n","4. What Are Vision Language Models, by NVIDIA, available at [https://www.nvidia.com/en-us/glossary/vision-language-models/](https://www.nvidia.com/en-us/glossary/vision-language-models/).\n","5. CLIP: Connecting text and images, OpenAI, available at [https://openai.com/index/clip/](https://openai.com/index/clip/)."],"metadata":{"id":"2amIei5Ns2En"}}]}