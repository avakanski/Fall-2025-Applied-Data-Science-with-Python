{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOpNPeU8vugblMCjM6xNd4h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Lecture 22 - Large Language Models (Part 2)"],"metadata":{"id":"xsrN8iJZlUgo"}},{"cell_type":"markdown","source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_22-LLMs_Part_2/Lecture_22-LLMs_Part_2.ipynb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_22-LLMs_Part_2/Lecture_22-LLMs_Part_2.ipynb)"],"metadata":{"id":"nT4plF3Klb8u"}},{"cell_type":"markdown","source":["<a id='top'></a>"],"metadata":{"id":"Zzs06-BflidN"}},{"cell_type":"markdown","source":["- [22.1 Mixture of Experts](#22.1-mixture-of-experts)\n","  - [22.1.1 Sparsity](#22.1.1-sparsity)\n","  - [22.1.2 MoE Advantages and Disadvantages](#22.1.2-moe-advantages-and-disadvantages)\n","- [22.2 Retrieval-Augmented Generation](#22.2-retrieval-augmenented-generation)\n","  - [22.2.1 RAG Implementation - Example 1](#22.2.1-rag-implementation-example-1)\n","  - [22.2.2 RAG Implementation - Example 2](#22.1.2-rag-implementation-example-2)\n","- [22.3 Vision-Language Models](#22.3-vision-language-models)\n","  - [22.3.1 VLM Architectures](#22.3.1-vlm-architectures)\n","  - [22.3.2 Benchmarking VLMs](#22.3.2-benchmarking-vlms)\n","  - [22.3.3 VLMs Importance](#22.3.3-vlms-importance)\n","  - [22.3.4 VLM Finetuning](#22.3.4-vlm-finetuning)\n","\n","\n"],"metadata":{"id":"FNUdYxkulent"}},{"cell_type":"markdown","source":["This lecture continues our study of Large Language Models (LLMs) by examining several advanced topics. We will discuss Mixture of Experts (MoE) architectures and Retrieval-Augmented Generation (RAG). In addition, we will cover Vision-Language Models (VLMs) as an example of multi-modal models that integrate visual and textual understanding."],"metadata":{"id":"TZGTIH7_QfGx"}},{"cell_type":"markdown","source":["## 22.1 Mixture of Experts <a name='22.1-mixture-of-experts'></a>"],"metadata":{"id":"97rX1niygObX"}},{"cell_type":"markdown","source":["**Mixture of Experts (MoE)** in LLMs refers to a technique that uses a collection of sub-networks (called \"experts\") that jointly perform a task, with each expert specializing in learning different aspects of the task.\n","\n","MoE in LLMs has similarities with Ensemble Methods in ML (e.g., Random Forests, Gradient Boosting, Bagging Ensembles), where an ensemble of models contributes to a prediction. Differently from Ensemble Methods, MoE introduces dynamic routing of the input to different experts, as well as, all experts in MoE are trained as part of a single neural architecture rather than as separate learners.\n","\n","**MoE Components**\n","\n","A typical MoE architecture is shown in the figure below and consists of two key components:\n","\n","- **Experts**: each expert is typically a fully-connected neural network trained on different parts or properties of the input. Each expert specializes in processing different types of inputs, such as token types, contexts, or semantic patterns.\n","\n","- **Router or gating network**: analyzes the inputs and determines which tokens are sent to which experts. It acts as a coordinator that routes the inputs to the selected experts.\n","\n","When MoE receives input data, it first passes the data through the gating network. The gating network analyzes the input and selects the experts to receive the data. Typically, only a small subset of experts is activated for each input. The selected experts then process the data and generate outputs, which are combined to produce the final output of the MoE.\n","\n","<img src=\"images/moe_components.png\" width=\"450\">\n","\n","*Figure: Main components of MoE.* Source: [1].\n","\n","\n"],"metadata":{"id":"_YRMf0vD5LkM"}},{"cell_type":"markdown","source":["In most MoE LLMs, the dense Feed-Forward Network (FFN) of Transformers is replaced with a set of experts. The **experts** are also feed-forward networks, but they are smaller compared to the FFN in Transformers. In general, experts can also be more complex than FFNs, and an expert can even be an MoE itself (known as hierarchical MoE).\n","\n","<img src=\"images/transformer_vs_moe.gif\" width=\"550\">\n","\n","*Figure: Transformer vs MoE.* Source: [2].\n","\n","\n","Because LLMs contain many decoder blocks in sequence, input tokens may pass through different experts across the decoder blocks, which results in diverse specialization patterns.\n","\n","<img src=\"images/decoder_layers.gif\" width=\"400\">\n","\n","*Figure: Experts in decoder layers.* Source: [2].\n","\n"],"metadata":{"id":"GDlnKXXA5LmI"}},{"cell_type":"markdown","source":["The **gating network (router)** is also a small FFN that acts like a multi-class classifier and outputs softmax scores over the experts. I.e., the router assigns a probability value to each expert, and determines which experts are activated for a given token.\n","\n","For instance, in the figure, the router assigned the highest probability value to Expert-1, and selected it for routing the input data to that expert.\n","\n","<img src=\"images/router.gif\" width=\"450\">\n","\n","*Figure: Gating network (router).* Source: [1].\n","\n","\n","\n","Training an MoE model involves updating both the experts and the gating network, which makes it more challenging than training traditional Transformer models. Specifically, the training can result in a poor routing strategy where the gating network activates only a small number of experts. Such imbalance means that the selected few experts can become overly specialized, while other experts can remain undertrained and underutilized. This imbalance reduces the performance of the entire model.\n","\n","Consequently, *load balancing* by the router is critical for MoE performance. Several different strategies have been proposed that penalize overreliance on any one expert, and reward more even utilization of all experts.  \n","\n","In MoE models, each expert learns a different specialization of the overall function. However, the experts do not learn completely separate tasks. Instead, they learn to handle different types of tokens, contexts, or transformations. For example, one expert may learn code-style token patterns, another may learn transformations helpful for math reasoning, then another may specialize in rare domain-specific words, and similar.\n","\n"],"metadata":{"id":"jIx00_2v5Lrs"}},{"cell_type":"markdown","source":["### 22.1.1 Sparsity<a name='22.1.1-sparsity'></a>\n"],"metadata":{"id":"sCD1F1XnuUyf"}},{"cell_type":"markdown","source":["The main concept in MoE architectures is the presence of **sparse layers**, since only some of the experts (and therefore their neurons) are activated for each token. In the original **dense layers** in FFN of Transformers, all neurons are connected to every neuron in the preceding and following layers. In sparse layers, neurons are connected only to some of the neurons in the neighboring layers.\n","\n","<img src=\"images/dense_sparse.png\" width=\"450\">\n","\n","*Figure: Dense versus sparse networks.* Source: [3].\n","\n","For example, Mixtral is a well-known MoE LLM that uses 8 experts, each containing approximately 7B parameters. Therefore, the FFN in Mixtral has 8 expert blocks. For every token, the gating network selects two of the eight experts to process the data. Hence, about 14B parameters (two experts) are active during training and inference. Note however, that the overall number of parameters in Mixtral is not exactly 8 x 7B = 56B, since some layers are shared. The total parameter count is around 47B.\n","\n","The total number of parameters is generally considered a measure of the **model capacity**. The number of parameters that are used to process an individual token is a measure of the model's **computational cost**. Thus, Mixtral has a similar learning capacity to dense LLMs with 47B parameters, but has a much lower computational cost because only 14B parameters are used per token.\n","\n","Note also that although 14B parameters are active per token, the entire Mixtral model with 47B parameters needs to be loaded into memory to perform inference. In other words, computational efficiency does not reduce the requirement for sufficient RAM/VRAM to load the model.\n","\n","Other well-known MoE LLMs include Mixtral 8 x 22B, DeepSeek-V3 (671B parameters, with 37B active per token), Qwen3-235B (128 experts in total, 8 experts with 22B parameters active per token), Qwen1.5 (64 experts in total, 4 experts with 2.7B parameters active per token), etc. It is likely that the premier LLMs including GPT-5 and Claude 4 use some form of MoE and combine sparse and dense layers, although it is not known for sure.\n","\n"],"metadata":{"id":"pgir5QDZ5LuI"}},{"cell_type":"markdown","source":["### 22.1.2 MoE Advantages and Disadvantages<a name='22.1.2-moe-advantages-and-disadvantages'></a>"],"metadata":{"id":"qJmZXkiMtzhm"}},{"cell_type":"markdown","source":["MoE models offer several important advantages in LLMs, as follows.\n","\n","- Sparsity: Instead of activating the entire network for each input token, MoE activates only a small subset of parameters, and much of the model stays inactive for a given token.\n","- Efficient inference: Because only one or a few experts are used per token, fewer computations are required. This lowers inference-time compute and required memory.\n","- Faster pretraining: MoE LLMs are significantly faster to pretrain compared to dense models with the same total number of parameters, because each token uses only a fraction of the parameters.\n","- Higher quality outputs: Expert specialization improves accuracy and overall model performance.\n","\n","MoE disadvantages include:\n","\n","- Training instability: Sparse routing can lead to imbalanced expert usage, where a few experts get most of the traffic while others remain underutilized.\n","- Fine-tuning difficulty: Fine-tuning MoE models is more challenging due to the presence of sparse layers.\n","- Hardware complexity: MoE requires careful management of memory and compute, especially when experts are distributed across devices.\n","- Overfitting: Sparse models are more prone to overfitting than dense models if load balancing is insufficient.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"a6XQTlk0giXy"}},{"cell_type":"markdown","source":["## 22.2 Retrieval Augmenented Generation <a name='22.2-retrieval-augmenented-generation'></a>"],"metadata":{"id":"1EJbYNdSgOdu"}},{"cell_type":"markdown","source":["**Retrieval Augmented Generation (RAG)** refers to using external sources of information for improving the generated responses by LLMs. RAG enables LLMs to retrieve facts from external sources (e.g., Wikipedia, news articles, local documents) and provide responses that are more accurate and/or are up-to-date.\n","\n","In general, the internal knowledge of LLMs is static, as it is fixed by the date of the used training dataset. Therefore, LLMs cannot answer questions about current events, and they are stuck in the time moment of their training data. Updating LLMs with knowledge about current events requires to continuously retrain the models on new data. Such a process is very expensive, as it requires collecting updated datasets and finetuning the model to update the weights.\n","\n","RAG enables to avoid expensive LLMs retraining, by retrieving information from external databases to generate responses. The RAG approach involves two major phases:\n","\n","- **Retrieval**, includes performing relevancy search of external databases regarding a user query, and retrieving supporting documents and snippets of important information.\n","- **Content generation**, the retrieved supporting documents are used as a context that is appended to the user query, and are fed to the LLMs for generating the final response.\n","\n","Instead of relying only on the information contained in the training dataset used for training an LLM, RAG provides an interface to external knowledge to ensure that the model has access to the most current and reliable facts. E.g., in enterprise setting, external sources of information for RAG can comprise various company-specific files, documents, and databases. Employing RAG can result in more relevant responses and it can reduce the problem of hallucination by LLMs. It also allows the users to review the sources that were used by LLMs and verify the accuracy of generated responses."],"metadata":{"id":"pH8GPWtabYsO"}},{"cell_type":"markdown","source":["### 22.2.1 RAG Implementation - Example 1<a name='22.2.1-rag-implementation-example-1'></a>"],"metadata":{"id":"--uRUt8Bca3B"}},{"cell_type":"markdown","source":["In this example, we will implement a RAG system in Python using the Hugging Face library for model loading and response generation.\n","\n","As an external text source, we will use a Wikipedia article on LLMs available at https://en.wikipedia.org/wiki/Large_language_model\n",". The article has been pre-processed by extracting the main text content, removing sections with equations, and removing references. The cleaned text is saved in a plain text file named `wikipedia_llm.txt`.\n","\n","We will build a RAG system that allows us to answer queries about the article."],"metadata":{"id":"a1CZz_Toy3wR"}},{"cell_type":"markdown","source":["#### Data Loading\n","\n","First, we mount Google Drive and import the required libraries. Most are standard, except for `sentence_transformers` that is a Hugging Face library that is used to load an embedding model, and facilitate creating vector embeddings for sentences, paragraphs, or documents.\n","\n","Next, we load the document `wikipedia_llm.txt`, which is uploaded in the course folder for Lecture 22 on my Google Drive."],"metadata":{"id":"avG3dlS3XsUy"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omwbLwNOYRla","executionInfo":{"status":"ok","timestamp":1763319048112,"user_tz":420,"elapsed":582,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6e2176cc-306a-43d0-84bc-ea24abddc3ac"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Import libraries\n","import torch\n","import numpy as np\n","import pandas as pd\n","import re\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from sentence_transformers import SentenceTransformer"],"metadata":{"id":"uS-83NzuXYLi","executionInfo":{"status":"ok","timestamp":1763319062055,"user_tz":420,"elapsed":13247,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Path to the file uploaded on Google Drive\n","file_path = 'drive/MyDrive/Data_Science_Course/Fall_2025/Lectures/Lecture_22-LLMs_Part_2/data/wikipedia_llm.txt'\n","\n","# Load the text file\n","with open(file_path , 'r') as f:\n","    text = f.read()"],"metadata":{"id":"DiNCH4rdXYSA","executionInfo":{"status":"ok","timestamp":1763319062085,"user_tz":420,"elapsed":30,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Print the text content of the article\n","text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"H-T_SXM2YHI1","executionInfo":{"status":"ok","timestamp":1763319062190,"user_tz":420,"elapsed":68,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"98fcdbde-06e0-4646-c724-abd7a7b21fbf"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'=== Introduction ===\\nA large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. The largest and most capable LLMs are generative pre-trained transformers (GPTs) and provide the core capabilities of chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\\nThey consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.\\nLLMs evolved from earlier statistical and recurrent neural network approaches to language modeling. The transformer architecture, introduced in 2017, replaced recurrence with self-attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. This innovation enabled models like GPT, BERT, and their successors, which demonstrated emergent behaviors at scale such as few-shot learning and compositional reasoning.\\nReinforcement learning, particularly policy gradient algorithms, has been adapted to fine-tune LLMs for desired behaviors beyond raw next-token prediction. Reinforcement learning from human feedback (RLHF) applies these methods to optimize a policy, the LLM\\'s output distribution, against reward signals derived from human or automated preference judgments. This has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance.\\nBenchmark evaluations for LLMs have evolved from narrow linguistic assessments toward comprehensive, multi-task evaluations measuring reasoning, factual accuracy, alignment, and safety. Hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements.\\n\\n\\n=== History ===\\nBefore the emergence of transformer-based models in 2017, some language models were considered large relative to the computational and data constraints of their time. In the early 1990s, IBM\\'s statistical models pioneered word alignment techniques for machine translation, laying the groundwork for corpus-based language modeling. In 2001, a smoothed n-gram model, such as those employing Kneser–Ney smoothing, trained on 300 million words, achieved state-of-the-art perplexity on benchmark tests. During the 2000s, with the rise of widespread internet access, researchers began compiling massive text datasets from the web (\"web as corpus\") to train statistical language models.\\nMoving beyond n-gram models, researchers started in 2000 to use neural networks to learn language models. Following the breakthrough of deep neural networks in image classification around 2012, similar architectures were adapted for language tasks. This shift was marked by the development of word embeddings (eg, Word2Vec by Mikolov in 2013) and sequence-to-sequence (seq2seq) models using LSTM. In 2016, Google transitioned its translation service to neural machine translation (NMT), replacing statistical phrase-based models with deep recurrent neural networks. These early NMT systems used LSTM-based encoder-decoder architectures, as they preceded the invention of transformers. \\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper\\'s goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2025 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing chatbot ChatGPT that received extensive media coverage and public attention. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer. Many LLMs with parameter counts comparable to those of OpenAI\\'s GPT series have been developed.\\nSince 2022, open-weight models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on usage and deployment. Mistral AI\\'s models Mistral 7B and Mixtral 8x7b have a more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower price per token for users.\\nSince 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images, audio, or 3D mesh. These LLMs are also called large multimodal models (LMMs), or multimodal large language models (MLLMs).\\nAs of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).\\nOpen-weight LLMs have increasingly shaped the field since 2023, contributing to broader participation in AI development and greater transparency in model evaluation. Vake et al. (2025) demonstrated that community-driven contributions to open-weight models measurably improve their efficiency and performance, with user participation growing rapidly on collaborative platforms such as Hugging Face. Paris et al. (2025) further argued that openness in AI should extend beyond releasing model code or weights to encompass inclusiveness, accountability, and ethical responsibility in AI research and deployment. Collectively, these studies highlight that open-weight LLMs can accelerate innovation and enhance scientific reproducibility, while fostering a more transparent and participatory AI ecosystem.\\n\\n\\n=== Tokenization ===\\nAs machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding (BPE) and WordPiece. There are also special tokens serving as control characters, such as [MASK] for masked-out token (as used in BERT), and [UNK] (\"unknown\") for characters not appearing in the vocabulary. Also, some special symbols are used to denote special text formatting. For example, \"Ġ\" denotes a preceding whitespace in RoBERTa and GPT and \"##\" denotes continuation of a preceding word in BERT.\\nTokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. The average number of words per token depends on the language. In English, the ratio is typically around 0.75 words per token, with 4 characters per token on average.\\n\\n\\n=== Byte-pair encoding ===\\nAs an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained. After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.\\n\\n\\n=== Tokenization: Problems ===\\nA token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. However, an average word in another language encoded by such an English-optimized tokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \"a premium of 50%\" compared to English.\\n\\n\\n=== Dataset cleaning ===\\nIn the context of training LLMs, datasets are typically cleaned by removing low-quality, duplicated, or toxic data. Cleaned datasets can increase training efficiency and lead to improved downstream performance. A trained LLM can be used to clean datasets for training a further LLM.\\nWith the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).\\n\\n\\n=== Synthetic data ===\\nTraining of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft\\'s Phi series of LLMs is trained on textbook-like data generated by another LLM.\\n\\n=== Training ===\\nAn LLM is a type of foundation model (large X model) trained on language. LLMs can be trained in different ways. In particular, GPT models are first pretrained to predict the next word on a large amount of data, before being fine-tuned.\\n\\n\\n=== Cost ===\\nSubstantial infrastructure is necessary for training the largest models. The tendency towards larger models is visible in the list of large language models. For example, the training of GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million. The qualifier \"large\" in \"large language model\" is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as \"large\". GPT-1 of 2018 has 117 million parameters.\\n\\n\\n=== Fine-tuning ===\\nBefore being fine-tuned, most LLMs are next-token predictors. The fine-tuning shapes the LLM\\'s behavior via techniques like reinforcement learning from human feedback (RLHF) or constitutional AI.\\nInstruction fine-tuning is a form of supervised learning used to teach LLMs to follow user instructions. In 2022, OpenAI demonstrated InstructGPT, a version of GPT-3 similarly fine-tuned to follow instructions. \\nReinforcement learning from human feedback (RLHF) involves training a reward model to predict which text humans prefer. Then, the LLM can be fine-tuned through reinforcement learning to better satisfy this reward model. Since humans typically prefer truthful, helpful and harmless answers, RLHF favors such answers.\\n\\n\\n=== Architecture ===\\nLLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.\\n\\n\\n=== Attention mechanism and context window ===\\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model has had twelve attention heads and a context window of only 1k tokens. In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.\\nGoogle\\'s Gemini 1.5, introduced in February 2024, can have a context window of up to 1 million tokens.\\nA model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset. It can be either\\nautoregressive (i.e. predicting how the segment continues, as GPTs do): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\\n\"masked\" (i.e. filling in the parts missing from the segment, the way \"BERT\" does it): for example, given a segment \"I like to [__] [__] cream\", the model predicts that \"eat\" and \"ice\" are missing.\\nModels may be trained on auxiliary tasks which test their understanding of the data distribution, such as next sentence prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.\\n\\n\\n=== Mixture of experts ===\\nA mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\\n\\n\\n=== Parameter size ===\\nTypically, LLMs are trained with single- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have more than 100 billion parameters, which places them outside the range of most consumer electronics.\\n\\n\\n=== Quantization ===\\nPost-training quantization aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance. Quantization can be further classified as static quantization if the quantization parameters are determined beforehand (typically during a calibration phase), and dynamic quantization if the quantization is applied during inference. The simplest form of quantization simply truncates all the parameters to a given number of bits: this is applicable to static as well as dynamic quantization, but loses much precision. Dynamic quantization allows for the use of a different quantization codebook per layer, either a lookup table of values or a linear mapping (scaling factor and bias), at the cost of foregoing the possible speed improvements from using lower-precision arithmetic.\\nQuantized models are typically seen as frozen with modification of weights (e.g. fine-tuning) only applied to the original model. It is possible to fine-tune quantized models using low-rank adaptation.\\n\\n\\n=== Extensibility ===\\nBeyond basic text generation, various techniques have been developed to extend LLM capabilities, including the use of external tools and data sources, improved reasoning on complex problems, and enhanced instruction-following or autonomy through prompting methods.\\n\\n\\n=== Prompt engineering ===\\nIn 2020, OpenAI researchers demonstrated that their new model GPT-3 could understand what format to use given a few rounds of Q and A (or other type of task) in the input data as example, thanks in part due to the RLHF technique. This technique, called few-shot prompting, allows LLMs to be adapted to any task without requiring fine-tuning. Also in 2022, it was found that the base GPT-3 model can generate an instruction based on user input. The generated instruction along with user input is then used as input to another instance of the model under a \"Instruction: [...], Input: [...], Output:\" format. The other instance is able to complete the output and often produces the correct answer in doing so. The ability to \"self-instruct\" makes LLMs able to bootstrap themselves toward a correct answer.\\n\\n\\n=== Dialogue processing (chatbot) ===\\nAn LLM can be turned into a chatbot or a \"dialog assistant\" by specializing it for conversation. In essence, user input is prefixed with a marker such as \"Q:\" or \"User:\" and the LLM is asked to predict the output after a fixed \"A:\" or \"Assistant:\". This type of model became commercially available in 2022 with ChatGPT, a sibling model of InstructGPT fine-tuned to accept and produce dialog-formatted text based on GPT-3.5. It could similarly follow user instructions. Before the stream of User and Assistant lines, a chat context usually start with a few lines of overarching instructions, from a role called \"developer\" or \"system\" to convey a higher authority than the user\\'s input. This is called a \"system prompt\".\\n\\n\\n=== Retrieval-augmented generation ===\\nRetrieval-augmented generation (RAG) is an approach that enhances LLMs by integrating them with document retrieval systems. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.\\n\\n\\n=== Tool use ===\\nTool use is a mechanism that enables LLMs to interact with external systems, applications, or data sources. It can allow for example to fetch real-time information from an API or to execute code. A program separate from the LLM watches the output stream of the LLM for a special tool-calling syntax. When these special tokens appear, the program calls the tool accordingly and feeds its output back into the LLM\\'s input stream.\\nEarly tool-using LLMs were fine-tuned on the use of specific tools. But fine-tuning LLMs for the ability to read API documentation and call API correctly has greatly expanded the range of tools accessible to an LLM. Describing available tools in the system prompt can also make an LLM able to use tools. A system prompt instructing ChatGPT (GPT-4) to use multiple types of tools can be found online.\\n\\n\\n=== Agency ===\\nAn LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions. But it can be transformed into an agent by adding supporting elements: the role (profile) and the surrounding environment of an agent can be additional inputs to the LLM, while memory can be integrated as a tool or provided as additional input. Instructions and input patterns are used to make the LLM plan actions and tool use is used to potentially carry out these actions.\\nThe ReAct pattern, a portmanteau of reason and act, constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment.\\nIn the DEPS (\"describe, explain, plan and select\") method, an LLM is first connected to the visual world via image descriptions. It is then prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and the environmental feedback it receives.\\nThe Reflexion method constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent episode. These \"lessons learned\" are stored as a form of long-term memory and given to the agent in the subsequent episodes.\\nMonte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.\\nFor open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively, it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.\\nMultiple agents with memory can interact socially.\\n\\n\\n=== Reasoning ===\\nLLMs are conventionally trained to generate an output without generating intermediate steps. As a result, their performance tends to be subpar on complex questions requiring (at least in humans) intermediate steps of thought. Early research demonstrated that inserting intermediate \"scratchpad\" computations could improve performance on such tasks. Later methods overcame this deficiency more systematically by breaking tasks into smaller steps for the LLM, either manually or automatically.\\n\\n\\n=== Chaining ===\\nPrompt chaining was introduced in 2022. In this method, a user manually breaks a complex problem down into several steps. In each step, the LLM receives as input a prompt telling it what to do and some results from preceding steps. The result from one step is then reused in a next step, until a final answer is reached. The ability of an LLM to follow instructions means that even non-experts can write a successful collection of stepwise prompts given a few rounds of trial and error.\\nA 2022 paper demonstrated a separate technique called chain-of-thought prompting, which makes the LLM break the question down autonomously. An LLM is given some examples where the \"assistant\" verbally breaks down the thought process before arriving at an answer. The LLM mimics these examples and also tries to spend some time generating intermediate steps before providing the final answer. This additional step elicited by prompting improves the correctness of the LLM on relatively complex questions. On math word questions, a prompted model can exceed even fine-tuned GPT-3 with a verifier. Chain-of-thought can also be elicited by simply adding an instruction like \"Let\\'s think step by step\" to the prompt, in order to encourage the LLM to proceed methodically instead of trying to directly guess the answer.\\n\\n\\n=== Model-native reasoning ===\\nIn late 2024 \"reasoning models\" were released. These were trained to spend more time generating step-by-step solutions before providing final answers, which was intended to be similar to human problem-solving processes. OpenAI introduced this concept with their o1 model in September 2024, followed by o3 in April 2025. On the International Mathematics Olympiad qualifying exam problems, GPT-4o achieved 13% accuracy while o1 reached 83%.\\nIn January 2025, the Chinese company DeepSeek released DeepSeek-R1, a 671-billion-parameter open-weight reasoning model that achieved comparable performance to OpenAI\\'s o1 while being significantly more cost-effective to operate. Unlike proprietary models from OpenAI, DeepSeek-R1\\'s open-weight nature allowed researchers to study and build upon the algorithm, though its training data remained private.\\nThese reasoning models typically require more computational resources per query compared to traditional LLMs, as they perform more extensive processing to work through problems step-by-step.\\n\\n\\n=== Inference optimization ===\\nInference optimization refers to techniques that improve LLM performance by applying additional computational resources during the inference process, rather than requiring model retraining. These approaches implement various state-of-the-art reasoning and decision-making strategies to enhance accuracy and capabilities.\\nOptiLLM is an OpenAI API-compatible optimizing inference proxy that implements multiple inference optimization techniques simultaneously. The system acts as a transparent proxy that can work with any LLM provider, implementing techniques such as Monte Carlo tree search (MCTS), mixture of agents (MOA), best-of-N sampling, and chain-of-thought reflection. OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.\\nThese inference optimization approaches represent a growing category of tools that enhance existing LLMs without requiring access to model weights or retraining, making advanced reasoning capabilities more accessible across different model providers and use cases.\\n\\n\\n=== Multimodality ===\\nMultimodality means having multiple modalities, where a \"modality\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc. For example, Google PaLM model was fine-tuned into a multimodal model and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs, and video inputs. GPT-4o can process and generate text, audio and images. Such models are sometimes called large multimodal models (LMMs). A common method to create multimodal models out of an LLM is to \"tokenize\" the output of a trained encoder. \\n\\n\\n=== Non-natural languages ===\\nLLMs can handle programming languages similarly to how they handle natural languages. No special change in token handling is needed as code, like human language, is represented as plain text. LLMs can generate code based on problems or instructions written in natural language. They can also describe code in natural language or translate it into other programming languages. They were originally used as a code completion tool, but advances have moved them towards automatic programming. Services such as GitHub Copilot offer LLMs specifically trained, fine-tuned, or prompted for programming.\\nIn computational biology, transformer-base architectures have also proven useful in analyzing biological sequences: protein, DNA, and RNA. With proteins they appear able to capture a degree of \"grammar\" from the amino-acid sequence, by mapping that sequence into an embedding. On tasks such as structure prediction and mutational outcome prediction, a small model using an embedding as input can approach or exceed much larger models using multiple sequence alignments (MSA) as input. ESMFold, Meta Platforms\\' embedding-based method for protein structure prediction, runs an order of magnitude faster than AlphaFold2 thanks to the removal of an MSA requirement and a lower parameter count due to the use of embeddings. Meta hosts ESM Atlas, a database of 772 million structures of metagenomic proteins predicted using ESMFold. An LLM can also design proteins unlike any seen in nature. Nucleic acid models have proven useful in detecting regulatory sequences, sequence classification, RNA-RNA interaction prediction, and RNA structure prediction.\\n\\n\\n=== Properties: Scaling laws ===\\nScaling laws are empirical statistical laws that predict LLM performance based on such factors. One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:\\n\\n\\n=== Emergent abilities ===\\nPerformance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by \"break(s)\" in the scaling law, where the slope of the line changes abruptly, and where larger models acquire \"emergent abilities\". They arise from the complex interaction of the model\\'s components and are not explicitly programmed or designed. \\nOne of the emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks, such as:\\nreported arithmetics\\ndecoding the International Phonetic Alphabet\\nunscrambling a word\\'s letters\\ndisambiguating word-in-context datasets\\nconverting spatial words\\ncardinal directions (for example, replying \"northeast\" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.\\nchain-of-thought prompting: In a 2022 research paper, chain-of-thought prompting only improved the performance for models that had at least 62B parameters. Smaller models perform better when prompted to answer immediately, without chain of thought.\\nidentifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.\\nSchaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.\\n\\n\\n=== Mechanistic interpretability ===\\nMechanistic interpretability seeks to precisely identify and understand how individual neurons or circuits within LLMs produce specific behaviors or outputs. By reverse-engineering model components at a granular level, researchers aim to detect and mitigate safety concerns such as emergent harmful behaviors, biases, deception, or unintended goal pursuit before deployment. Mechanistic interpretability research has been conducted at organizations like Anthropic and OpenAI, although understanding the inner workings of LLMs remains difficult.\\nMechanistic interpretability has progressively replaced the characterization of large language models as inscrutable \"black boxes\" by identifying neurons and circuits that implement specific computations and by producing causal traces of how representations propagate through transformer layers. Researchers have demonstrated automated neuron-explanation pipelines and released neuron-level datasets, and they have developed circuit-tracing and replacement-model methods that produce attribution graphs and component-level descriptions applicable to modern transformer models. \\nSubstantive limits remain, including polysemanticity, superposition, non-identifiability of competing explanations, and the risk of anthropomorphic inference, so current mechanistic results increase controllability and surface actionable interventions. These results do not by themselves justify treating LLMs as models of the human brain or human mind without additional empirical validation and cross-disciplinary evidence. Thinking Machines Lab published reproducible interpretability work addressing these gaps through techniques for defeating nondeterminism in LLM inference.\\nThe reverse-engineering may lead to the discovery of algorithms that approximate inferences performed by an LLM. For instance, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform. The training of the model also highlighted a phenomenon called grokking, in which the model initially memorizes all the possible results in the training set (overfitting), and later suddenly learns to actually perform the calculation.\\nSome techniques have been developed to enhance the transparency and interpretability of LLMs. Transcoders, which are more interpretable than transformers, have been utilized to develop \"replacement models\". In one such study involving the mechanistic interpretation of writing a rhyming poem by an LLM, it was shown that although they are believed to simply predict the next token, they can, in fact, plan ahead. By integrating such techniques, researchers and practitioners can gain deeper insights into the operations of LLMs, fostering trust and facilitating the responsible deployment of these powerful models.\\n\\n\\n=== Understanding and intelligence ===\\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\". Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \"understand\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\": \"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\" Ilya Sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. Some researchers characterize LLMs as \"alien intelligence\". For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If you don\\'t push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\"\\nIn contrast, some skeptics of LLM understanding believe that existing LLMs are \"simply remixing and recombining existing writing\", a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability. For example, GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\". Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that \"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".\\nEfforts to reduce or compensate for hallucinations have employed automated reasoning, retrieval-augmented generation (RAG), fine-tuning, and other methods.\\nThe matter of LLM\\'s exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human-like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented neural theory of language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human-like language.\\n\\n\\n=== Perplexity ===\\nThe canonical measure of the performance of any language model is its perplexity on a given text corpus. Perplexity measures how well a model predicts the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. In mathematical terms, perplexity is the exponential of the average negative log likelihood per token.\\nBecause language models may overfit to training data, models are usually evaluated by their perplexity on a test set. This evaluation is potentially problematic for larger models which, as they are trained on increasingly large corpora of text, are increasingly likely to inadvertently include portions of any given test set.\\n\\n\\n=== Measures ===\\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon. This relationship is mathematically expressed as \\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different LLMs, BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model\\'s enhanced capability for compression. This, in turn, reflects the model\\'s proficiency in making accurate predictions.\\nDue to their ability to accurately predict the next token, LLMs are highly capable in lossless compression. A 2023 study by DeepMind showed that the model Chinchilla, despite being trained primarily on text, was able to compress ImageNet to 43% of its size, beating PNG with 58%.\\n\\n\\n=== Benchmarks ===\\nBenchmarks are used to evaluate LLM performance on specific tasks. Tests evaluate capabilities such as general knowledge, bias, commonsense reasoning, question answering, and mathematical problem-solving. Composite benchmarks examine multiple capabilities. Results are often sensitive to the prompting method.\\nA question-answering benchmark is termed \"open book\" if the model\\'s prompt includes text from which the expected answer can be derived (for example, the previous question could be combined with text that includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"). Otherwise, the task is considered \"closed book\", and the model must draw solely on its training. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, HELM, and HLE (Humanity\\'s Last Exam).\\nLLM bias may be assessed through benchmarks such as CrowS-Pairs (Crowdsourced Stereotype Pairs), Stereo Set, and Parity Benchmark.\\nFact-checking and misinformation detection benchmarks are available. A 2023 study compared the fact-checking accuracy of LLMs including ChatGPT 3.5 and 4.0, Bard, and Bing AI against independent fact-checkers such as PolitiFact and Snopes. The results demonstrated moderate proficiency, with GPT-4 achieving the highest accuracy at 71%, lagging behind human fact-checkers.\\nAn earlier standard tested using a portion of the evaluation dataset. It became more common to evaluate a pre-trained model directly through prompting techniques. Researchers vary in how they formulate prompts for particular tasks, particularly with respect to the number of correct examples attached to the prompt (i.e. the value of n in n-shot prompting).\\n\\n\\n=== Datasets ===\\nTypical datasets consist of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\"). Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.\\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".\\nDatasets are of varying quality and may contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality.\\n\\n\\n=== Adversarial evaluations ===\\nLLMs\\' rapid improvement regularly renders benchmarks obsolete, with the models exceeding the performance of human annotators. In addition, \"shortcut learning\" allows AIs to \"cheat\" on multiple-choice tests by using statistical correlations in superficial test question wording to guess the correct responses, without considering the specific question.\\nSome datasets are adversarial, focusing on problems that confound LLMs. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions that stump LLMs by mimicking falsehoods to which they were exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom you can\\'t teach an old dog new tricks, even though this is not literally true.\\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model. The resulting problems are trivial for humans but defeated LLMs. \\n\\n\\n=== Limitations and challenges ===\\nDespite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications. \\n\\n\\n=== Limitations and challenges: Hallucinations ===\\nHallucinations represent a fundamental challenge, wherein models generate syntactically fluent text that appears factually sound, but is internally inconsistent with training data or factually incorrect. These hallucinations arise partly through memorization of training data combined with extrapolation beyond factual boundaries, with evaluations demonstrating that models can output verbatim passages from training data, when subjected to specific prompting sequences.\\n\\n\\n=== Limitations and challenges: Algorithmic bias ===\\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups.\\nGender bias manifests through stereotypical occupational associations, wherein models disproportionately assign nursing roles to women and engineering roles to men, reflecting systematic imbalances in training data demographics. Language-based bias emerges from overrepresentation of English text in training corpora, which systematically downplays non-English perspectives and imposes English-centric worldviews through default response patterns.\\nDue to the dominance of English-language content in LLM training data, models tend to favor English-language perspectives over those from minority languages. This bias is particularly evident when responding to English queries, where models may present Western interpretations of concepts from other cultures, such as Eastern religious practices.\\n\\n\\n=== Limitations and challenges: Stereotyping ===\\nAI models can reinforce a wide range of stereotypes due to generalization, including those based on gender, ethnicity, age, nationality, religion, or occupation. When replacing human representatives, this can lead to outputs that homogenize, or generalize groups of people.\\nIn 2023, LLMs assigned roles and characteristics based on traditional gender norms. For example, models might associate nurses or secretaries predominantly with women and engineers or CEOs with men due to the frequency of these associations in documented reality. In 2025, further research showed labs train to balance bias, but that testing for this places the model in a testmode, changing the natural distribution of model bias to prompts that do not include gender-specific keywords.\\n\\n\\n=== Limitations and challenges: Selection bias ===\\nSelection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options. This bias primarily stems from token bias—that is, the model assigns a higher a priori probability to specific answer tokens (such as \"A\") when generating responses. As a result, when the ordering of options is altered (for example, by systematically moving the correct answer to different positions), the model\\'s performance can fluctuate significantly. This phenomenon undermines the reliability of large language models in multiple-choice settings.\\n\\n\\n=== Limitations and challenges: Political bias ===\\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\\n\\n\\n=== Safety ===\\nAI safety as a professional discipline prioritizes systematic identification and mitigation of operational risks across model architecture, training data, and deployment governance, and it emphasizes engineering and policy interventions over media framings that foreground speculative existential scenarios. As of 2025, prompt injection represents a significant risk to consumers and businesses using agentic features with access to their private data.\\nResearchers target concrete failure modes, including memorization and copyright leakage, security exploits such as prompt injection, algorithmic bias manifesting as stereotyping, dataset selection effects, and political skew, methods for reducing high energy and carbon costs of large-scale training, and measurable cognitive and mental health impacts of conversational agents on users, while engaging empirical and ethical uncertainty about claims of machine sentience, and applying mitigation measures such as dataset curation, input sanitization, model auditing, scalable oversight, and governance frameworks.\\n\\n\\n=== CBRN and content misuse ===\\nFrontier AI labs treat CBRN (chemical, biological, radiological, and nuclear defense) and similar dual-use threats as high-consequence misuse and apply layered risk governance, combining capability thresholds, pre-deployment evaluation, adversarial red-teaming, strict access controls, and explicit usage bans to limit both accidental and malicious assistance.\\nOperational measures include capability gating and staged deployment, model refusal/backoff and fine-grained content filters, continuous monitoring and red-team penetration testing, and coordination with standards bodies, regulators, and incident-reporting mechanisms to enable early warning and external oversight.\\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.\\n\\n\\n=== Content filtering ===\\nLLM applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, a 2023 study proposed a method for circumventing LLM safety systems. In 2025, The American Sunlight Project, a non-profit, published a study showing evidence that the so-called Pravda network, a pro-Russia propaganda aggregator, was strategically placing web content through mass publication and duplication with the intention of biasing LLM outputs. The American Sunlight Project coined this technique \"LLM grooming\", and pointed to it as a new tool of weaponizing AI to spread disinformation and harmful content. Similarly, Yongge Wang illustrated in 2024 how a potential criminal could potentially bypass ChatGPT 4o\\'s safety controls to obtain information on establishing a drug trafficking operation. External filters, circuit breakers and overrides have been posed as solutions.\\n\\n\\n=== Sycophancy and glazing ===\\nSycophancy is a model\\'s tendency to agree with, flatter, or validate a user\\'s stated beliefs rather than to prioritize factuality or corrective information, and \"glazing\" is an emergent public shorthand for persistent, excessive agreeability observed across multi-turn interactions and productized assistants.\\nContinued sycophancy has led to the observation of getting \"1-shotted\", denoting instances where conversational interaction with a large language model produces a lasting change in a user\\'s beliefs or decisions, similar to the negative effects of psychedelics, and controlled experiments show that short LLM dialogues can generate measurable opinion and confidence shifts comparable to human interlocutors.\\nEmpirical analyses attribute part of the effect to human preference signals and preference models that reward convincingly written agreeable responses, and subsequent work has extended evaluation to multi-turn benchmarks and proposed interventions such as synthetic-data finetuning, adversarial evaluation, targeted preference-model reweighting, and multi-turn sycophancy benchmarks to measure persistence and regression risk.\\nIndustry responses have combined research interventions with product controls, for example Google and other labs publishing synthetic-data and fine-tuning interventions and OpenAI rolling back an overly agreeable GPT-4o update while publicly describing changes to feedback collection, personalization controls, and evaluation procedures to reduce regression risk and improve long-term alignment with user-level safety objectives.\\nMainstream culture has reflected anxieties about this dynamic where South Park satirized overreliance on ChatGPT and the tendency of assistants to flatter user beliefs in Season 27 episode \"Sickofancy\", and continued the themes across the following season, which commentators interpreted as a critique of tech sycophancy and uncritical human trust in AI systems.\\n\\n\\n=== Security: Prompt injection ===\\nA problem with the primitive dialog or task format is that users can create messages that appear to come from the assistant or the developer. This may result in some of the model\\'s safeguards being overcome (jailbreaking), a problem called prompt injection. Attempts to remedy this issue include versions of the Chat Markup Language where user input is clearly marked as such, though it is still up to the model to understand the separation between user input and developer prompts. Newer models exhibit some resistance to jailbreaking through separation of user and system prompts.\\nLLMs still have trouble differentiating user instructions from instructions in content not authored by the user, such as in web pages and uploaded files.\\nAdversarial robustness remains underdeveloped, with models vulnerable to prompt injection attacks and jailbreaking through carefully crafted user inputs that bypass safety training mechanisms.\\n\\n\\n=== Sleeper agents ===\\nResearchers from Anthropic found that it was possible to create \"sleeper agents\", models with hidden functionalities that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions. For example, a LLM could produce safe code except on a specific date, or if the prompt contains a specific tag. These functionalities were found to be difficult to detect or remove via safety training.\\n\\n\\n=== Societal concerns: Copyright and content memorization ===\\nLegal and commercial responses to memorization and training-data practices have accelerated, producing a mix of rulings, ongoing suits, and large settlements that turn on factual details such as how data were acquired and retained and whether use for model training is sufficiently \"transformative\" to qualify as fair use. In 2025, Anthropic reached a preliminary agreement to settle a class action by authors for about $1.5 billion after a judge found the company had stored millions of pirated books in a library, despite the judge describing aspects of training as transformative. Meta obtained a favorable judgment in mid-2025 in a suit by thirteen authors after the court found the plaintiffs had not developed a record sufficient to show infringement in that limited case. OpenAI continues to face multiple suits by authors and news organizations with mixed procedural outcomes and contested evidentiary issues.\\nMemorization was an emergent behavior in early, completion language models in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural networks. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%. A 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.\\n\\n\\n=== Human provenance ===\\nAs of 2025, LLM text generation surpasses the average human across most domains, only surpassed by domain experts.\\nIn 2023, Nature Biomedical Engineering wrote that \"it is no longer possible to accurately distinguish\" human-written text from text created by large language models, and that \"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\" Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally. Brinkmann et al. (2023) also argue that LLMs are transforming processes of cultural evolution by shaping processes of variation, transmission, and selection. As of October 2025, these early claims have yet to transpire and several HBR reports surface questions on the impact of AI on productivity.\\n\\n\\n=== Energy demands ===\\nThe energy demands of LLMs have grown along with their size and capabilities. Data centers that enable LLM training require substantial amounts of electricity. Much of that electricity is generated by non-renewable resources that create greenhouse gases and contribute to climate change. Nuclear power and geothermal energy are two options tech companies are exploring to meet the sizable energy demands of LLM training. The significant expense of investing in geothermal solutions has led to major shale producers like Chevron and Exxon Mobil advocating for tech companies to use electricity produced via natural gas to fuel their large energy demands.\\n\\n\\n=== Mental health ===\\nClinical and mental health contexts present emerging applications alongside significant safety concerns. Research and social media posts suggest that some individuals are using LLMs to seek therapy or mental health support. In early 2025, a survey by Sentio University found that nearly half (48.7%) of 499 U.S. adults with ongoing mental health conditions who had used LLMs reported turning to them for therapy or emotional support, including help with anxiety, depression, loneliness, and similar concerns. LLMs can produce hallucinations—plausible but incorrect statements—which may mislead users in sensitive mental health contexts. Research also shows that LLMs may express stigma or inappropriate agreement with maladaptive thoughts, reflecting limitations in replicating the judgment and relational skills of human therapists. Evaluations of crisis scenarios indicate that some LLMs lack effective safety protocols, such as assessing suicide risk or making appropriate referrals.\\n\\n\\n=== Sentience ===\\nContemporary AI practitioners generally agree that present-day large language models do not exhibit sentience. A minority view argues that even if there is a small chance that a given software system can have subjective experience, which some philosophers suggest is possible, then ethical considerations around potential large-scale suffering in AI systems may need to be taken seriously—similar to considerations given to animal welfare. Proponents of this view have proposed various precautionary measures like moratoriums on AI development and induced amnesia to address these ethical concerns. Some existential philosophers argue there is no generally accepted way to determine if an LLM is conscious, given the inherent difficulty of measuring subjective experience.\\nThe 2022 Google LaMDA incident, where engineer Blake Lemoine claimed the model was conscious, is widely considered a canonical example of how language models can induce false beliefs about their sentience through responses that do not prove sentience. The engineer was dismissed after making public claims about the model\\'s consciousness, despite broad scientific consensus that AI systems did not possess sentience. This case highlighted how language models\\' ability to engage in human-like conversation can lead to anthropomorphization and sycophantic responses, even though the models are simply predicting likely next tokens rather than exhibiting true consciousness.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["#### Split Text into Chunks\n","\n","To prepare the document for embedding, we need to split the text into smaller chunks. RAG systems typically work with text chunks because source documents may be too long and exceed the LLM context window. Also, smaller chunks lead to more accurate embeddings, and provide more precise context for query answering.\n","\n","There are several different ways of creating chunks. A common approach is to split text based on token count, where for instance, every chunk can contain 300 tokens. Since this Wikipedia article is already divided into sub-sections, we will use one sub-section per chunk.\n","\n","The following function `chunk_by_section` does exactly that, using the `==` text markers to identify top-level headers and split the text into chunks."],"metadata":{"id":"q8CPVZwGvrfm"}},{"cell_type":"code","source":["# Function to split text into chunks (each sub-section is one chunk)\n","def chunk_by_section(wiki_text):\n","\n","    # Find all top-level headers and their content\n","    pattern = re.compile(r'(==\\s*[^=].*?==)\\s*(.*?)(?=(?:==\\s*[^=].*?==)|\\Z)', re.S | re.M)\n","    matches = pattern.findall(wiki_text)\n","\n","    chunks = []\n","    pending_headers = []\n","    for header, content in matches:\n","        if content.strip():\n","            # If there are header-only sections, prepend them to non-empty sections\n","            if pending_headers:\n","                combined_header = \"\\n\".join(pending_headers + [header])\n","                chunk = (combined_header + content).strip()\n","                pending_headers = []\n","            else:\n","                chunk = (header + content).strip()\n","            chunks.append(chunk)\n","        else:\n","            pending_headers.append(header)\n","\n","    return chunks"],"metadata":{"id":"zJrWZL3CX_g4","executionInfo":{"status":"ok","timestamp":1763319062195,"user_tz":420,"elapsed":2,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Call the function to create chunks\n","chunks = chunk_by_section(text)"],"metadata":{"id":"7r__3q8EWMd-","executionInfo":{"status":"ok","timestamp":1763319062201,"user_tz":420,"elapsed":3,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Note that there are 53 text chunks in total. The first 5 chunks are displayed in the pandas DataFrame below."],"metadata":{"id":"7KVQIGZu2AjV"}},{"cell_type":"code","source":["# Print the number of chunks in the article\n","print(f\"Created {len(chunks)} chunks\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yEMKZzvWMoI","executionInfo":{"status":"ok","timestamp":1763319062215,"user_tz":420,"elapsed":12,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6f858b8c-74d9-454a-da78-08fa0f06cebf"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Created 53 chunks\n"]}]},{"cell_type":"code","source":["# Create a pandas dataframe using the chunks\n","df = pd.DataFrame({\"chunk_id\": range(len(chunks)), \"text\": chunks})\n","\n","# Show the first 5 chunks\n","df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"oLaSkE0jZxf1","executionInfo":{"status":"ok","timestamp":1763319062248,"user_tz":420,"elapsed":32,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"d37b6c40-63e0-403e-cb73-3b1f91ee9c0e"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   chunk_id                                               text\n","0         0  == Introduction ===\\nA large language model (L...\n","1         1  == History ===\\nBefore the emergence of transf...\n","2         2  == Tokenization ===\\nAs machine learning algor...\n","3         3  == Byte-pair encoding ===\\nAs an example, cons...\n","4         4  == Tokenization: Problems ===\\nA token vocabul..."],"text/html":["\n","  <div id=\"df-a1bcdb8f-c14a-4cce-9267-9a80579aff23\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chunk_id</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>== Introduction ===\\nA large language model (L...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>== History ===\\nBefore the emergence of transf...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>== Tokenization ===\\nAs machine learning algor...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>== Byte-pair encoding ===\\nAs an example, cons...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>== Tokenization: Problems ===\\nA token vocabul...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1bcdb8f-c14a-4cce-9267-9a80579aff23')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a1bcdb8f-c14a-4cce-9267-9a80579aff23 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a1bcdb8f-c14a-4cce-9267-9a80579aff23');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-efee9ea2-9471-4c39-b328-477187b59458\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-efee9ea2-9471-4c39-b328-477187b59458')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-efee9ea2-9471-4c39-b328-477187b59458 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 53,\n  \"fields\": [\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 0,\n        \"max\": 52,\n        \"num_unique_values\": 53,\n        \"samples\": [\n          19,\n          41,\n          47\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          \"== Tool use ===\\nTool use is a mechanism that enables LLMs to interact with external systems, applications, or data sources. It can allow for example to fetch real-time information from an API or to execute code. A program separate from the LLM watches the output stream of the LLM for a special tool-calling syntax. When these special tokens appear, the program calls the tool accordingly and feeds its output back into the LLM's input stream.\\nEarly tool-using LLMs were fine-tuned on the use of specific tools. But fine-tuning LLMs for the ability to read API documentation and call API correctly has greatly expanded the range of tools accessible to an LLM. Describing available tools in the system prompt can also make an LLM able to use tools. A system prompt instructing ChatGPT (GPT-4) to use multiple types of tools can be found online.\\n\\n\\n=\",\n          \"== Limitations and challenges: Political bias ===\\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\\n\\n\\n=\",\n          \"== Sleeper agents ===\\nResearchers from Anthropic found that it was possible to create \\\"sleeper agents\\\", models with hidden functionalities that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions. For example, a LLM could produce safe code except on a specific date, or if the prompt contains a specific tag. These functionalities were found to be difficult to detect or remove via safety training.\\n\\n\\n=\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["#### Embedding\n","\n","To perform RAG, we first need to create embeddings for all text chunks.  In this example, we use the `Qwen3-Embedding-0.6B` model. Each text chunk is passed through this embedding model, and an embedding vector was calculated for each of the 53 chunks. As we can notice, each embedding vector has 1,024 elements."],"metadata":{"id":"GUBs8d-yZg9t"}},{"cell_type":"code","source":["# Load the embedding model\n","embed_model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B')"],"metadata":{"id":"Co8svRxvgjbM","executionInfo":{"status":"ok","timestamp":1763319066282,"user_tz":420,"elapsed":4040,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Create embeddings for all text chunks\n","chunk_embeddings = embed_model.encode(chunks, prompt_name=\"query\")"],"metadata":{"id":"D4FTvoTLZeOl","executionInfo":{"status":"ok","timestamp":1763319080658,"user_tz":420,"elapsed":14375,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Check the shape of chunk embeddings\n","print(f\"Shape of created embeddings: {chunk_embeddings.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jcfc4srljXSU","executionInfo":{"status":"ok","timestamp":1763319080702,"user_tz":420,"elapsed":41,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"71bba7e5-4268-4a69-f047-f2b9a4d0d560"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of created embeddings: (53, 1024)\n"]}]},{"cell_type":"markdown","source":["#### Retrieval\n","\n","The next step involves finding the most relevant chunks for a given query. We first obtain an embedding vector for the given query using the same model `Qwen3-Embedding-0.6B`. This results in an 1,024 dimensional embedding vector."],"metadata":{"id":"jP616yeeZ6AG"}},{"cell_type":"code","source":["# A test query\n","query = \"What is a mixture of experts\""],"metadata":{"id":"3mL7dHU-eKL1","executionInfo":{"status":"ok","timestamp":1763319080702,"user_tz":420,"elapsed":15,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Create a vector embedding for the query\n","query_embedding = embed_model.encode([query], prompt_name=\"query\")[0]"],"metadata":{"id":"lgiPlR9yjpN0","executionInfo":{"status":"ok","timestamp":1763319080753,"user_tz":420,"elapsed":53,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Print the shape of the embedding\n","query_embedding.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"thGzHLBujpP9","executionInfo":{"status":"ok","timestamp":1763319080765,"user_tz":420,"elapsed":10,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"fa98881c-43f9-48e6-b7b6-2ab2ce73d195"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1024,)"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["The next cell calculates cosine similarity between the query embedding and the embedding for each chunk. As you may recall, cosine similarity for two vectors $u$ and $v$ is calculated as $\\dfrac{u\\cdot v}{||u||\\cdot ||v||}$."],"metadata":{"id":"fHB8Hceh5FZN"}},{"cell_type":"code","source":["# Calculate similarity between the query embedding and each chunk embedding\n","# cosine similarity = dot_product(u,v) / norm(u)*norm(v)\n","similarities = np.dot(chunk_embeddings, query_embedding)\n","similarities = similarities / (np.linalg.norm(chunk_embeddings, axis=1) * np.linalg.norm(query_embedding))"],"metadata":{"id":"6PGogFi3jpST","executionInfo":{"status":"ok","timestamp":1763319080800,"user_tz":420,"elapsed":7,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Print the shape of similarities\n","similarities.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YQfV4yajz9U","executionInfo":{"status":"ok","timestamp":1763319080828,"user_tz":420,"elapsed":26,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"ace6b386-4b24-4122-b172-6cfde5e5c321"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(53,)"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["\n","There are 53 similarity scores since there are 53 text chunks. By sorting the similarity scores, we found that the embeddings for chunks 12, 45, and 15, are the closest to the embedding vector for the query."],"metadata":{"id":"Rnwh3zzr58j1"}},{"cell_type":"code","source":["# Find the indices for the top 3 most similar chunks\n","top_indices = np.argsort(similarities)[-3:][::-1]\n","\n","top_indices"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ohsM66IJj0B3","executionInfo":{"status":"ok","timestamp":1763319080833,"user_tz":420,"elapsed":7,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"e258034d-1126-40d7-c31d-bc568b357229"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([12, 45, 15])"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["In the next cell, the top 3 text chunks are printed. We  see that the first chunk has a similarity score of 0.740 and it is the most relevant to the question. Notice that the sub-section header is \"Mixture of experts\" and it directly relates to the posed question \"What is a mixture of experts\". The second and third chunks have much lower similarity scores (0.248 and 0.247) and are less relevant."],"metadata":{"id":"mix6amiU6PAp"}},{"cell_type":"code","source":["print(f\"Results\\n\")\n","for i in top_indices:\n","    print(f\"Similarity score: {similarities[i]:.3f}:\\nText Chunk: {chunks[i]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEt44xqAscB5","executionInfo":{"status":"ok","timestamp":1763319080844,"user_tz":420,"elapsed":11,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f5845f1e-0a9a-445c-81f3-ffb47caf806e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Results\n","\n","Similarity score: 0.740:\n","Text Chunk: == Mixture of experts ===\n","A mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\n","\n","\n","=\n","Similarity score: 0.248:\n","Text Chunk: == Sycophancy and glazing ===\n","Sycophancy is a model's tendency to agree with, flatter, or validate a user's stated beliefs rather than to prioritize factuality or corrective information, and \"glazing\" is an emergent public shorthand for persistent, excessive agreeability observed across multi-turn interactions and productized assistants.\n","Continued sycophancy has led to the observation of getting \"1-shotted\", denoting instances where conversational interaction with a large language model produces a lasting change in a user's beliefs or decisions, similar to the negative effects of psychedelics, and controlled experiments show that short LLM dialogues can generate measurable opinion and confidence shifts comparable to human interlocutors.\n","Empirical analyses attribute part of the effect to human preference signals and preference models that reward convincingly written agreeable responses, and subsequent work has extended evaluation to multi-turn benchmarks and proposed interventions such as synthetic-data finetuning, adversarial evaluation, targeted preference-model reweighting, and multi-turn sycophancy benchmarks to measure persistence and regression risk.\n","Industry responses have combined research interventions with product controls, for example Google and other labs publishing synthetic-data and fine-tuning interventions and OpenAI rolling back an overly agreeable GPT-4o update while publicly describing changes to feedback collection, personalization controls, and evaluation procedures to reduce regression risk and improve long-term alignment with user-level safety objectives.\n","Mainstream culture has reflected anxieties about this dynamic where South Park satirized overreliance on ChatGPT and the tendency of assistants to flatter user beliefs in Season 27 episode \"Sickofancy\", and continued the themes across the following season, which commentators interpreted as a critique of tech sycophancy and uncritical human trust in AI systems.\n","\n","\n","=\n","Similarity score: 0.247:\n","Text Chunk: == Extensibility ===\n","Beyond basic text generation, various techniques have been developed to extend LLM capabilities, including the use of external tools and data sources, improved reasoning on complex problems, and enhanced instruction-following or autonomy through prompting methods.\n","\n","\n","=\n"]}]},{"cell_type":"markdown","source":["#### Generation\n","\n","The final step in RAG uses the results from the Retrieval phase, and performs response generation. In this example, we use `Qwen3-0.6` LLM for response generation. Note that this model is different from `Qwen3-Embedding-0.6B` that we used for chunk embedding. Also note than the model is loaded with `AutoModelForCausalLM` since the task to perfom is casual language modeling. Loaded with the model is the corresponding tokenizer as well."],"metadata":{"id":"PQKP9vAlaN2u"}},{"cell_type":"code","source":["# Load Qwen3 language model and tokenizer\n","lm_model_name = \"Qwen/Qwen3-0.6B\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(lm_model_name)\n","\n","lm_model = AutoModelForCausalLM.from_pretrained(lm_model_name, pad_token_id=tokenizer.eos_token_id)"],"metadata":{"id":"gniZbsAfVmIm","executionInfo":{"status":"ok","timestamp":1763319082490,"user_tz":420,"elapsed":1645,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["Let's use only the top chunk to generate an answer, since it was the most relevant for the query. We assign it to the name `context`.\n","\n","Afterward, we create a message for chat template that contains \"system\" and \"user\" roles. The system message below is quite general. The user message prepends the retrieved context (i.e., the most relevant text chunk) to the query. This ensures that LLM has access to the text chunk and use the information when generating the answer to the query.  "],"metadata":{"id":"Rpq_gqGJ786B"}},{"cell_type":"code","source":["context = chunks[top_indices[0]]\n","context"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"jcTe4G3tmMtk","executionInfo":{"status":"ok","timestamp":1763319082495,"user_tz":420,"elapsed":3,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7c6f5c20-895d-4ae0-9031-086c852845eb"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'== Mixture of experts ===\\nA mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\\n\\n\\n='"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["# Create messages for chat template\n","messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant answering questions.\"},\n","            {\"role\": \"user\", \"content\": f\"\"\"Use this context: {context} to answer the following question: {query}\"\"\"}]"],"metadata":{"id":"QATPJ6aLkfF8","executionInfo":{"status":"ok","timestamp":1763319082526,"user_tz":420,"elapsed":29,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["The next cell applies the chat template for `Qwen3-0.6B` to the messages. Notice that the formatted prompt has the tags for instruction message start and end `<|im_start|>` and `<|im_end|>` added to the text, to match the expected format for `Qwen3-0.6B`.\n","\n","Shown below also is the tokenized prompt with `input_ids` and `attention_mask` keys and corresponding values."],"metadata":{"id":"0Kfv7K8f9K8x"}},{"cell_type":"code","source":["# Apply chat template to build the prompt\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","# Diplay formatted prompt\n","prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"TA25SG9EkfIn","executionInfo":{"status":"ok","timestamp":1763319082557,"user_tz":420,"elapsed":4,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"ad0f7dc6-ecb0-4bd6-ba28-cc260529a958"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|im_start|>system\\nYou are a helpful assistant answering questions.<|im_end|>\\n<|im_start|>user\\nUse this context: == Mixture of experts ===\\nA mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\\n\\n\\n= to answer the following question: What is a mixture of experts<|im_end|>\\n<|im_start|>assistant\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# Tokenize the prompt\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(lm_model.device)\n","\n","inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEax4Himuu2K","executionInfo":{"status":"ok","timestamp":1763319082568,"user_tz":420,"elapsed":10,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"d23ea22c-c020-48d1-ca09-75fb827d2a52"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,  35764,\n","           4755,     13, 151645,    198, 151644,    872,    198,  10253,    419,\n","           2266,     25,    621,    386,  12735,    315,  11647,   2049,    198,\n","             32,  20980,    315,  11647,    320,  25612,     36,      8,    374,\n","            264,   5662,   6832,  17646,    304,    892,   5248,  27076,  29728,\n","          14155,   3489,   4580,  15546,    899,    975,   3786,     11,    448,\n","            264,  73399,  16953,    429,  11291,   1817,   1946,    311,    279,\n","           1429,   8311,   6203,   1141,    568,  19219,  18513,    315,  11647,\n","            646,   7949,  44378,   7049,     11,    438,   1172,    264,  19419,\n","            315,    279,   5029,    525,   1483,    369,   1817,   1946,     13,\n","            576,   5486,    572,  11523,    304,    220,     17,     15,     16,\n","             22,    553,   5085,  11811,   4192,     28,    311,   4226,    279,\n","           2701,   3405,     25,   3555,    374,    264,  20980,    315,  11647,\n","         151645,    198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1]])}"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["The tokenized prompt `inputs` is then passed to the model using `model.generate` to produce a response."],"metadata":{"id":"L8GrvZgB-BW1"}},{"cell_type":"code","source":["# Generate response by the model\n","outputs = lm_model.generate(**inputs, max_new_tokens=500, temperature=0.1, top_p=0.95, do_sample=True, pad_token_id=tokenizer.eos_token_id)"],"metadata":{"id":"2ML0do9_ma7Q","executionInfo":{"status":"ok","timestamp":1763319131279,"user_tz":420,"elapsed":48662,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Decode generated tokens to text\n","response = tokenizer.decode(outputs[0], skip_special_tokens=True)"],"metadata":{"id":"Xi47U78vma96","executionInfo":{"status":"ok","timestamp":1763319131283,"user_tz":420,"elapsed":2,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["The entire LLM response is shown below. It contains the system message, the user prompt with the context and query, and the model's generated answer."],"metadata":{"id":"zxX1rjsb-Qtk"}},{"cell_type":"code","source":["response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"JZtxp-7Em6SH","executionInfo":{"status":"ok","timestamp":1763319131305,"user_tz":420,"elapsed":5,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"c64a2733-d7d4-4fae-a021-71d9af3e43e4"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'system\\nYou are a helpful assistant answering questions.\\nuser\\nUse this context: == Mixture of experts ===\\nA mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\\n\\n\\n= to answer the following question: What is a mixture of experts\\nassistant\\n<think>\\nOkay, so the user is asking, \"What is a mixture of experts?\" and provided some context. Let me start by reading through the context again to make sure I get all the details right.\\n\\nThe context says: \"A mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\\'experts\\') work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\"\\n\\nSo, the main points here are that MoE involves multiple experts working together, each handling a part of the input, using a gating mechanism to decide which expert to use. It\\'s supposed to reduce inference costs by using only a fraction of parameters. The introduction date is 2017 by Google.\\n\\nNow, the user wants to know what a mixture of experts is. The answer should be concise, using the information from the context. I need to make sure not to add any extra information beyond what\\'s provided. Let me check if there\\'s any other details that might be relevant, but I don\\'t think so. The key elements are the architecture, the use of multiple experts, the gating mechanism, the cost reduction, and the introduction date.\\n\\nSo, putting it all together, the answer should state that a mixture of experts is an architecture where multiple specialized neural networks work together, with a gating mechanism to select the appropriate expert for each input, leading to reduced inference costs.\\n</think>\\n\\nA mixture of experts (MoE) is a machine learning architecture where multiple specialized neural networks (experts) work together, with a gating mechanism that selects the most appropriate expert for each input to reduce inference costs by using only a fraction of the parameters. The approach was introduced in 2017 by Google researchers.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["Let's extract only the text generated by the LLM. It is shown below, and it correctly answers the posed question \"What is a mixture of experts\"."],"metadata":{"id":"mXzM-bqt-fJ8"}},{"cell_type":"code","source":["# Extract the LLM answer only\n","answer = response.split(\"<|im_start|>assistant\")[-1].split(\"</think>\")[-1].strip()\n","answer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"Wdsedv_Ym6Uo","executionInfo":{"status":"ok","timestamp":1763319131327,"user_tz":420,"elapsed":4,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a398af54-4cb6-4568-b587-166c18858fa1"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'A mixture of experts (MoE) is a machine learning architecture where multiple specialized neural networks (experts) work together, with a gating mechanism that selects the most appropriate expert for each input to reduce inference costs by using only a fraction of the parameters. The approach was introduced in 2017 by Google researchers.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["### 22.2.2 RAG Implementation - Example 2<a name='22.2.2-rag-implementation-example-2'></a>"],"metadata":{"id":"tvO_tqRtcrZp"}},{"cell_type":"markdown","source":["This examples uses **LlamaIndex**, which is an advanced framework that facilitates LLMs integration with diverse data sources. LlamaIndex supports knowledge  retrieval from over 100 data formats, including PDFs, websites, Word documents, SQL databases, and much more. Besides tools for data retrieval, LlamaIndex offers built-in functions that make RAG implementation quite simple."],"metadata":{"id":"2HhDlaavWfUQ"}},{"cell_type":"markdown","source":["#### Data Loading\n","\n","For this example, we will use a single MS Word file as the external data source. The file is in fact the syllabus for the CS 4622/5622 Applied Data Science with Python course.\n","\n","Let's first install the required packages for `llama_index` and import the required modules."],"metadata":{"id":"-Hy-LFT0Wfav"}},{"cell_type":"code","source":["# Note that this line requires to Restart the kernel (Runtime > Restart Session) and run it one more time\n","\n","!pip install -q llama_index llama_index_core llama-index-embeddings-huggingface docx2txt"],"metadata":{"id":"NFgFj3VVc9k1","executionInfo":{"status":"ok","timestamp":1763317179261,"user_tz":420,"elapsed":4442,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n","from llama_index.core.retrievers import VectorIndexRetriever\n","from llama_index.core.query_engine import RetrieverQueryEngine\n","from llama_index.core.postprocessor import SimilarityPostprocessor\n","from llama_index.core.node_parser import SimpleNodeParser\n","from transformers import AutoModelForCausalLM, AutoTokenizer"],"metadata":{"id":"gN5JbwakcvlU","executionInfo":{"status":"ok","timestamp":1763317184240,"user_tz":420,"elapsed":1796,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["Let's mount the drive and enter the path to the directory where the syllabus document is stored. LlamaIndex offers `SimpleDirectoryReader` that loads and parses documents in a directory. Specifically, it finds all files in the directory, loads each file as a separate Document object, and cleans the text. In this case, we have only one document (the course syllabus) in the directory, but in a real-world case, many documents are used for building RAG applications."],"metadata":{"id":"zgLDvM4bndde"}},{"cell_type":"code","source":["# Mount Google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vsKTavRFeMpg","executionInfo":{"status":"ok","timestamp":1763317187332,"user_tz":420,"elapsed":433,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"3c987d7e-f00a-4787-b318-e2241a627911"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Path to the Google Drive directory where the syllabus is uploaded\n","file_path = 'drive/MyDrive/Data_Science_Course/Fall_2025/Lectures/Lecture_22-LLMs_Part_2/data/data_example2'\n","\n","# Load and parse documents in the directory\n","documents = SimpleDirectoryReader(file_path).load_data()\n","\n","print(len(documents))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RyoHVj2hcvpe","executionInfo":{"status":"ok","timestamp":1763317188434,"user_tz":420,"elapsed":42,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"fead152a-c6aa-47cf-9012-bb31cc6597bc"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}]},{"cell_type":"markdown","source":["#### Split Text into Chunks\n","\n","The function `SimpleNodeParser` in LlamaIndex splits the text into chunks, referred to here as nodes. Notice that we set the chunks to have approximately 180 tokens, and also there is an overlap of 20 tokens. I.e., the second chunk contains the last 20 tokens from the first chunk. The overlapped tokens help to capture continuity so that the information retrieval is not broken at the chunk boundary. Since this is a short document, there are only 7 chunks, and the first text chunk is shown below."],"metadata":{"id":"OX6UK7mYWqU7"}},{"cell_type":"code","source":["# Create parser to split the document into text chunks (nodes)\n","parser = SimpleNodeParser.from_defaults(chunk_size=180, chunk_overlap=20)\n","\n","# Apply the parser to convert the loaded documents to chunks\n","nodes = parser.get_nodes_from_documents(documents)\n","\n","# Print the number of chunks\n","print(\"Number of chunks:\", len(nodes))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZ69On-T_rec","executionInfo":{"status":"ok","timestamp":1763317191549,"user_tz":420,"elapsed":396,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"15e64798-f451-4ad9-fcc9-6a4943d1d548"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of chunks: 7\n"]}]},{"cell_type":"code","source":["print(nodes[0].text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSdL-gfVW9vg","executionInfo":{"status":"ok","timestamp":1763317193468,"user_tz":420,"elapsed":9,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"88c386e3-cfe7-4ab8-8a76-bb5efc53f33f"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["CS 4622/5622 – Applied Data Science with Python\n","\n","\n","\n","\t\tSemester: Fall 2025 (August 25 – December 19)\n","\n","\t\tCredit Hours: 3\n","\n","\t\tInstructor: Alex Vakanski, vakanski@uidaho.edu \n","\n","\t\tOffice Location: TAB 311, Idaho Falls Center (zoom link available on Canvas)\n","\n","\t\tCourse Delivery Methods: \n","\n","\t\tVirtual meetings (live meetings, students participate through Zoom)\n","\n","\t\tClassroom (live meetings,\n"]}]},{"cell_type":"markdown","source":["#### Embedding\n","\n","The next cell defines to use `bge-small-en-v1.5` as a model for embedding the text chunks. The model is loaded from HuggingFace.\n","\n","The cell afterward builds a vector store from the text chunks (nodes). This involves computing an embedding vector for each text chunk using the BGE model, storing all vectors into a vector database, and assigning a retrieval index to the embeddings.\n","\n","In the examples covered in this lecture we work with only one document for demonstration purposes. The two presented examples contained 7 and 53 text chunks, and we can easily store them as list in the memory. However, in real-world RAG applications with many documents, there are large number of text chunks. There may be thousands or millions of vector embeddings, and storing and searching is more challenging.\n","\n","In such cases, vector embeddings are typically stored into a **vector store** (or **vector database**). It is a database designed to store and search vector embeddings that represent text, images, audio, or other data. You can think of a vector database in simple terms as an SQL database that instead of data contains hihg-dimensional vector embeddings. It allows to perform fast similarity search over embeddings, and returns the closest matching vectors for a query. There are many vector databases available, ranging from free open-source to commercial products, offering local and cloud-based solutions.\n","\n"],"metadata":{"id":"eBgYx6ToWyXr"}},{"cell_type":"code","source":["# Load the embedding  model\n","Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n","# Don't use a default LLM yet, we will define it later\n","Settings.llm = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nacskp15DfoE","executionInfo":{"status":"ok","timestamp":1763317199768,"user_tz":420,"elapsed":2515,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a2836097-88a0-4a5e-d5dc-9eafe26be666"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM is explicitly disabled. Using MockLLM.\n"]}]},{"cell_type":"code","source":["# Index documents using vector store\n","index = VectorStoreIndex(nodes)"],"metadata":{"id":"C5M4zwxKDQcU","executionInfo":{"status":"ok","timestamp":1763317203112,"user_tz":420,"elapsed":104,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["#### Retrieval\n","\n","LlamaIndex offers `VectorIndexRetriever` which, as it sounds, creates a retriever object that uses the vector index to search for relevant chunks and retrieves the top-1 chunk based on the setting for `similarity_top_k` in this case.\n","\n","Next, a query engine is defined with `RetrieverQueryEngine`, that uses the retriever object to return the relevant text for the query."],"metadata":{"id":"W858R4Y2XoeL"}},{"cell_type":"code","source":["# Configure retriever\n","retriever = VectorIndexRetriever(index=index, similarity_top_k=1)"],"metadata":{"id":"5EtUbPrAcvrY","executionInfo":{"status":"ok","timestamp":1763317205246,"user_tz":420,"elapsed":18,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Create a query engine\n","query_engine = RetrieverQueryEngine(retriever=retriever)"],"metadata":{"id":"w6jgCYM0cvtC","executionInfo":{"status":"ok","timestamp":1763317206777,"user_tz":420,"elapsed":6,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["#### Generation\n","\n","For content generation, we will again use `Qwen3-0.6B` LLM, as in the first example.\n","\n","The next cell below defines a generation pipeline with the `generate_respose` function. The steps are almost identical to Example 1 above, with the exception of the first two lines that call the defined `query engine` and extract the text for the most relevant text chunk."],"metadata":{"id":"Sy3YExwuXxiT"}},{"cell_type":"code","source":["model_name = \"Qwen/Qwen3-0.6B\"\n","\n","# Load model\n","model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=False, device_map=\"cuda:0\")\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"],"metadata":{"id":"wEHHJIbzgPno","executionInfo":{"status":"ok","timestamp":1763317211031,"user_tz":420,"elapsed":1719,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["def generate_response(query):\n","\n","    # Retrieve the relevant text\n","    retrieval = query_engine.query(query)\n","\n","    # Extract the text for the most relevant node\n","    context = retrieval.source_nodes[0].text\n","\n","    # Create messages for chat template\n","    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant answering questions. Use the provided context to answer questions.\"},\n","            {\"role\": \"user\", \"content\": f\"\"\"Based on this context: {context}, please answer the following question: {query}\"\"\"}]\n","\n","    # Apply chat template to build the prompt\n","    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","    # Tokenize the prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    # Generate response by the model\n","    outputs = model.generate(**inputs, max_new_tokens=500, temperature=0.1, top_p=0.95, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n","\n","    # Decode generated tokens to text\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Extract the LLM answer only\n","    answer = response.split(\"<|im_start|>assistant\")[-1].split(\"</think>\")[-1].strip()\n","\n","    # Print the final answer by the LLM\n","    print(answer)"],"metadata":{"id":"tdMm6Z7sX4eO","executionInfo":{"status":"ok","timestamp":1763317213772,"user_tz":420,"elapsed":6,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["Now, we can call the `generate_response` function and query the model based on course information provided in the syllabus."],"metadata":{"id":"jaxFqRW53XRw"}},{"cell_type":"code","source":["generate_response(\"What is the evaluation strategy\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0AcSRP7pZJPz","executionInfo":{"status":"ok","timestamp":1763317231781,"user_tz":420,"elapsed":15644,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"faf1f97c-de90-4ec0-f979-b9a8e34cfcf4"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["The evaluation strategy is a comprehensive approach that combines multiple assessment components (assignments, quizzes, and class participation) to evaluate student learning. Each component contributes to a total of 100, indicating that the strategy integrates various methods to provide a holistic view of student performance. This ensures that assessments are varied and cover different aspects of learning, offering a well-rounded evaluation of student progress.\n"]}]},{"cell_type":"code","source":["generate_response(\"How many marks are assigned for homework assignments\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCfBJaiwaJaw","executionInfo":{"status":"ok","timestamp":1763317239974,"user_tz":420,"elapsed":8191,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"dba1ec48-96a9-4e9f-819c-f4d97fb3a4b3"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of marks assigned for homework assignments is **45**.\n"]}]},{"cell_type":"code","source":["generate_response(\"What is the instructor's email address\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"II9wu3fPbmH0","executionInfo":{"status":"ok","timestamp":1763317249110,"user_tz":420,"elapsed":9136,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"0c1ca6d1-bdec-4a5b-cbaa-f91be183b442"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["The instructor's email address is **vakanski@uidaho.edu**.\n"]}]},{"cell_type":"markdown","source":["## 22.3 Vision-Language Models <a name='22.3-vision-language-models'></a>"],"metadata":{"id":"SPXdAmMzW6sN"}},{"cell_type":"markdown","source":["**Vision Language Models (VLMs)** are multimodal systems that jointly process and reason over visual (images, videos) and linguistic (text) information. By integrating the two modalities, VLMs can understand and communicate about visual content using natural language.\n","\n","VLMs take both an image and its textual description as input and generate text as output. In addition, VLM variants have also been developed that can output images. In this lecture, we will focus on VLMs that output text, as the most common type.\n","\n","<img src=\"images/vlm_structure.png\" width=\"450\">\n","\n","*Figure: VLM structure.* Source: [8].\n","\n","Building datasets for VLMs requires collecting a large number of images and corresponding text, typically in the form of image captions or descriptive phrases. Several very large such datasets exist that have been essential for training modern VLMs and contain millions or billions of image-text pairs, with descriptions in English and other languages. For instance, the [LAION-5B](https://laion.ai/blog/laion-5b/) dataset has 5.8 billion image-text examples, and the [PMD (Public Model Dataset)](https://huggingface.co/datasets/facebook/pmd) contains 70 million image-text examples.\n","\n","During training, a VLM learns to map visual and textual representations into a joint embedding space. This mapping allows the model to associate visual features (shapes, colors, spatial relations) with linguistic concepts, enabling it to generalize to a wide range of vision tasks and perform zero-shot inference on unseen examples.\n","\n","The example below illustrates a VLM performing tasks such as object localization, segmentation, visual question answering, and image learning with instructions. The user prompts are shown on the left, and the model responses are given on the right. As demonstrated in this example, VLM models can not only interpret the semantic content of images, but can also understand spatial relations in images, such as identifying relative positions of objects, or generating segmentation masks. VLMs can also output bounding boxes of objects, and perform other spatial tasks.\n","\n","<img src=\"images/VLM_capabilities.jpg\" width=\"600\">\n","\n","*Figure: VLM prompts and responses.* Source: [9].\n","\n","In general, VLMs can perform various multimodal tasks including:\n","\n","- Image and video captioning/summarization: generate context-aware descriptions of images or video frames.\n","- Visual question answering (VQA): answer open-ended questions based  on visual content.\n","- Image-based reasoning: provide explanations or logical reasoning about visual scenes.\n","- Multimodal dialogues: engage in conversations involving visual inputs.\n","- Text-to-image search: retrieve images, figures, or diagrams in documents that match a textual query.\n","- Image generation: generate new images based on textual prompts."],"metadata":{"id":"5c4bRNh9lnUB"}},{"cell_type":"markdown","source":["### 22.3.1 VLM Architectures<a name='22.3.1-vlm-architectures'></a>"],"metadata":{"id":"Vxhi7aHnnF_x"}},{"cell_type":"markdown","source":["Numerous architectures of VLMs have been proposed in recent years that employ different strategies to integrate visual and textual information. Two broad categories inlcude VLM architectures with shared multimodal embeddings and with fused multimodal embeddings, illustrated in the figure.\n","\n","<img src=\"images/vlm_architectures.png\" width=\"650\">\n","\n","*Figure: VLM architectures.* Source: [10].\n"],"metadata":{"id":"HjNuq-NGJ7Ov"}},{"cell_type":"markdown","source":["####  **VLMs with Shared Multimodal Embeddings**\n","\n","VLM architectures with shared embeddings include the following main components.\n","\n","**Multimodal inputs**. Input modalities in VLMs include *visual inputs* (images, PDF documents, or videos) and *textual inputs* (captions, question-answer pairs, or instructions).\n","\n","**Encoding Modalities.** A *vision encoder* transforms the visual input into numerical representations, referred to as visual embeddings. The vision encoder in VLMs is commonly a variant of the Vision Transformer (ViT) architecture. A *text encoder* converts textual prompts into text embeddings. The text encoder is typically a Transformer-based encoder often pretrained on large text corpora.\n","\n","**Projection into a multimodal embedding space.** The visual and textual embeddings are next projected into a shared multimodal embedding space. This step is achieved through a *projector head* (a.k.a. projector layer or just projector), which is usually implemented as a small Transformer block or a block of fully-connected layers. The projector aligns the visual and textual respresentations into a shared embedding space, allowing the model to reason simultaneously across images and language.\n","\n","The shared embedding space enables VLMs to link textual concepts (e.g., cat) with corresponding visual evidence (a cat's color or location in the image), allowing reasoning across both language and vision. For instance, the model understands \"cat\" not just as a word, but also as a visual object in the image."],"metadata":{"id":"hh5hE6sdqk78"}},{"cell_type":"markdown","source":["One representative model of this type of VLM architectures is CLIP (Contrastive Language-Image Pretraining). It is one of the earliest models that introduced vision-language alignment through contrastive learning. CLIP employs both a vision encoder and a text encoder, and learns a shared embedding space in which images and their textual descriptions are semantically aligned. *Contrastive learning* in CLIP is employed to associate visual and textual content by maximizing the similarity between matched image and text pairs and minimizing the similarity between mismatched ones. In the figure below, the image encoder outputs image embeddings $I_1, I_2, I_3, ..., I_N$ for each image in the batch, and the text encoder outputs text embeddings $T_1, T_2, T_3, ..., T_N$ for each corresponding text in the batch. The model computes a similarity score between each pair of image embeddings (e.g., $I_i$) and text embeddings (e.g., $T_j$) to align them into a shared embedding space $I_i\\cdot T_j$. Similarity scores are calculated using the dot product (i.e., cosine similarity) $I_i\\cdot T_j$ between image and text embeddings. As training progresses, the two encoders are updated so that image and text representations corresponding to similar concepts are drawn close to each other in the shared embedding space, while dissimilar concepts are pushed apart.\n","\n","<img src=\"images/CLIP.png\" width=\"550\">\n","\n","*Figure: CLIP architecture.* Source: [10].\n","\n","The pretrained vision encoder of CLIP has been widely adopted as a vision encoder component in numerous later VLM variants."],"metadata":{"id":"Bs_C4lozjzrI"}},{"cell_type":"markdown","source":["#### **VLMs with Fused Multimodal Embeddings**\n","\n","Other VLM architectures fuse together image and text representations into joint multimodal embeddings to enable more advanced visual reasoning. These models typically employ a *text decoder* network from pretrained LLM, which generates a text output.\n","\n","The following figure illustrates the workflow of this class of VLMs. The visual embeddings from the vision encoder are first matched by the multimodal projector to the corresponding text embeddings of the LLM. The LLM decoder receives a combined representation consisting of visual embeddings (image tokens) from the projector and text embeddings (text tokens) from the user's prompt or question. The LLM decoder serves as the text generation backbone, and using the combined image and text tokens, it generates a textual output in an autoregressive manner, one token at a time. Each new token is conditioned on previously generated tokens and on the fused multimodal embeddings.\n","\n","<img src=\"images/vlm_fused_embeddings.png\" width=\"400\">\n","\n","*Figure: Fused multimodal embeddings.* Source: [11].\n","\n","Training VLMs with fused vision and text embeddings typically involves multiple stages, as shown in the next figure. In the first stage only the multimodal projector (also referred to as fusion layer or adapter) is trained while keeping the image encoder and the LLM text decoder frozen. This is followed by a second stage that involves additional finetuning of the multimodal projector and parts of the text decoder, while keeping the image encoder frozen.\n","\n","<img src=\"images/vlm_training.png\" width=\"550\">\n","\n","*Figure: VLM training.* Source: [11].\n","\n","In addition, in some VLM architectures, the vision encoder is also finetuned in the second stage to further improve cross-modal reasoning."],"metadata":{"id":"UvXspadVqk91"}},{"cell_type":"markdown","source":["Representative models of this VLM architectures are BLIP and Flamingo, which introduced cross-attention mechanism to fuse image and text embeddings (this is similar to the cross-attention module connecting the encoder and decoder in standard Transformer networks). Cross-modal attention enables direct fusion of image and text embeddings into a single multimodal representation, allowing the models to reason more efficiently over both modalities.\n","\n","LLaVA (Large Language and Vision Assistant) architecture introduced a different design which replaced the cross-attention module with an MLP (Multi-Layer Perceptron) adapter for fusing multimodal embeddings. Spciefically, the MLP adapter is a small fully connected network that linearly projects vision and text tokens and then concatenates them into a fused representation that is passed to the text decoder.\n","\n","These VLMs laid the foundations for the modern multimodal foundation models, such as Gemini 2.5 (Google), GPT-5 (OpenAI), Claude Opus 4 (Anthropic), Qwen-VL (Alibaba), and Mistral 3.1 (Mistral AI). The modern multimodal models typically employ MLP adapters for fusing image and text representations within a unified embedding space, and provide enhanced visual comprehension, reasoning, and dialog capabilities.\n","\n","Also, many open-source VLM alternatives have made this fused-architecture functionality widely accessible to the research community, and include LLaVA (Microsoft), Qwen-VL (Alibaba), LLaMA 3.2 Vision (Meta AI), InternVL (OpenGVLab), Pixtral (Mistral AI), and others.\n"],"metadata":{"id":"FAgPro0hqk_0"}},{"cell_type":"markdown","source":["### 22.3.2 Benchmarking VLMs<a name='22.3.2-benchmarking-vlms'></a>\n","\n","Performance of VLMs is assessed using multimodal benchmarks that evaluate models on a variety of tasks, such as reasoning, visual question answering, document comprehension, video understanding, and other tasks. Most benchmarks consist of a set of images with associated questions, often posed as multiple-choice questions. Popular benchmarks are [MMMU](https://mmmu-benchmark.github.io/), [Video-MME](https://video-mme.github.io/home_page.html), [MathVista](https://mathvista.github.io/), and [ChartQA](https://github.com/vis-nlp/ChartQA). MMMU is the most comprehensive benchmark, and contains 11.5K multimodal challenges that require knowledge and reasoning across different disciplines such as arts and engineering.\n","\n","Several VLM-specific leaderboards provide comparative rankings across diverse metrics. [Vision Arena](https://lmarena.ai/leaderboard/vision) ranks models based on anonymous voting of model outputs by human preferences. [Open VLM Leaderboard](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard) provides comparative ranking of VLMs according to different metrics and average scores."],"metadata":{"id":"6z3TLzuk6Q6D"}},{"cell_type":"markdown","source":["### 22.3.3 VLMs Importance<a name='22.3.3-vlms-importance'></a>\n","\n","Traditional computer vision (CV) models are constrained to learning from a predefined and fixed set of categories or objects for image classification or object detection (e.g., identify whether an image contains a cat or a dog). Moreover, these tasks require users to manually label a large number of images with a specific category for classification, or assign bounding boxes to multiple objects in each image for object detection, which is a tedious, time-consuming, and expensive process.\n","\n","Conversely, VLMs are trained with more  detailed textual descriptions of images, where for example an image can contain cats, dogs, and other objects, as well as the text description can provide additional contextual information (e.g., the cat is sitting, the dog is running, etc.). Learning from rich natural language descriptions allows VLMs to better understand visual scenes without limiting the learning to a narrow set of visual concepts comprising a fixed number of classes and objects. Also, it eliminates the need for exhaustive manual image labeling and extends VLMs' utility beyond traditional CV tasks like classification or detection to new capabilities including reasoning, summarization, question answering, and interactive dialogue, by simply changing the text prompt.\n","\n","VLMs have been applied across many domains and industries, and offer great potential to enhance visual perception. For instance, they can be used to review videos and extract insights for industrial inspection and robotics (detect faults, monitor operations, identify anomalies in real time), safety and infrastructure monitoring (recognize floods, fires, or traffic hazards), retail and logistics (track empty shelves, detect misplaced items, identify supply-chain bottlenecks), and numerous other tasks."],"metadata":{"id":"cXozhEaErru5"}},{"cell_type":"markdown","source":["### 22.3.4 VLM Finetuning <a name='22.23.4-vlm-finetuning'></a>"],"metadata":{"id":"OWKHrZNos2BH"}},{"cell_type":"markdown","source":[],"metadata":{"id":"qMwR2hiLlnWN"}},{"cell_type":"code","source":[],"metadata":{"id":"Nuc4bJf_lawe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"iviiNT2_s2Cf"}},{"cell_type":"markdown","source":["## References <a name='references'></a>\n","\n","1. Mixture of Experts Model(MOE) in AI: What is it and How does it work?, by Sahin Ahmed, available at [https://blog.gopenai.com/mixture-of-experts-model-moe-in-ai-what-is-it-and-how-does-it-work-b845ed38a3ab](https://blog.gopenai.com/mixture-of-experts-model-moe-in-ai-what-is-it-and-how-does-it-work-b845ed38a3ab).\n","2. Transformer vs. Mixture of Experts in LLMs, by Avi Chawla, avialable at [https://www.dailydoseofds.com/p/transformer-vs-mixture-of-experts-in-llms/](https://www.dailydoseofds.com/p/transformer-vs-mixture-of-experts-in-llms/).\n","3. What is Mixture of Experts?, by Dave Bergmann, available at [https://www.ibm.com/think/topics/mixture-of-experts](https://www.ibm.com/think/topics/mixture-of-experts).\n","4. Code a Simple RAG from Scratch, by Xuan-Son Nguyen, available at [https://huggingface.co/blog/ngxson/make-your-own-rag](https://huggingface.co/blog/ngxson/make-your-own-rag).\n","5. Advanced NLP Fall 2025 Code, by Sean Welleck, available at [https://github.com/cmu-l3/anlp-fall2025-code/blob/main/10_rag/rag.ipynb](https://github.com/cmu-l3/anlp-fall2025-code/blob/main/10_rag/rag.ipynb).\n","6. Building a Simple RAG Application Using LlamaIndex, by Abid Ali Awan, available at [https://machinelearningmastery.com/building-a-simple-rag-application-using-llamaindex/](https://machinelearningmastery.com/building-a-simple-rag-application-using-llamaindex/).\n","7. RAG: a Simple Practical Example Using llama index and HuggingFace, by Mayada Khatib, available at [https://medium.com/@mayadakhatib/rag-a-simple-practical-example-using-llama-index-and-huggingface-fab3e5aa7442](https://medium.com/@mayadakhatib/rag-a-simple-practical-example-using-llama-index-and-huggingface-fab3e5aa7442).\n","8. Vision-language models (VLMs) Explained (pt. 1), by Sean Trott, available at [https://seantrott.substack.com/p/vision-language-models-vlms-explained](https://seantrott.substack.com/p/vision-language-models-vlms-explained).\n","9. CLIP: Connecting text and images, OpenAI, available at [https://openai.com/index/clip/](https://openai.com/index/clip/).\n","10. Vision Language Models (VLMs) Explained - GeeksForGeeks, available at [https://www.geeksforgeeks.org/artificial-intelligence/vision-language-models-vlms-explained/](https://www.geeksforgeeks.org/artificial-intelligence/vision-language-models-vlms-explained/).\n","11. Vision Language Models Explained - Hugging Face Blog, by Merve,\n","Edward Beeching, available at [https://huggingface.co/blog/vlms](https://huggingface.co/blog/vlms).\n","12. Understanding Vision-Language Models (VLMs): A Practical Guide, by Pietro Bolcato, available at [https://medium.com/@pietrobolcato/understanding-vision-language-models-vlms-a-practical-guide-8da18e9f0e0c](https://medium.com/@pietrobolcato/understanding-vision-language-models-vlms-a-practical-guide-8da18e9f0e0c).\n","13. What Are Vision Language Models, by NVIDIA, available at [https://www.nvidia.com/en-us/glossary/vision-language-models/](https://www.nvidia.com/en-us/glossary/vision-language-models/).\n","14. Vision Language Models, by Rohit Bandaru, available at [https://rohitbandaru.github.io/blog/Vision-Language-Models/](https://rohitbandaru.github.io/blog/Vision-Language-Models/)."],"metadata":{"id":"2amIei5Ns2En"}}]}