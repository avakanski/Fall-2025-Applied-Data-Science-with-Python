{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNnfsw29tzRcYn3xvgSJlnm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Lecture 22 - Large Language Models (Part 2)"],"metadata":{"id":"xsrN8iJZlUgo"}},{"cell_type":"markdown","source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_22-LLMs_Part_2/Lecture_22-LLMs_Part_2.ipynb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_22-LLMs_Part_2/Lecture_22-LLMs_Part_2.ipynb)"],"metadata":{"id":"nT4plF3Klb8u"}},{"cell_type":"markdown","source":["<a id='top'></a>"],"metadata":{"id":"Zzs06-BflidN"}},{"cell_type":"markdown","source":["- [22.1 Mixture of Experts](#22.1-mixture-of-experts)\n","  - [22.1.1 Sparsity](#22.1.1-sparsity)\n","  - [22.1.2 MoE Advantages and Disadvantages](#22.1.2-moe-advantages-and-disadvantages)\n","- [22.2 Retrieval-Augmented Generation](#22.2-retrieval-augmented-generation)\n","  - [22.2.1 RAG Implementation - Example 1](#22.2.1-rag-implementation---example-1)\n","  - [22.2.2 RAG Implementation - Example 2](#22.1.2-rag-implementation---example-2)\n","- [22.3 Vision-Language Models](#22.3-vision-language-models)\n","  - [22.3.1 VLM Architectures](#22.3.1-vlm-architectures)\n","  - [22.3.2 Benchmarking VLMs](#22.3.2-benchmarking-vlms)\n","  - [22.3.3 VLMs Importance](#22.3.3-vlms-importance)\n","  - [22.3.4 VLM Finetuning](#22.3.4-vlm-finetuning)\n","\n","\n"],"metadata":{"id":"FNUdYxkulent"}},{"cell_type":"markdown","source":["This lecture continues our study of Large Language Models (LLMs) by examining several advanced topics. We will discuss Mixture of Experts (MoE) architectures and Retrieval-Augmented Generation (RAG). In addition, we will cover Vision-Language Models (VLMs) as an example of multi-modal models that integrate visual and textual understanding."],"metadata":{"id":"TZGTIH7_QfGx"}},{"cell_type":"markdown","source":["## 22.1 Mixture of Experts <a name='22.1-mixture-of-experts'></a>"],"metadata":{"id":"97rX1niygObX"}},{"cell_type":"markdown","source":["**Mixture of Experts (MoE)** in LLMs refers to a technique that uses a collection of sub-networks (called \"experts\") that jointly perform a task, with each expert specializing in learning different aspects of the task.\n","\n","MoE in LLMs has similarities with Ensemble Methods in ML (e.g., Random Forests, Gradient Boosting, Bagging Ensembles), where an ensemble of models contributes to a prediction. Differently from Ensemble Methods, MoE introduces dynamic routing of the input to different experts, as well as, all experts in MoE are trained as part of a single neural architecture rather than as separate learners.\n","\n","**MoE Components**\n","\n","A typical MoE architecture is shown in the figure below and consists of two key components:\n","\n","- **Experts**: each expert is typically a fully-connected neural network trained on different parts or properties of the input. Each expert specializes in processing different types of inputs, such as token types, contexts, or semantic patterns.\n","\n","- **Router or gating network**: analyzes the inputs and determines which tokens are sent to which experts. It acts as a coordinator that routes the inputs to the selected experts.\n","\n","When MoE receives input data, it first passes the data through the gating network. The gating network analyzes the input and selects the experts to receive the data. Typically, only a small subset of experts is activated for each input. The selected experts then process the data and generate outputs, which are combined to produce the final output of the MoE.\n","\n","<img src=\"images/moe_components.png\" width=\"450\">\n","\n","*Figure: Main components of MoE.* Source: [1].\n","\n","\n"],"metadata":{"id":"_YRMf0vD5LkM"}},{"cell_type":"markdown","source":["In most MoE LLMs, the dense Feed-Forward Network (FFN) of Transformers is replaced with a set of experts. The **experts** are also feed-forward networks, but they are smaller compared to the FFN in Transformers. In general, experts can also be more complex than FFNs, and an expert can even be an MoE itself (known as hierarchical MoE).\n","\n","<img src=\"images/transformer_vs_moe.gif\" width=\"550\">\n","\n","*Figure: Transformer vs MoE.* Source: [2].\n","\n","\n","Because LLMs contain many decoder blocks in sequence, input tokens may pass through different experts across the decoder blocks, which results in diverse specialization patterns.\n","\n","<img src=\"images/decoder_layers.gif\" width=\"400\">\n","\n","*Figure: Experts in decoder layers.* Source: [2].\n","\n"],"metadata":{"id":"GDlnKXXA5LmI"}},{"cell_type":"markdown","source":["The **gating network (router)** is also a small FFN that acts like a multi-class classifier and outputs softmax scores over the experts. I.e., the router assigns a probability value to each expert, and determines which experts are activated for a given token.\n","\n","For instance, in the figure, the router assigned the highest probability value to Expert-1, and selected it for routing the input data to that expert.\n","\n","<img src=\"images/router.gif\" width=\"450\">\n","\n","*Figure: Gating network (router).* Source: [1].\n","\n","\n","\n","Training an MoE model involves updating both the experts and the gating network, which makes it more challenging than training traditional Transformer models. Specifically, the training can result in a poor routing strategy where the gating network activates only a small number of experts. Such imbalance means that the selected few experts can become overly specialized, while other experts can remain undertrained and underutilized. This imbalance reduces the performance of the entire model.\n","\n","Consequently, *load balancing* by the router is critical for MoE performance. Several different strategies have been proposed that penalize overreliance on any one expert, and reward more even utilization of all experts.  \n","\n","In MoE models, each expert learns a different specialization of the overall function. However, the experts do not learn completely separate tasks. Instead, they learn to handle different types of tokens, contexts, or transformations. For example, one expert may learn code-style token patterns, another may learn transformations helpful for math reasoning, then another may specialize in rare domain-specific words, and similar.\n","\n"],"metadata":{"id":"jIx00_2v5Lrs"}},{"cell_type":"markdown","source":["### 22.1.1 Sparsity<a name='22.1.1-sparsity'></a>\n"],"metadata":{"id":"sCD1F1XnuUyf"}},{"cell_type":"markdown","source":["The main concept in MoE architectures is the presence of **sparse layers**, since only some of the experts (and therefore their neurons) are activated for each token. In the original **dense layers** in FFN of Transformers, all neurons are connected to every neuron in the preceding and following layers. In sparse layers, neurons are connected only to some of the neurons in the neighboring layers.\n","\n","<img src=\"images/dense_sparse.png\" width=\"450\">\n","\n","*Figure: Dense versus sparse networks.* Source: [3].\n","\n","For example, Mixtral is a well-known MoE LLM that uses 8 experts, each containing approximately 7B parameters. Therefore, the FFN in Mixtral has 8 expert blocks. For every token, the gating network selects two of the eight experts to process the data. Hence, about 14B parameters (two experts) are active during training and inference. Note however, that the overall number of parameters in Mixtral is not exactly 8 x 7B = 56B, since some layers are shared. The total parameter count is around 47B.\n","\n","The total number of parameters is generally considered a measure of the **model capacity**. The number of parameters that are used to process an individual token is a measure of the model's **computational cost**. Thus, Mixtral has a similar learning capacity to dense LLMs with 47B parameters, but has a much lower computational cost because only 14B parameters are used per token.\n","\n","Note also that although 14B parameters are active per token, the entire Mixtral model with 47B parameters needs to be loaded into memory to perform inference. In other words, computational efficiency does not reduce the requirement for sufficient RAM/VRAM to load the model.\n","\n","Other well-known MoE LLMs include Mixtral 8 x 22B, DeepSeek-V3 (671B parameters, with 37B active per token), Qwen3-235B (128 experts in total, 8 experts with 22B parameters active per token), Qwen1.5 (64 experts in total, 4 experts with 2.7B parameters active per token), etc. It is likely that the premier LLMs including GPT-5 and Claude 4 use some form of MoE and combine sparse and dense layers, although it is not known for sure.\n","\n"],"metadata":{"id":"pgir5QDZ5LuI"}},{"cell_type":"markdown","source":["### 22.1.2 MoE Advantages and Disadvantages<a name='22.1.2-moe-advantages-and-disadvantages'></a>"],"metadata":{"id":"qJmZXkiMtzhm"}},{"cell_type":"markdown","source":["MoE models offer several important advantages in LLMs, as follows.\n","\n","- Sparsity: Instead of activating the entire network for each input token, MoE activates only a small subset of parameters, and much of the model stays inactive for a given token.\n","- Efficient inference: Because only one or a few experts are used per token, fewer computations are required. This lowers inference-time compute and required memory.\n","- Faster pretraining: MoE LLMs are significantly faster to pretrain compared to dense models with the same total number of parameters, because each token uses only a fraction of the parameters.\n","- Higher quality outputs: Expert specialization improves accuracy and overall model performance.\n","\n","MoE disadvantages include:\n","\n","- Training instability: Sparse routing can lead to imbalanced expert usage, where a few experts get most of the traffic while others remain underutilized.\n","- Fine-tuning difficulty: Fine-tuning MoE models is more challenging due to the presence of sparse layers.\n","- Hardware complexity: MoE requires careful management of memory and compute, especially when experts are distributed across devices.\n","- Overfitting: Sparse models are more prone to overfitting than dense models if load balancing is insufficient.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"a6XQTlk0giXy"}},{"cell_type":"markdown","source":["## 22.2 Retrieval Augmented Generation <a name='22.2-retrieval-augmented-generation'></a>"],"metadata":{"id":"1EJbYNdSgOdu"}},{"cell_type":"markdown","source":["**Retrieval Augmented Generation (RAG)** refers to the use of external sources of information for improving the generated responses by LLMs. RAG allows LLMs to retrieve facts from external sources (e.g., Wikipedia, news articles, local documents) and provide responses that are more accurate and/or are up-to-date.\n","\n","In general, the internal knowledge of LLMs is static, as it is fixed by the date of the used training dataset. Therefore, LLMs cannot answer questions about current events, and they are stuck in the time moment of their training data. Updating LLMs with knowledge about current events requires to continuously retrain the models on new data. Such a process is very expensive, as it requires collecting updated datasets and finetuning the model to update the weights.\n","\n","RAG enables to avoid expensive LLMs retraining, by retrieving information from external databases to generate responses. The RAG approach involves two major phases:\n","\n","- **Retrieval**, includes performing relevancy search of external databases regarding a user query, and retrieving supporting documents and snippets of important information.\n","- **Content generation**, the retrieved supporting documents are used as a context that is appended to the user query, and are fed to the LLMs for generating the final response.\n","\n","Instead of relying only on the information contained in the training dataset used for training an LLM, RAG provides an interface to external knowledge to ensure that the model has access to the most current and reliable facts. E.g., in an enterprise setting, external sources of information for RAG can comprise various company-specific files, documents, and databases. Employing RAG can result in more relevant responses and it can reduce the problem of hallucination by LLMs. It also allows the users to review the sources that were used by LLMs and verify the accuracy of generated responses. However, RAG systems have also limitations, which include potential retrieval of irrelevant or outdated documents, increased response latency due to the retrieval step, and dependency on the quality and coverage of the external knowledge sources.\n","\n","An overview of a RAG system is depicted in the figure. The provided examples that follow demonstrate two different ways to implement RAG systems.\n","\n","<img src=\"images/RAG.png\" width=\"450\">\n","\n","*Figure: RAG system.* Source: [4]."],"metadata":{"id":"pH8GPWtabYsO"}},{"cell_type":"markdown","source":["### 22.2.1 RAG Implementation - Example 1<a name='22.2.1-rag-implementation---example-1'></a>"],"metadata":{"id":"--uRUt8Bca3B"}},{"cell_type":"markdown","source":["In this example, we will implement a RAG system in Python using the Hugging Face library for model loading and response generation.\n","\n","As an external text source, we will use a Wikipedia article on LLMs available at https://en.wikipedia.org/wiki/Large_language_model\n",". The article has been pre-processed by extracting the main text content, removing sections with equations, and removing references. The cleaned text is saved in a plain text file named `wikipedia_llm.txt`.\n","\n","We will build a RAG system that allows us to answer queries about the article."],"metadata":{"id":"a1CZz_Toy3wR"}},{"cell_type":"markdown","source":["#### Data Loading\n","\n","First, we mount Google Drive and import the required libraries. Most are standard, except for `sentence_transformers` that is a Hugging Face library that is used to load an embedding model, and facilitate creating vector embeddings for sentences, paragraphs, or documents.\n","\n","Next, we load the document `wikipedia_llm.txt`, which is uploaded in the course folder for Lecture 22 on my Google Drive."],"metadata":{"id":"avG3dlS3XsUy"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omwbLwNOYRla","executionInfo":{"status":"ok","timestamp":1763324420467,"user_tz":420,"elapsed":478,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f25f01e6-4006-4ae8-c668-3b5c122d6f32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Import libraries\n","import torch\n","import numpy as np\n","import pandas as pd\n","import re\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from sentence_transformers import SentenceTransformer"],"metadata":{"id":"uS-83NzuXYLi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Path to the file uploaded on Google Drive\n","file_path = 'drive/MyDrive/Data_Science_Course/Fall_2025/Lectures/Lecture_22-LLMs_Part_2/data/wikipedia_llm.txt'\n","\n","# Load the text file\n","with open(file_path , 'r') as f:\n","    text = f.read()"],"metadata":{"id":"DiNCH4rdXYSA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the text content of the article\n","text[:1000]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"H-T_SXM2YHI1","executionInfo":{"status":"ok","timestamp":1763324432957,"user_tz":420,"elapsed":3,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"36172970-21be-4c1c-8498-b625bb0cebf8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'=== Introduction ===\\nA large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. The largest and most capable LLMs are generative pre-trained transformers (GPTs) and provide the core capabilities of chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\\nThey consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational ag'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["#### Split Text into Chunks\n","\n","To prepare the document for embedding, we need to split the text into smaller chunks. RAG systems typically work with text chunks because source documents may be too long and exceed the LLM context window. Also, smaller chunks lead to more accurate embeddings, and provide more precise context for query answering.\n","\n","There are several different ways of creating chunks. A common approach is to split text based on token count, where for instance, every chunk can contain 300 tokens. Since this Wikipedia article is already divided into sub-sections, we will use one sub-section per chunk.\n","\n","The following function `chunk_by_section` does exactly that, using the `==` text markers to identify top-level headers and split the text into chunks."],"metadata":{"id":"q8CPVZwGvrfm"}},{"cell_type":"code","source":["# Function to split text into chunks (each sub-section is one chunk)\n","def chunk_by_section(wiki_text):\n","\n","    # Find all top-level headers and their content\n","    pattern = re.compile(r'(==\\s*[^=].*?==)\\s*(.*?)(?=(?:==\\s*[^=].*?==)|\\Z)', re.S | re.M)\n","    matches = pattern.findall(wiki_text)\n","\n","    chunks = []\n","    pending_headers = []\n","    for header, content in matches:\n","        if content.strip():\n","            # If there are header-only sections, prepend them to non-empty sections\n","            if pending_headers:\n","                combined_header = \"\\n\".join(pending_headers + [header])\n","                chunk = (combined_header + content).strip()\n","                pending_headers = []\n","            else:\n","                chunk = (header + content).strip()\n","            chunks.append(chunk)\n","        else:\n","            pending_headers.append(header)\n","\n","    return chunks"],"metadata":{"id":"zJrWZL3CX_g4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call the function to create chunks\n","chunks = chunk_by_section(text)"],"metadata":{"id":"7r__3q8EWMd-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that there are 53 text chunks in total. The first 5 chunks are displayed in the pandas DataFrame below."],"metadata":{"id":"7KVQIGZu2AjV"}},{"cell_type":"code","source":["# Print the number of chunks in the article\n","print(f\"Created {len(chunks)} chunks\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yEMKZzvWMoI","executionInfo":{"status":"ok","timestamp":1763324432977,"user_tz":420,"elapsed":12,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"0a2bbf36-39e6-4d6a-9021-8019126d93ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Created 53 chunks\n"]}]},{"cell_type":"code","source":["# Create a pandas dataframe using the chunks\n","df = pd.DataFrame({\"chunk_id\": range(len(chunks)), \"text\": chunks})\n","\n","# Show the first 5 chunks\n","df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"oLaSkE0jZxf1","executionInfo":{"status":"ok","timestamp":1763324432981,"user_tz":420,"elapsed":3,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"83f4c985-1f28-4776-bc5c-42fbefee55ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   chunk_id                                               text\n","0         0  == Introduction ===\\nA large language model (L...\n","1         1  == History ===\\nBefore the emergence of transf...\n","2         2  == Tokenization ===\\nAs machine learning algor...\n","3         3  == Byte-pair encoding ===\\nAs an example, cons...\n","4         4  == Tokenization: Problems ===\\nA token vocabul..."],"text/html":["\n","  <div id=\"df-d15ede00-b5a0-4302-8db0-e2696627d7e9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chunk_id</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>== Introduction ===\\nA large language model (L...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>== History ===\\nBefore the emergence of transf...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>== Tokenization ===\\nAs machine learning algor...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>== Byte-pair encoding ===\\nAs an example, cons...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>== Tokenization: Problems ===\\nA token vocabul...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d15ede00-b5a0-4302-8db0-e2696627d7e9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d15ede00-b5a0-4302-8db0-e2696627d7e9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d15ede00-b5a0-4302-8db0-e2696627d7e9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-e49a532f-3cbc-4f48-ba8e-6182873f4c2e\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e49a532f-3cbc-4f48-ba8e-6182873f4c2e')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-e49a532f-3cbc-4f48-ba8e-6182873f4c2e button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 53,\n  \"fields\": [\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 0,\n        \"max\": 52,\n        \"num_unique_values\": 53,\n        \"samples\": [\n          19,\n          41,\n          47\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          \"== Tool use ===\\nTool use is a mechanism that enables LLMs to interact with external systems, applications, or data sources. It can allow for example to fetch real-time information from an API or to execute code. A program separate from the LLM watches the output stream of the LLM for a special tool-calling syntax. When these special tokens appear, the program calls the tool accordingly and feeds its output back into the LLM's input stream.\\nEarly tool-using LLMs were fine-tuned on the use of specific tools. But fine-tuning LLMs for the ability to read API documentation and call API correctly has greatly expanded the range of tools accessible to an LLM. Describing available tools in the system prompt can also make an LLM able to use tools. A system prompt instructing ChatGPT (GPT-4) to use multiple types of tools can be found online.\\n\\n\\n=\",\n          \"== Limitations and challenges: Political bias ===\\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\\n\\n\\n=\",\n          \"== Sleeper agents ===\\nResearchers from Anthropic found that it was possible to create \\\"sleeper agents\\\", models with hidden functionalities that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions. For example, a LLM could produce safe code except on a specific date, or if the prompt contains a specific tag. These functionalities were found to be difficult to detect or remove via safety training.\\n\\n\\n=\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["#### Embedding\n","\n","To perform RAG, we first need to create embeddings for all text chunks.  In this example, we use the `Qwen3-Embedding-0.6B` model. Each text chunk is passed through this embedding model, and an embedding vector was calculated for each of the 53 chunks. As we can notice, each embedding vector has 1,024 elements."],"metadata":{"id":"GUBs8d-yZg9t"}},{"cell_type":"code","source":["# Load the embedding model\n","embed_model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B')"],"metadata":{"id":"Co8svRxvgjbM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create embeddings for all text chunks\n","chunk_embeddings = embed_model.encode(chunks, prompt_name=\"query\")"],"metadata":{"id":"D4FTvoTLZeOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the shape of chunk embeddings\n","print(f\"Shape of created embeddings: {chunk_embeddings.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jcfc4srljXSU","executionInfo":{"status":"ok","timestamp":1763324450793,"user_tz":420,"elapsed":13,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"870a1b8f-2e98-4dcd-e678-c5ea46ce3486"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of created embeddings: (53, 1024)\n"]}]},{"cell_type":"markdown","source":["#### Retrieval\n","\n","The next step involves finding the most relevant chunks for a given query. We first obtain an embedding vector for the given query using the same model `Qwen3-Embedding-0.6B`. This results in an 1,024 dimensional embedding vector."],"metadata":{"id":"jP616yeeZ6AG"}},{"cell_type":"code","source":["# A test query\n","query = \"What is a mixture of experts\""],"metadata":{"id":"3mL7dHU-eKL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a vector embedding for the query\n","query_embedding = embed_model.encode([query], prompt_name=\"query\")[0]"],"metadata":{"id":"lgiPlR9yjpN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the shape of the embedding\n","query_embedding.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"thGzHLBujpP9","executionInfo":{"status":"ok","timestamp":1763324450848,"user_tz":420,"elapsed":4,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7ce3174d-bfcd-434d-f2b5-7acbf6284602"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1024,)"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["The next cell calculates cosine similarity between the query embedding and the embedding for each chunk. As you may recall, cosine similarity for two vectors $u$ and $v$ is calculated as $\\dfrac{u\\cdot v}{||u||\\cdot ||v||}$."],"metadata":{"id":"fHB8Hceh5FZN"}},{"cell_type":"code","source":["# Calculate similarity between the query embedding and each chunk embedding\n","# cosine similarity = dot_product(u,v) / norm(u)*norm(v)\n","similarities = np.dot(chunk_embeddings, query_embedding)\n","similarities = similarities / (np.linalg.norm(chunk_embeddings, axis=1) * np.linalg.norm(query_embedding))"],"metadata":{"id":"6PGogFi3jpST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the shape of similarities\n","similarities.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YQfV4yajz9U","executionInfo":{"status":"ok","timestamp":1763324450886,"user_tz":420,"elapsed":22,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"83af076d-09c5-4f12-f2e0-53281b29b642"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(53,)"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["\n","There are 53 similarity scores since there are 53 text chunks. By sorting the similarity scores, we found that the embeddings for chunks 12, 45, and 15, are the closest to the embedding vector for the query."],"metadata":{"id":"Rnwh3zzr58j1"}},{"cell_type":"code","source":["# Find the indices for the top 3 most similar chunks\n","top_indices = np.argsort(similarities)[-3:][::-1]\n","\n","top_indices"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ohsM66IJj0B3","executionInfo":{"status":"ok","timestamp":1763324450890,"user_tz":420,"elapsed":5,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"b3630cf0-f61a-4332-e7d0-a11cb0a054b2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([12, 45, 15])"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["In the next cell, the top 3 text chunks are printed. We  see that the first chunk has a similarity score of 0.740 and it is the most relevant to the question. Notice that the sub-section header is \"Mixture of experts\" and it directly relates to the posed question \"What is a mixture of experts\". The second and third chunks have much lower similarity scores (0.248 and 0.247) and are less relevant."],"metadata":{"id":"mix6amiU6PAp"}},{"cell_type":"code","source":["print(f\"Results\\n\")\n","for i in top_indices:\n","    print(f\"Similarity score: {similarities[i]:.3f}:\\nText Chunk: {chunks[i][:1000]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEt44xqAscB5","executionInfo":{"status":"ok","timestamp":1763324450897,"user_tz":420,"elapsed":6,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"3901faa7-2c80-4325-a22a-fc7dbe29c809"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Results\n","\n","Similarity score: 0.740:\n","Text Chunk: == Mixture of experts ===\n","A mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\n","\n","\n","=\n","Similarity score: 0.248:\n","Text Chunk: == Sycophancy and glazing ===\n","Sycophancy is a model's tendency to agree with, flatter, or validate a user's stated beliefs rather than to prioritize factuality or corrective information, and \"glazing\" is an emergent public shorthand for persistent, excessive agreeability observed across multi-turn interactions and productized assistants.\n","Continued sycophancy has led to the observation of getting \"1-shotted\", denoting instances where conversational interaction with a large language model produces a lasting change in a user's beliefs or decisions, similar to the negative effects of psychedelics, and controlled experiments show that short LLM dialogues can generate measurable opinion and confidence shifts comparable to human interlocutors.\n","Empirical analyses attribute part of the effect to human preference signals and preference models that reward convincingly written agreeable responses, and subsequent work has extended evaluation to multi-turn benchmarks and proposed interventions such \n","Similarity score: 0.247:\n","Text Chunk: == Extensibility ===\n","Beyond basic text generation, various techniques have been developed to extend LLM capabilities, including the use of external tools and data sources, improved reasoning on complex problems, and enhanced instruction-following or autonomy through prompting methods.\n","\n","\n","=\n"]}]},{"cell_type":"markdown","source":["#### Generation\n","\n","The final step in RAG uses the results from the Retrieval phase, and performs response generation. In this example, we use `Qwen3-0.6` LLM for response generation. Note that this model is different from `Qwen3-Embedding-0.6B` that we used for chunk embedding. Also note than the model is loaded with `AutoModelForCausalLM` since the task to perfom is casual language modeling. Loaded with the model is the corresponding tokenizer as well."],"metadata":{"id":"PQKP9vAlaN2u"}},{"cell_type":"code","source":["# Load Qwen3 language model and tokenizer\n","lm_model_name = \"Qwen/Qwen3-0.6B\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(lm_model_name)\n","\n","lm_model = AutoModelForCausalLM.from_pretrained(lm_model_name, pad_token_id=tokenizer.eos_token_id)"],"metadata":{"id":"gniZbsAfVmIm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's use only the top chunk to generate an answer, since it was the most relevant for the query. We assign it to the name `context`.\n","\n","Afterward, we create a message for chat template that contains \"system\" and \"user\" roles. The system message below is quite general. The user message prepends the retrieved context (i.e., the most relevant text chunk) to the query. This ensures that LLM has access to the text chunk and use the information when generating the answer to the query.  "],"metadata":{"id":"Rpq_gqGJ786B"}},{"cell_type":"code","source":["context = chunks[top_indices[0]]\n","context"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"jcTe4G3tmMtk","executionInfo":{"status":"ok","timestamp":1763324452488,"user_tz":420,"elapsed":48,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f6cc3b00-18c8-43d2-9d65-7a957800d2be"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'== Mixture of experts ===\\nA mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\\n\\n\\n='"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["# Create messages for chat template\n","messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant answering questions.\"},\n","            {\"role\": \"user\", \"content\": f\"\"\"Use this context: {context} to answer the following question: {query}\"\"\"}]"],"metadata":{"id":"QATPJ6aLkfF8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next cell applies the chat template for `Qwen3-0.6B` to the messages. Notice that the formatted prompt has the tags for instruction message start and end `<|im_start|>` and `<|im_end|>` added to the text, to match the expected format for `Qwen3-0.6B`.\n","\n","Shown below also is the tokenized prompt with `input_ids` and `attention_mask` keys and corresponding values."],"metadata":{"id":"0Kfv7K8f9K8x"}},{"cell_type":"code","source":["# Apply chat template to build the prompt\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","# Diplay formatted prompt\n","prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"TA25SG9EkfIn","executionInfo":{"status":"ok","timestamp":1763324452492,"user_tz":420,"elapsed":2,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"cc4e0a53-1fae-42d1-8396-748de8afe64c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|im_start|>system\\nYou are a helpful assistant answering questions.<|im_end|>\\n<|im_start|>user\\nUse this context: == Mixture of experts ===\\nA mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\\n\\n\\n= to answer the following question: What is a mixture of experts<|im_end|>\\n<|im_start|>assistant\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# Tokenize the prompt\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(lm_model.device)\n","\n","inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEax4Himuu2K","executionInfo":{"status":"ok","timestamp":1763324452536,"user_tz":420,"elapsed":29,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"9b26778f-e9f2-4cf9-b69e-9ca225092322"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,  35764,\n","           4755,     13, 151645,    198, 151644,    872,    198,  10253,    419,\n","           2266,     25,    621,    386,  12735,    315,  11647,   2049,    198,\n","             32,  20980,    315,  11647,    320,  25612,     36,      8,    374,\n","            264,   5662,   6832,  17646,    304,    892,   5248,  27076,  29728,\n","          14155,   3489,   4580,  15546,    899,    975,   3786,     11,    448,\n","            264,  73399,  16953,    429,  11291,   1817,   1946,    311,    279,\n","           1429,   8311,   6203,   1141,    568,  19219,  18513,    315,  11647,\n","            646,   7949,  44378,   7049,     11,    438,   1172,    264,  19419,\n","            315,    279,   5029,    525,   1483,    369,   1817,   1946,     13,\n","            576,   5486,    572,  11523,    304,    220,     17,     15,     16,\n","             22,    553,   5085,  11811,   4192,     28,    311,   4226,    279,\n","           2701,   3405,     25,   3555,    374,    264,  20980,    315,  11647,\n","         151645,    198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1]])}"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["The tokenized prompt `inputs` is then passed to the model using `model.generate` to produce a response."],"metadata":{"id":"L8GrvZgB-BW1"}},{"cell_type":"code","source":["# Generate response by the model\n","outputs = lm_model.generate(**inputs, max_new_tokens=500, temperature=0.1, top_p=0.95, do_sample=True, pad_token_id=tokenizer.eos_token_id)"],"metadata":{"id":"2ML0do9_ma7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decode generated tokens to text\n","response = tokenizer.decode(outputs[0], skip_special_tokens=True)"],"metadata":{"id":"Xi47U78vma96"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The entire LLM response is shown below. It contains the system message, the user prompt with the context and query, and the model's generated answer."],"metadata":{"id":"zxX1rjsb-Qtk"}},{"cell_type":"code","source":["response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"JZtxp-7Em6SH","executionInfo":{"status":"ok","timestamp":1763324492177,"user_tz":420,"elapsed":2,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"dc41d62c-d9bd-4d00-c3cd-b5bb99f26c47"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'system\\nYou are a helpful assistant answering questions.\\nuser\\nUse this context: == Mixture of experts ===\\nA mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\\n\\n\\n= to answer the following question: What is a mixture of experts\\nassistant\\n<think>\\nOkay, so the user is asking, \"What is a mixture of experts?\" and provided some context. Let me start by reading through the context again to make sure I understand it correctly.\\n\\nThe context says: \"A mixture of experts (MoE) is a machine learning architecture in which multiple specialized neural networks (\\'experts\\') work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.\"\\n\\nSo, the main points here are that MoE involves multiple experts working together, each handling a part of the input, using a gating mechanism to decide which expert to use, reducing parameters, and introduced by Google in 2017.\\n\\nThe user wants to know what a mixture of experts is. The answer should be concise, using the information from the context. I need to make sure to mention the key components: multiple experts, gating mechanism, reducing parameters, and the introduction by Google in 2017. Also, check if there\\'s any other details I might have missed, but from the context, that\\'s all. So the answer should be a clear definition based on the given information.\\n</think>\\n\\nA mixture of experts (MoE) is a machine learning architecture where multiple specialized neural networks (experts) work together, with a gating mechanism that selects the most appropriate expert for each input. This approach reduces inference costs by using only a fraction of the parameters for each input, as introduced by Google researchers in 2017.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["Let's extract only the text generated by the LLM. It is shown below, and it correctly answers the posed question \"What is a mixture of experts\"."],"metadata":{"id":"mXzM-bqt-fJ8"}},{"cell_type":"code","source":["# Extract the LLM answer only\n","answer = response.split(\"<|im_start|>assistant\")[-1].split(\"</think>\")[-1].strip()\n","answer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"Wdsedv_Ym6Uo","executionInfo":{"status":"ok","timestamp":1763324492182,"user_tz":420,"elapsed":3,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"1be5f136-c8c2-4b29-8402-2242819b5563"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'A mixture of experts (MoE) is a machine learning architecture where multiple specialized neural networks (experts) work together, with a gating mechanism that selects the most appropriate expert for each input. This approach reduces inference costs by using only a fraction of the parameters for each input, as introduced by Google researchers in 2017.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["### 22.2.2 RAG Implementation - Example 2<a name='22.2.2-rag-implementation---example-2'></a>"],"metadata":{"id":"tvO_tqRtcrZp"}},{"cell_type":"markdown","source":["This examples uses **LlamaIndex**, which is an advanced framework that facilitates LLMs integration with diverse data sources. LlamaIndex supports knowledge  retrieval from over 100 data formats, including PDFs, websites, Word documents, SQL databases, and much more. Besides tools for data retrieval, LlamaIndex offers built-in functions that make RAG implementation quite simple."],"metadata":{"id":"2HhDlaavWfUQ"}},{"cell_type":"markdown","source":["#### Data Loading\n","\n","For this example, we will use a single MS Word file as the external data source. The file is in fact the syllabus for the CS 4622/5622 Applied Data Science with Python course.\n","\n","Let's first install the required packages for `llama_index` and import the required modules."],"metadata":{"id":"-Hy-LFT0Wfav"}},{"cell_type":"code","source":["# Note that this line requires to Restart the kernel (Runtime > Restart Session) and run it one more time\n","\n","!pip install -q llama_index llama_index_core llama-index-embeddings-huggingface docx2txt"],"metadata":{"id":"NFgFj3VVc9k1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n","from llama_index.core.retrievers import VectorIndexRetriever\n","from llama_index.core.query_engine import RetrieverQueryEngine\n","from llama_index.core.postprocessor import SimilarityPostprocessor\n","from llama_index.core.node_parser import SimpleNodeParser\n","from transformers import AutoModelForCausalLM, AutoTokenizer"],"metadata":{"id":"gN5JbwakcvlU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's mount the drive and enter the path to the directory where the syllabus document is stored. LlamaIndex offers `SimpleDirectoryReader` that loads and parses documents in a directory. Specifically, it finds all files in the directory, loads each file as a separate Document object, and cleans the text. In this case, we have only one document (the course syllabus) in the directory, but in a real-world case, many documents are used for building RAG applications."],"metadata":{"id":"zgLDvM4bndde"}},{"cell_type":"code","source":["# Mount Google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vsKTavRFeMpg","executionInfo":{"status":"ok","timestamp":1763324498254,"user_tz":420,"elapsed":464,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"bd99c312-5925-4d14-ccff-d9dbe4cd56e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Path to the Google Drive directory where the syllabus is uploaded\n","file_path = 'drive/MyDrive/Data_Science_Course/Fall_2025/Lectures/Lecture_22-LLMs_Part_2/data/data_example2'\n","\n","# Load and parse documents in the directory\n","documents = SimpleDirectoryReader(file_path).load_data()\n","\n","print(len(documents))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RyoHVj2hcvpe","executionInfo":{"status":"ok","timestamp":1763324498284,"user_tz":420,"elapsed":28,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"d32c31cf-129f-401f-d67b-366559195512"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}]},{"cell_type":"markdown","source":["#### Split Text into Chunks\n","\n","The function `SimpleNodeParser` in LlamaIndex splits the text into chunks, referred to here as nodes. Notice that we set the chunks to have approximately 180 tokens, and also there is an overlap of 20 tokens. I.e., the second chunk contains the last 20 tokens from the first chunk. The overlapped tokens help to capture continuity so that the information retrieval is not broken at the chunk boundary. Since this is a short document, there are only 7 chunks, and the first text chunk is shown below."],"metadata":{"id":"OX6UK7mYWqU7"}},{"cell_type":"code","source":["# Create parser to split the document into text chunks (nodes)\n","parser = SimpleNodeParser.from_defaults(chunk_size=180, chunk_overlap=20)\n","\n","# Apply the parser to convert the loaded documents to chunks\n","nodes = parser.get_nodes_from_documents(documents)\n","\n","# Print the number of chunks\n","print(\"Number of chunks:\", len(nodes))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZ69On-T_rec","executionInfo":{"status":"ok","timestamp":1763324499208,"user_tz":420,"elapsed":901,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"1de0a8df-93c3-4bad-9c7a-742c77fd7b44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of chunks: 7\n"]}]},{"cell_type":"code","source":["print(nodes[0].text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSdL-gfVW9vg","executionInfo":{"status":"ok","timestamp":1763324499214,"user_tz":420,"elapsed":5,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6d4631c6-8ab1-4bbe-c80b-66a16e6ae931"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CS 4622/5622  Applied Data Science with Python\n","\n","\n","\n","\t\tSemester: Fall 2025 (August 25  December 19)\n","\n","\t\tCredit Hours: 3\n","\n","\t\tInstructor: Alex Vakanski, vakanski@uidaho.edu \n","\n","\t\tOffice Location: TAB 311, Idaho Falls Center (zoom link available on Canvas)\n","\n","\t\tCourse Delivery Methods: \n","\n","\t\tVirtual meetings (live meetings, students participate through Zoom)\n","\n","\t\tClassroom (live meetings,\n"]}]},{"cell_type":"markdown","source":["#### Embedding\n","\n","The next cell defines to use `bge-small-en-v1.5` as a model for embedding the text chunks. The model is loaded from HuggingFace.\n","\n","The cell afterward builds a vector store from the text chunks (nodes). This involves computing an embedding vector for each text chunk using the BGE model, storing all vectors into a vector database, and assigning a retrieval index to the embeddings.\n","\n","In the examples covered in this lecture we work with only one document for demonstration purposes. The two presented examples contained 7 and 53 text chunks, and we can easily store them as list in the memory. However, in real-world RAG applications with many documents, there are large number of text chunks. There may be thousands or millions of vector embeddings, and storing and searching is more challenging.\n","\n","In such cases, vector embeddings are typically stored into a **vector store** (or **vector database**). It is a database designed to store and search vector embeddings that represent text, images, audio, or other data. You can think of a vector database in simple terms as an SQL database that instead of data contains hihg-dimensional vector embeddings. It allows to perform fast similarity search over embeddings, and returns the closest matching vectors for a query. There are many vector databases available, ranging from free open-source to commercial products, offering local and cloud-based solutions.\n","\n"],"metadata":{"id":"eBgYx6ToWyXr"}},{"cell_type":"code","source":["# Load the embedding  model\n","Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n","# Don't use a default LLM yet, we will define it later\n","Settings.llm = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nacskp15DfoE","executionInfo":{"status":"ok","timestamp":1763324501226,"user_tz":420,"elapsed":2003,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"14ffea56-72f7-47fd-a7eb-891bee52dc44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM is explicitly disabled. Using MockLLM.\n"]}]},{"cell_type":"code","source":["# Index documents using vector store\n","index = VectorStoreIndex(nodes)"],"metadata":{"id":"C5M4zwxKDQcU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Retrieval\n","\n","LlamaIndex offers `VectorIndexRetriever` which, as it sounds, creates a retriever object that uses the vector index to search for relevant chunks and retrieves the top-1 chunk based on the setting for `similarity_top_k` in this case.\n","\n","Next, a query engine is defined with `RetrieverQueryEngine`, that uses the retriever object to return the relevant text for the query."],"metadata":{"id":"W858R4Y2XoeL"}},{"cell_type":"code","source":["# Configure retriever\n","retriever = VectorIndexRetriever(index=index, similarity_top_k=1)"],"metadata":{"id":"5EtUbPrAcvrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a query engine\n","query_engine = RetrieverQueryEngine(retriever=retriever)"],"metadata":{"id":"w6jgCYM0cvtC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Generation\n","\n","For content generation, we will again use `Qwen3-0.6B` LLM, as in the first example.\n","\n","The next cell below defines a generation pipeline with the `generate_respose` function. The steps are almost identical to Example 1 above, with the exception of the first two lines that call the defined `query engine` and extract the text for the most relevant text chunk."],"metadata":{"id":"Sy3YExwuXxiT"}},{"cell_type":"code","source":["model_name = \"Qwen/Qwen3-0.6B\"\n","\n","# Load model\n","model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=False, device_map=\"cuda:0\")\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"],"metadata":{"id":"wEHHJIbzgPno"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_response(query):\n","\n","    # Retrieve the relevant text\n","    retrieval = query_engine.query(query)\n","\n","    # Extract the text for the most relevant node\n","    context = retrieval.source_nodes[0].text\n","\n","    # Create messages for chat template\n","    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant answering questions. Use the provided context to answer questions.\"},\n","            {\"role\": \"user\", \"content\": f\"\"\"Based on this context: {context}, please answer the following question: {query}\"\"\"}]\n","\n","    # Apply chat template to build the prompt\n","    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","    # Tokenize the prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    # Generate response by the model\n","    outputs = model.generate(**inputs, max_new_tokens=500, temperature=0.1, top_p=0.95, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n","\n","    # Decode generated tokens to text\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Extract the LLM answer only\n","    answer = response.split(\"<|im_start|>assistant\")[-1].split(\"</think>\")[-1].strip()\n","\n","    # Print the final answer by the LLM\n","    print(answer)"],"metadata":{"id":"tdMm6Z7sX4eO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can call the `generate_response` function and query the model based on course information provided in the syllabus."],"metadata":{"id":"jaxFqRW53XRw"}},{"cell_type":"code","source":["generate_response(\"What is the evaluation strategy\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0AcSRP7pZJPz","executionInfo":{"status":"ok","timestamp":1763324517491,"user_tz":420,"elapsed":14411,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"26a19155-633a-44d2-a3be-f196e7d7624a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The evaluation strategy combines assignments, quizzes, and class participation to assess student performance. Each component contributes to a total of 100, indicating a structured approach to evaluating learning outcomes.\n"]}]},{"cell_type":"code","source":["generate_response(\"How many marks are assigned for homework assignments\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCfBJaiwaJaw","executionInfo":{"status":"ok","timestamp":1763324525863,"user_tz":420,"elapsed":8356,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"e0b2331d-fd49-42c1-936b-3b4e37f7c393"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of marks assigned for homework assignments is **45**.\n"]}]},{"cell_type":"code","source":["generate_response(\"What is the instructor's email address\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"II9wu3fPbmH0","executionInfo":{"status":"ok","timestamp":1763324532595,"user_tz":420,"elapsed":6731,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a82c189c-0bd4-448f-d933-e639ccd0295f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The instructor's email address is **vakanski@uidaho.edu**.\n"]}]},{"cell_type":"markdown","source":["## 22.3 Vision-Language Models <a name='22.3-vision-language-models'></a>"],"metadata":{"id":"SPXdAmMzW6sN"}},{"cell_type":"markdown","source":["**Vision Language Models (VLMs)** are multimodal systems that jointly process and reason over visual (images, videos) and linguistic (text) information. By integrating the two modalities, VLMs can understand and communicate about visual content using natural language.\n","\n","VLMs take both an image and its textual description as input and generate text as output. In addition, VLM variants have also been developed that can output images. In this lecture, we will focus on VLMs that output text, as the most common type.\n","\n","<img src=\"images/vlm_structure.png\" width=\"450\">\n","\n","*Figure: VLM structure.* Source: [8].\n","\n","Building datasets for VLMs requires collecting a large number of images and corresponding text, typically in the form of image captions or descriptive phrases. Several very large such datasets exist that have been essential for training modern VLMs and contain millions or billions of image-text pairs, with descriptions in English and other languages. For instance, the [LAION-5B](https://laion.ai/blog/laion-5b/) dataset has 5.8 billion image-text examples, and the [PMD (Public Model Dataset)](https://huggingface.co/datasets/facebook/pmd) contains 70 million image-text examples.\n","\n","During training, a VLM learns to map visual and textual representations into a joint embedding space. This mapping allows the model to associate visual features (shapes, colors, spatial relations) with linguistic concepts, enabling it to generalize to a wide range of vision tasks and perform zero-shot inference on unseen examples.\n","\n","The example below illustrates a VLM performing tasks such as object localization, segmentation, visual question answering, and image learning with instructions. The user prompts are shown on the left, and the model responses are given on the right. As demonstrated in this example, VLM models can not only interpret the semantic content of images, but can also understand spatial relations in images, such as identifying relative positions of objects, or generating segmentation masks. VLMs can also output bounding boxes of objects, and perform other spatial tasks.\n","\n","<img src=\"images/VLM_capabilities.jpg\" width=\"600\">\n","\n","*Figure: VLM prompts and responses.* Source: [9].\n","\n","In general, VLMs can perform various multimodal tasks including:\n","\n","- Image and video captioning/summarization: generate context-aware descriptions of images or video frames.\n","- Visual question answering (VQA): answer open-ended questions based  on visual content.\n","- Image-based reasoning: provide explanations or logical reasoning about visual scenes.\n","- Multimodal dialogues: engage in conversations involving visual inputs.\n","- Text-to-image search: retrieve images, figures, or diagrams in documents that match a textual query.\n","- Image generation: generate new images based on textual prompts."],"metadata":{"id":"5c4bRNh9lnUB"}},{"cell_type":"markdown","source":["### 22.3.1 VLM Architectures<a name='22.3.1-vlm-architectures'></a>"],"metadata":{"id":"Vxhi7aHnnF_x"}},{"cell_type":"markdown","source":["Numerous architectures of VLMs have been proposed in recent years that employ different strategies to integrate visual and textual information. Two broad categories inlcude VLM architectures with shared multimodal embeddings and with fused multimodal embeddings, illustrated in the figure.\n","\n","<img src=\"images/vlm_architectures.png\" width=\"550\">\n","\n","*Figure: VLM architectures.* Source: [10].\n"],"metadata":{"id":"HjNuq-NGJ7Ov"}},{"cell_type":"markdown","source":["####  **VLMs with Shared Multimodal Embeddings**\n","\n","VLM architectures with shared embeddings include the following main components.\n","\n","**Multimodal inputs**. Input modalities in VLMs include *visual inputs* (images, PDF documents, or videos) and *textual inputs* (captions, question-answer pairs, or instructions).\n","\n","**Encoding Modalities.** A *vision encoder* transforms the visual input into numerical representations, referred to as visual embeddings. The vision encoder in VLMs is commonly a variant of the Vision Transformer (ViT) architecture. A *text encoder* converts textual prompts into text embeddings. The text encoder is typically a Transformer-based encoder often pretrained on large text corpora.\n","\n","**Projection into a multimodal embedding space.** The visual and textual embeddings are next projected into a shared multimodal embedding space. This step is achieved through a *projector head* (a.k.a. projector layer or just projector), which is usually implemented as a small Transformer block or a block of fully-connected layers. The projector aligns the visual and textual respresentations into a shared embedding space, allowing the model to reason simultaneously across images and language.\n","\n","The shared embedding space enables VLMs to link textual concepts (e.g., cat) with corresponding visual evidence (a cat's color or location in the image), allowing reasoning across both language and vision. For instance, the model understands \"cat\" not just as a word, but also as a visual object in the image."],"metadata":{"id":"hh5hE6sdqk78"}},{"cell_type":"markdown","source":["One representative model of this type of VLM architectures is CLIP (Contrastive Language-Image Pretraining). It is one of the earliest models that introduced vision-language alignment through contrastive learning. CLIP employs both a vision encoder and a text encoder, and learns a shared embedding space in which images and their textual descriptions are semantically aligned. *Contrastive learning* in CLIP is employed to associate visual and textual content by maximizing the similarity between matched image and text pairs and minimizing the similarity between mismatched ones. In the figure below, the image encoder outputs image embeddings $I_1, I_2, I_3, ..., I_N$ for each image in the batch, and the text encoder outputs text embeddings $T_1, T_2, T_3, ..., T_N$ for each corresponding text in the batch. The model computes a similarity score between each pair of image embeddings (e.g., $I_i$) and text embeddings (e.g., $T_j$) to align them into a shared embedding space $I_i\\cdot T_j$. Similarity scores are calculated using the dot product (i.e., cosine similarity) $I_i\\cdot T_j$ between image and text embeddings. As training progresses, the two encoders are updated so that image and text representations corresponding to similar concepts are drawn close to each other in the shared embedding space, while dissimilar concepts are pushed apart.\n","\n","<img src=\"images/CLIP.png\" width=\"450\">\n","\n","*Figure: CLIP architecture.* Source: [10].\n","\n","The pretrained vision encoder of CLIP has been widely adopted as a vision encoder component in numerous later VLM variants."],"metadata":{"id":"Bs_C4lozjzrI"}},{"cell_type":"markdown","source":["#### **VLMs with Fused Multimodal Embeddings**\n","\n","Other VLM architectures fuse together image and text representations into joint multimodal embeddings to enable more advanced visual reasoning. These models typically employ a *text decoder* network from pretrained LLM, which generates a text output.\n","\n","The following figure illustrates the workflow of this class of VLMs. The visual embeddings from the vision encoder are first matched by the multimodal projector to the corresponding text embeddings of the LLM. The LLM decoder receives a combined representation consisting of visual embeddings (image tokens) from the projector and text embeddings (text tokens) from the user's prompt or question. The LLM decoder serves as the text generation backbone, and using the combined image and text tokens, it generates a textual output in an autoregressive manner, one token at a time. Each new token is conditioned on previously generated tokens and on the fused multimodal embeddings.\n","\n","<img src=\"images/vlm_fused_embeddings.png\" width=\"350\">\n","\n","*Figure: Fused multimodal embeddings.* Source: [11].\n","\n","Training VLMs with fused vision and text embeddings typically involves multiple stages, as shown in the next figure. In the first stage only the multimodal projector (also referred to as fusion layer or adapter) is trained while keeping the image encoder and the LLM text decoder frozen. This is followed by a second stage that involves additional finetuning of the multimodal projector and parts of the text decoder, while keeping the image encoder frozen.\n","\n","<img src=\"images/vlm_training.png\" width=\"550\">\n","\n","*Figure: VLM training.* Source: [11].\n","\n","In addition, in some VLM architectures, the vision encoder is also finetuned in the second stage to further improve cross-modal reasoning."],"metadata":{"id":"UvXspadVqk91"}},{"cell_type":"markdown","source":["Representative models of this VLM architectures are BLIP and Flamingo, which introduced cross-attention mechanism to fuse image and text embeddings (this is similar to the cross-attention module connecting the encoder and decoder in standard Transformer networks). Cross-modal attention enables direct fusion of image and text embeddings into a single multimodal representation, allowing the models to reason more efficiently over both modalities.\n","\n","LLaVA (Large Language and Vision Assistant) architecture introduced a different design which replaced the cross-attention module with an MLP (Multi-Layer Perceptron) adapter for fusing multimodal embeddings. Spciefically, the MLP adapter is a small fully connected network that linearly projects vision and text tokens and then concatenates them into a fused representation that is passed to the text decoder.\n","\n","These VLMs laid the foundations for the modern multimodal foundation models, such as Gemini 2.5 (Google), GPT-5 (OpenAI), Claude Opus 4 (Anthropic), Qwen-VL (Alibaba), and Mistral 3.1 (Mistral AI). The modern multimodal models typically employ MLP adapters for fusing image and text representations within a unified embedding space, and provide enhanced visual comprehension, reasoning, and dialog capabilities.\n","\n","Also, many open-source VLM alternatives have made this fused-architecture functionality widely accessible to the research community, and include LLaVA (Microsoft), Qwen-VL (Alibaba), LLaMA 3.2 Vision (Meta AI), InternVL (OpenGVLab), Pixtral (Mistral AI), and others.\n"],"metadata":{"id":"FAgPro0hqk_0"}},{"cell_type":"markdown","source":["### 22.3.2 Benchmarking VLMs<a name='22.3.2-benchmarking-vlms'></a>\n","\n","Performance of VLMs is assessed using multimodal benchmarks that evaluate models on a variety of tasks, such as reasoning, visual question answering, document comprehension, video understanding, and other tasks. Most benchmarks consist of a set of images with associated questions, often posed as multiple-choice questions. Popular benchmarks are [MMMU](https://mmmu-benchmark.github.io/), [Video-MME](https://video-mme.github.io/home_page.html), [MathVista](https://mathvista.github.io/), and [ChartQA](https://github.com/vis-nlp/ChartQA). MMMU is the most comprehensive benchmark, and contains 11.5K multimodal challenges that require knowledge and reasoning across different disciplines such as arts and engineering.\n","\n","Several VLM-specific leaderboards provide comparative rankings across diverse metrics. [Vision Arena](https://lmarena.ai/leaderboard/vision) ranks models based on anonymous voting of model outputs by human preferences. [Open VLM Leaderboard](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard) provides comparative ranking of VLMs according to different metrics and average scores."],"metadata":{"id":"6z3TLzuk6Q6D"}},{"cell_type":"markdown","source":["### 22.3.3 VLMs Importance<a name='22.3.3-vlms-importance'></a>\n","\n","Traditional computer vision (CV) models are constrained to learning from a predefined and fixed set of categories or objects for image classification or object detection (e.g., identify whether an image contains a cat or a dog). Moreover, these tasks require users to manually label a large number of images with a specific category for classification, or assign bounding boxes to multiple objects in each image for object detection, which is a tedious, time-consuming, and expensive process.\n","\n","Conversely, VLMs are trained with more  detailed textual descriptions of images, where for example an image can contain cats, dogs, and other objects, as well as the text description can provide additional contextual information (e.g., the cat is sitting, the dog is running, etc.). Learning from rich natural language descriptions allows VLMs to better understand visual scenes without limiting the learning to a narrow set of visual concepts comprising a fixed number of classes and objects. Also, it eliminates the need for exhaustive manual image labeling and extends VLMs' utility beyond traditional CV tasks like classification or detection to new capabilities including reasoning, summarization, question answering, and interactive dialogue, by simply changing the text prompt.\n","\n","VLMs have been applied across many domains and industries, and offer great potential to enhance visual perception. For instance, they can be used to review videos and extract insights for industrial inspection and robotics (detect faults, monitor operations, identify anomalies in real time), safety and infrastructure monitoring (recognize floods, fires, or traffic hazards), retail and logistics (track empty shelves, detect misplaced items, identify supply-chain bottlenecks), and numerous other tasks."],"metadata":{"id":"cXozhEaErru5"}},{"cell_type":"markdown","source":["### 22.3.4 VLM Finetuning <a name='22.23.4-vlm-finetuning'></a>"],"metadata":{"id":"OWKHrZNos2BH"}},{"cell_type":"markdown","source":[],"metadata":{"id":"qMwR2hiLlnWN"}},{"cell_type":"code","source":[],"metadata":{"id":"Nuc4bJf_lawe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"iviiNT2_s2Cf"}},{"cell_type":"markdown","source":["## References <a name='references'></a>\n","\n","1. Mixture of Experts Model(MOE) in AI: What is it and How does it work?, by Sahin Ahmed, available at [https://blog.gopenai.com/mixture-of-experts-model-moe-in-ai-what-is-it-and-how-does-it-work-b845ed38a3ab](https://blog.gopenai.com/mixture-of-experts-model-moe-in-ai-what-is-it-and-how-does-it-work-b845ed38a3ab).\n","2. Transformer vs. Mixture of Experts in LLMs, by Avi Chawla, avialable at [https://www.dailydoseofds.com/p/transformer-vs-mixture-of-experts-in-llms/](https://www.dailydoseofds.com/p/transformer-vs-mixture-of-experts-in-llms/).\n","3. What is Mixture of Experts?, by Dave Bergmann, available at [https://www.ibm.com/think/topics/mixture-of-experts](https://www.ibm.com/think/topics/mixture-of-experts).\n","4. A Crash Course on Building RAG Systems - Part 1 (With Implementation), by Akshay Pachaar, Avi Chawla, available at [https://www.dailydoseofds.com/a-crash-course-on-building-rag-systems-part-1-with-implementations/](https://www.dailydoseofds.com/a-crash-course-on-building-rag-systems-part-1-with-implementations/).\n","5. Advanced NLP Fall 2025 Code, by Sean Welleck, available at [https://github.com/cmu-l3/anlp-fall2025-code/blob/main/10_rag/rag.ipynb](https://github.com/cmu-l3/anlp-fall2025-code/blob/main/10_rag/rag.ipynb).\n","6. Building a Simple RAG Application Using LlamaIndex, by Abid Ali Awan, available at [https://machinelearningmastery.com/building-a-simple-rag-application-using-llamaindex/](https://machinelearningmastery.com/building-a-simple-rag-application-using-llamaindex/).\n","7. RAG: a Simple Practical Example Using llama index and HuggingFace, by Mayada Khatib, available at [https://medium.com/@mayadakhatib/rag-a-simple-practical-example-using-llama-index-and-huggingface-fab3e5aa7442](https://medium.com/@mayadakhatib/rag-a-simple-practical-example-using-llama-index-and-huggingface-fab3e5aa7442).\n","8. Vision-language models (VLMs) Explained (pt. 1), by Sean Trott, available at [https://seantrott.substack.com/p/vision-language-models-vlms-explained](https://seantrott.substack.com/p/vision-language-models-vlms-explained).\n","9. CLIP: Connecting text and images, OpenAI, available at [https://openai.com/index/clip/](https://openai.com/index/clip/).\n","10. Vision Language Models (VLMs) Explained - GeeksForGeeks, available at [https://www.geeksforgeeks.org/artificial-intelligence/vision-language-models-vlms-explained/](https://www.geeksforgeeks.org/artificial-intelligence/vision-language-models-vlms-explained/).\n","11. Vision Language Models Explained - Hugging Face Blog, by Merve,\n","Edward Beeching, available at [https://huggingface.co/blog/vlms](https://huggingface.co/blog/vlms).\n","12. Understanding Vision-Language Models (VLMs): A Practical Guide, by Pietro Bolcato, available at [https://medium.com/@pietrobolcato/understanding-vision-language-models-vlms-a-practical-guide-8da18e9f0e0c](https://medium.com/@pietrobolcato/understanding-vision-language-models-vlms-a-practical-guide-8da18e9f0e0c).\n","13. What Are Vision Language Models, by NVIDIA, available at [https://www.nvidia.com/en-us/glossary/vision-language-models/](https://www.nvidia.com/en-us/glossary/vision-language-models/).\n","14. Vision Language Models, by Rohit Bandaru, available at [https://rohitbandaru.github.io/blog/Vision-Language-Models/](https://rohitbandaru.github.io/blog/Vision-Language-Models/)."],"metadata":{"id":"2amIei5Ns2En"}}]}