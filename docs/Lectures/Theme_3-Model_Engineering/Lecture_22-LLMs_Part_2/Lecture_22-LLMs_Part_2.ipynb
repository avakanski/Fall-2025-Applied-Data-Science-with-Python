{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyODHk2zoD8ebhfzttbCbwr0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lecture 22 - Large Language Models (Part 2)"],"metadata":{"id":"xsrN8iJZlUgo"}},{"cell_type":"markdown","source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_22-LLMs_Part_2/Lecture_22-LLMs_Part_2.ipynb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_22-LLMs_Part_2/Lecture_22-LLMs_Part_2.ipynb)"],"metadata":{"id":"nT4plF3Klb8u"}},{"cell_type":"markdown","source":[],"metadata":{"id":"iaCiejDTiCUu"}},{"cell_type":"markdown","source":["<a id='top'></a>"],"metadata":{"id":"Zzs06-BflidN"}},{"cell_type":"markdown","source":["- [22.2 Vision-Language Models](#22.2-vision-language-models)\n","  - [22.2.1 VLM Workflow](#22.2.1-vlm-workflow)\n","  - [22.2.2 Representative VLM Models](#22.2.2-representative-vlm-models)\n","  - [22.2.3 VLM Finetuning](#22.2.3-vlm-finetuning)"],"metadata":{"id":"FNUdYxkulent"}},{"cell_type":"markdown","source":["## 22.2 Vision-Language Models <a name='22.2-vision-language-models'></a>"],"metadata":{"id":"SPXdAmMzW6sN"}},{"cell_type":"markdown","source":["**Vision Language Models (VLMs)** are multimodal systems that jointly process and reason over visual (images, videos) and linguistic (text) information. By integrating these two modalities, VLMs can understand and communicate about visual content using natural language.\n","\n","VLMs take both an image and its textual description as input and generate text as output. Building datasets for such models requires large scale collection of image and corresponding text, typically in the form of captions or descriptive phrases. Several very large such datasets exist that have been essential for training modern VLMs and contain billions of image-text pairs, with descriptions in English and other languages. For instance, the [LAION-5B](https://laion.ai/blog/laion-5b/) dataset has over 5.8 billion image-text examples.\n","\n","<img src=\"images/vlm_structure.png\" width=\"450\">\n","\n","*Figure: VLM structure.* Source: [1].\n","\n","During training, a VLM learns to map visual and textual representations into a shared embedding space. This alignment allows the model to associate visual features (shapes, colors, spatial relations) with linguistic concepts, enabling it to generalize to a wide range of vision tasks and perform zero-shot inference on unseen examples.\n","\n","The example below illustrates a VLM performing tasks such as object localization, segmentation, visual question answering, and image learning via instructions. The user prompts are shown on the left, and the model responses are given on the right. As demonstrated in this example, VLM models can not only interpret the semantic content of images, but can also understand spatial relations in images, such as identifying relative positions of objects, generating segmentation masks, outputting bounding boxes of objects, and performing other spatial tasks.\n","\n","<img src=\"images/VLM_capabilities.jpg\" width=\"650\">\n","\n","*Figure: VLM prompts and responses.* Source: [2].\n","\n","In general, VLMs can perform various multimodal tasks including:\n","\n","- Image and video captioning/summarization: generate context-aware descriptions of images or video frames.\n","- Visual Question Answering (VQA): answer open-ended questions based  on visual content.\n","- Image-based reasoning: provide explanations or logical reasoning about visual scenes.\n","- Multimodal dialogues: engage in conversations involving visual inputs.\n","- Text-to-image search: retrieve images, figures, or diagrams in documents that match a textual query.\n","- Image generation: generate new images based on textual prompts."],"metadata":{"id":"5c4bRNh9lnUB"}},{"cell_type":"markdown","source":["### 22.2.1 VLM Workflow <a name='22.2.1-vlm-workflow'></a>"],"metadata":{"id":"Vxhi7aHnnF_x"}},{"cell_type":"markdown","source":["Despite differences among architectures, most VLMs share a similar workflow with the main components shown in the figure above\n","\n","**Multimodal inputs**. Input modalities in VLMs include *visual inputs* (images, pdf documents, videos) and *textual inputs* (captions, question-answer pairs, instuctions).\n","\n","**Encoding Modalities.** A *vision encoder* transforms the visual input into numerical representations known as visual embeddings. The vision encoder in VLMs is commonly a Vision Transformer (ViT) or a Convolutional Neural Network (CNNs). A *text encoder* converts textual prompts into textual embeddings. The text encoder is typically a pretrained LLM, such as LLaMA, Mistral, or Qwen.\n","\n","**Projection and fusion into a joint embeddings space.** The visual and textual embedding are next aligned into a shared embedding space. This is done by a *projector layer* (also known as a fusion layer), which is usually a small Transformer block or a fully-connected neural network (MLP). The projector layer merges the visual and textual embeddings into a joint representation, enabling the VLM to reason simultaneously across images and language.\n","\n","**Autoregressive multimodal decoding.** The combined multimodal embeddings are passed to a *text decoder*, which generates a textual response one token at a time. Each new token is conditioned on previously generated tokens and the multimodal embeddings.\n","\n","<img src=\"images/multimodal_decoding.jpg\" width=\"650\">\n","\n","*Figure: Multimodal decoding.* Source: [2]."],"metadata":{"id":"hh5hE6sdqk78"}},{"cell_type":"markdown","source":["The shared embedding space enables VLMs to link textual concepts (e.g., cat) with corresponding visual evidence (a cat's color or location), allowing reasoning about both language and vision simultaneously. For instance, the model understands \"cat\" not just as a word, but as a visual object in the image.\n","\n","Training a VLM typically involves multiple stages of alignment and fusion between pretrained encoders. A common approach begins with frozen visual and textual encoders, while the projector layer is trained to align their embeddings. In later stages, additional finetuning is performed to update the  text encoder and projector for improved multimodal reasoning. Although this way of training VLMs has been common, different VLMs adopt variations of this procedure to balance efficiency and task-specific performance."],"metadata":{"id":"UvXspadVqk91"}},{"cell_type":"markdown","source":["### 22.2.2 Representative VLM Models <a name='22.2.2-representative-vlm-models'></a>\n","\n","CLIP (Contrastive Language-Image Pretraining) is an early model that introduced vision-language alignment and it has been used extensively as a vision encoder in various VLMs. CLIP employs *contrastive learning* to associate visual and textual content by maximizing the similarity score between matched image and text embeddings and minimizing the score for mismatched ones. In the figure below, an image encoder outputs image embeddings $I_1, I_2, I_3, ..., I_N$, and a text encoder outputs text embeddings $T_1, T_2, T_3, ..., T_N$. The model computes the similarities between each pair of text and image embeddings to align them into a combined embeddings space $I_i T_j$.\n","\n","<img src=\"images/CLIP.png\" width=\"550\">\n","\n","*Figure: CLIP architecture.* Source: [5].\n","\n","Subsequent models, including BLIP and Flamingo, introduced cross-attention mechanisms that enable efficient few-shot multimodal reasoning.\n","\n","More recent models such as Gemini 2.5 Pro (Google), GPT-5 (OpenAI), Claude Opus 4 (Anthropic), Qwen-VL Max (Alibaba), and Mistral 3.1 (Mistral AI) demonstrate advanced visual comprehension, reasoning, and dialogic capabilities.\n","\n","Also, there are many open-source VLM alternatives that have made this functionality widely accessible, and include LLaVA, Qwen-VL, LLaMA 3.2 Vision, InternVL, Pixtral, and others."],"metadata":{"id":"FAgPro0hqk_0"}},{"cell_type":"markdown","source":["#### Benchmarking VLMs\n","\n","Performance of VLMs is assessed using multimodal benchmarks that evaluate models on a variety of tasks, such as reasoning, visual question answering, document comprehension, video understanding, and other tasks. Most benchmarks consist of a set of images with associated questions, often posed as multiple-choice questions. Popular benchmarks are [MMMU](https://mmmu-benchmark.github.io/), [Video-MME](https://video-mme.github.io/home_page.html), [MathVista](https://mathvista.github.io/), and [ChartQA](https://github.com/vis-nlp/ChartQA). MMMU is the most comprehensive benchmark, and contains 11.5K multimodal challenges that require knowledge and reasoning across different disciplines such as arts and engineering.\n","\n","Several VLM-specific leaderboards provide comparative rankings across diverse metrics. [Vision Arena](https://lmarena.ai/leaderboard/vision) ranks models based on anonymous voting of model outputs by human preferences. [Open VLM Leaderboard](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard) provides comparative ranking of VLMs according to different metrics and average scores."],"metadata":{"id":"6z3TLzuk6Q6D"}},{"cell_type":"markdown","source":["#### Importance\n","\n","Traditional computer vision (CV) models are constrained to learning from a predefined and fixed set of categories or objects for image classification or object detection (e.g., identify whether an image contains a cat or a dog). Moreover, these tasks require users to manually label a large number of images with a specific category or assign bounding boxes to multiple objects in each image for object detection, which is a tedious, time-consuming, and expensive process.\n","\n","Conversely, VLMs are trained with more  detailed textual descriptions of images, where for example an image can contain cats, dogs, and other objects, as well as, the text description can provide additional contextual information (e.g., the cat is sitting, the dog is running, etc.). Learning from rich natural language descriptions allows VLMs to better understand visual scenes without limiting the learning to a narrow set of visual concepts comprising a fixed number of categories and objects. Also, it eliminates the need for exhaustive manual image labeling and extends VLMs utility beyond traditional CV tasks like classification or detection to new capabilities including reasoning, summarization, question answering, and interactive dialogue, by simply changing the text prompt.\n","\n","VLMs have been applied across many domains and industries, and offer great potential for enhancing visual perception. For instance, they can be used to review videos and extract insights for industrial inspection and robotics (detect faults, monitor operations, identify anomalies in real time), safety and infrastructure monitoring (recognize floods, fires, or traffic hazards), retail and logistics (track empty shelves, detect misplaced items, identify supply-chain bottlenecks), and numerous other tasks."],"metadata":{"id":"cXozhEaErru5"}},{"cell_type":"markdown","source":["### 2.2.3 VLM Finetuning <a name='22.2.3-vlm-finetuning'></a>"],"metadata":{"id":"OWKHrZNos2BH"}},{"cell_type":"markdown","source":["We are excited to announce that TRLâ€™s SFTTrainer now includes experimental support for Vision Language Models! We provide an example here of how to perform SFT on a Llava 1.5 VLM using the llava-instruct dataset which contains 260k image-conversation pairs. The dataset contains user-assistant interactions formatted as a sequence of messages. For example, each conversation is paired with an image that the user asks questions about."],"metadata":{"id":"qMwR2hiLlnWN"}},{"cell_type":"code","source":[],"metadata":{"id":"Nuc4bJf_lawe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"iviiNT2_s2Cf"}},{"cell_type":"markdown","source":["## References <a name='references'></a>\n","\n","1. Vision Language Models (VLMs) Explained - GeeksForGeeks, available at [https://www.geeksforgeeks.org/artificial-intelligence/vision-language-models-vlms-explained/](https://www.geeksforgeeks.org/artificial-intelligence/vision-language-models-vlms-explained/).\n","2. Vision Language Models Explained - Hugging Face Blog, by Merve,\n","Edward Beeching, available at [https://huggingface.co/blog/vlms](https://huggingface.co/blog/vlms).\n","3. Understanding Vision-Language Models (VLMs): A Practical Guide, by Pietro Bolcato, available at [https://medium.com/@pietrobolcato/understanding-vision-language-models-vlms-a-practical-guide-8da18e9f0e0c](https://medium.com/@pietrobolcato/understanding-vision-language-models-vlms-a-practical-guide-8da18e9f0e0c).\n","4. What Are Vision Language Models, by NVIDIA, available at [https://www.nvidia.com/en-us/glossary/vision-language-models/](https://www.nvidia.com/en-us/glossary/vision-language-models/).\n","5. CLIP: Connecting text and images, OpenAI, available at [https://openai.com/index/clip/](https://openai.com/index/clip/)."],"metadata":{"id":"2amIei5Ns2En"}}]}