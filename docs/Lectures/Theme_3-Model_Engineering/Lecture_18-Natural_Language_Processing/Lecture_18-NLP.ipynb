{"cells":[{"cell_type":"markdown","metadata":{"id":"3JhHIQqNe4Qs"},"source":["# Lecture 18 - Natural Language Processing"]},{"cell_type":"markdown","metadata":{"id":"kPG5INqg-_qn"},"source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_18-Natural_Language_Processing/Lecture_18-NLP.ipynb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_18-Natural_Language_Processing/Lecture_18-NLP.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"oR4oHH5b5fP1"},"source":["<a id='top'></a>"]},{"cell_type":"markdown","metadata":{"id":"iEkmemKte4Qv"},"source":["- [18.1 Introduction to NLP](#18.1-introduction-to-nlp)\n","- [18.2 Preprocessing Text Data](#18.2-preprocessing-text-data)\n","- [18.3 Text Tokenization](#18.3-text-tokenization)\n","    - [18.3.1 Character-level Tokens](#18.3.1-character-level-tokens)\n","    - [18.3.2 Word-level Tokens](#18.3.2-word-level-tokens)\n","    - [18.3.3 Padding Word Sequences](#18.3.3-padding-word-sequences)\n","- [18.4 Representation of Groups of Words](#18.4-representation-of-groups-of-words)\n","- [18.5 Sequence Models Approach](#18.5-sequence-models-approach)\n","    - [18.5.1 Word Embeddings](#18.5.1-word-embeddings)\n","    - [18.5.2 Using TextVectorization Layer](#18.5.2-using-textvectorization-layer)\n","    - [18.5.3 Sequence Modeling with Recurrent Neural Networks](#18.5.3-sequence-modeling-with-recurrent-neural-networks)\n","- [References](#references)"]},{"cell_type":"markdown","metadata":{"id":"gTFgm_bJe4Qw"},"source":["## 18.1 Introduction to NLP <a name='18.1-introduction-to-nlp'></a>"]},{"cell_type":"markdown","metadata":{"id":"UFMFGCdwiw5k"},"source":["**Natural Language Processing (NLP)** is a branch of Computer Science (and more broadly, a branch of Artificial Intelligence) that is concerned with providing computers with the ability to understand texts and human language.\n","\n","Common tasks in NLP include:\n","\n","- *Text classification* — assign a class label to text based on the topic discussed in the text, e.g., sentiment analysis (positive or negative movie review), spam detection, content filtering (detect abusive content).\n","- *Text summarization/reading comprehension* — summarize a long input document with a shorter text.\n","- *Speech recognition* — convert spoken language to text.\n","- *Machine translation* — convert text in a source language to a target language.\n","- *Part of Speech (PoS) tagging* — mark up words in text as nouns, verbs, adverbs, etc.\n","- *Question answering* — output an answer to an input question.\n","- *Dialog generation* — generate the next reply in a conversation given the history of the conversation.\n","- *Text generation* — generate text to complete the sentence or to complete the paragraph.\n"]},{"cell_type":"markdown","metadata":{"id":"GF5bogcC7mUa"},"source":["## 18.2 Preprocessing Text Data <a name='18.2-preprocessing-text-data'></a>"]},{"cell_type":"markdown","metadata":{"id":"rWMSWyTqjNym"},"source":["In order to perform operations with text data, the data first need to be converted into a numerical representation.\n","\n","Converting text data into numerical form for processing by ML models typically involves the following steps:\n","\n","- *Standardization* - remove punctuation, convert the text to lowercase.\n","- *Tokenization* - break up the text into tokens (e.g., tokens can be individual words, several consecutive words (N-grams), parts of words, or individual characters).\n","- *Indexing* - assign a numerical index to each token in the training set (i.e., vocabulary).\n","\n","Modern ML models typically include an additional step - *embedding*, which involves assigning a numerical vector to each token (e.g., one-hot encoding and word embedding are explained in Section 18.5 below).\n"]},{"cell_type":"markdown","metadata":{"id":"cc6LF4pHjNyn"},"source":["### Text Standardization\n","\n","**Text standardization** usually includes several steps, which depend on the specific application. The steps involve:\n","\n","- Remove punctuation marks (such as comma, period) or non-alphabetic characters (@, #, {, ]).\n","- Change all words to lower-case letters, since ML models should consider *Text* and *text* as the same word.\n","\n","Some NLP tasks apply additional steps, such as:\n","\n","- Correct spelling errors, or replace abbreviations with full words.\n","- Remove stop words, such as *for*, *the*, *is*, *to*, *some*, etc.; if the task is text classification, these words are not relevant to the meaning of the text.\n","- Apply stemming and lemmatization, which transforms words to their base form, such as changing the word *driving* to *drive*, or *grilled* to *grill* since they have a common root.\n","\n","Applying text standardization is helpful for training ML models, for example, because the models do not need to consider *Text* and *text* as two different words, which reduces the requirements for large training datasets. However, depending on the application, text standardization may remove information that can be important for some tasks, and this should always be considered when performing text preprocessing."]},{"cell_type":"markdown","metadata":{"id":"k7RF_VR3jNyo"},"source":["### Tokenization\n","\n","**Tokenization** is breaking up a sequence of text into individual units called *tokens*.\n","\n","Tokenization can be performed at different levels:\n","\n","- *Character-level tokenization* - the text is divided into individual characters, and each character is a token, including letters, digits, punctuation marks, and symbols. One disadvantage of this type of tokenization is that antigrams (words with same letters in different order, such as *silent* and *listen*) can have the same numerical encoding, which can affect the performance of ML models. As well as, character-level tokenization does not capture semantic meaning of words as effectively as word-level tokens. Consequently, it is not widely used in practice.\n","- *Word-level tokenization* - each word in the text is a token. This type of tokenization provides a natural representation of input text with the words as building blocks of language, and it is commonly used.\n","- *Subword-level tokenization* - the words are divided into smaller units (e.g., tokenizing the word \"unhappiness\" into two tokens \"un\" + \"happiness\"). This type of tokenization is very common in recent Large Language Models. Also, in some languages with complex word structures, subword-level tokenization is the most suitable.\n","- *N-gram tokenization* - N consecutive words represent a token. For instance, N-grams consisting of two adjacent words are called bigrams, or three words constitute a trigram, etc. N-gram tokens preserve the words order and can potentially capture more information in the text. For instance, for spam filtering using  bigram tokens such as *mailing list* or *bank account* may provide more helpful information than using word-level tokens.\n","\n","For some NLP tasks, tokenization can also be performed at other levels of text. An example is sentence-level tokenization for the task of document segmentation in sentences."]},{"cell_type":"markdown","metadata":{"id":"NbD7810XjNyp"},"source":["An example of text standardization, word-level tokenization, and indexing is shown in the next figure.\n","\n","<img src=\"images/tokenization.png\" width=\"500\">\n","\n","*Figure: Text standardization, word-level tokenization, and indexing.*"]},{"cell_type":"markdown","metadata":{"id":"u2SDR8h32fE9"},"source":["## 18.3 Text Tokenization <a name='18.3-text-tokenization'></a>"]},{"cell_type":"markdown","metadata":{"id":"9tDuayp80W1M"},"source":["TensorFlow-Keras library provides a text preprocessing function `Tokenizer` for converting raw text into sequences of tokens. The `Tokenizer` function performs text standardization, tokenization, and indexing.\n","\n","The TensorFlow-Keras `Tokenizer` has the following arguments:\n","\n","- *num_words*: the maximum number of words to keep in the input text. It is recommended to set a high number if we are not sure what is the maximum number of words in the training set, because if we set a number less than the words in the text, some words will not be tokenized.\n","- *filters*: by default, all punctuations and special characters in the text will be removed. If we want to change that, we can provide a list of punctuations and characters to keep.\n","- *lower*: can be True or False. By default, it is True, and that means all texts will be converted to lowercase.\n","- *split*: separator for splitting words. A default separator is a space `(\" \")`.  \n","- *char_level*: can be True or False. By default, it is False and will perform word-level tokenization. If it is True, the function will perform character-level tokenization.\n","- *oov_token*: oov stands for Out Of Vocabulary, and it denotes a special token that will replace tokens that are not present in the input text."]},{"cell_type":"markdown","metadata":{"id":"hA9dZFJejNys"},"source":["### 18.3.1 Character-level Tokens <a name='18.3.1-character-level-tokens'></a>"]},{"cell_type":"markdown","metadata":{"id":"eP3igoipEMDW"},"source":["To use `Tokenizer` for character-level tokenization, we need to set `char_level` to `True`. Let's set the number of tokens to 1,000.\n","\n","Let's apply it to the following sentence by using  the method `fit_on_texts()`."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13362,"status":"ok","timestamp":1761958635265,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"alBwiPau5LC9","outputId":"126fe8ef-ca8f-46fa-8464-f13609cc7f6c","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version:2.19.0\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","# Print the version of tf\n","print(\"TensorFlow version:{}\".format(tf.__version__))"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"CNUzPUWIDfGi","tags":[],"executionInfo":{"status":"ok","timestamp":1761958635266,"user_tz":360,"elapsed":15,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["# A sample sentence\n","sentence = ['TensorFlow is a Machine Learning framework']"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"vr496JvAEEIN","tags":[],"executionInfo":{"status":"ok","timestamp":1761958635267,"user_tz":360,"elapsed":14,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","character_tokenizer = Tokenizer(num_words=1000, char_level=True)\n","\n","# Fitting tokenizer on sentences\n","character_tokenizer.fit_on_texts(sentence)"]},{"cell_type":"markdown","metadata":{"id":"_n39kkPvGQV7"},"source":["When the `Tokenizer` separates the characters in text, it creates a  dictionary that maps each character to an integer index. We can inspect the dictionary by using the attribute `word_index`, although since we have set `char_level` to `True` in this case it is the character index.\n","\n","Note that the start index is 1. By default, all letters are converted to lowercase. The first token is an empty space `\" \"`, the second is the letter `'e'`, etc. There are 17 unique characters in the sentence, including the empty space."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1761958635268,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"R7ztDhDIGPuo","outputId":"c0cf201a-0e81-4da0-8472-0bf23b0eeca4","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["{' ': 1, 'e': 2, 'n': 3, 'r': 4, 'a': 5, 'o': 6, 'i': 7, 's': 8, 'f': 9, 'l': 10, 'w': 11, 'm': 12, 't': 13, 'c': 14, 'h': 15, 'g': 16, 'k': 17}\n"]}],"source":["char_index = character_tokenizer.word_index\n","print(char_index)"]},{"cell_type":"markdown","metadata":{"id":"I2L349BGjNyu"},"source":["The method `text_to_sequences` outputs the indices for the text. You can check that the word `TensorFlow` has the indices 13, 2, 3, 8, 6, 4, 9, 10, 6, 11, where each index corresponds to the letters listed in `char_index`."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1761958635283,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"JAPxvWUljNyv","outputId":"c5c8de16-87b8-401f-c2cd-9e43854d66a9","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[13, 2, 3, 8, 6, 4, 9, 10, 6, 11, 1, 7, 8, 1, 5, 1, 12, 5, 14, 15, 7, 3, 2, 1, 10, 2, 5, 4, 3, 7, 3, 16, 1, 9, 4, 5, 12, 2, 11, 6, 4, 17]]\n"]}],"source":["print(character_tokenizer.texts_to_sequences(sentence))"]},{"cell_type":"markdown","metadata":{"id":"MzhZW63ljNyv"},"source":["As we mentioned earlier, character-level tokenization is rarely used, because it does not capture semantic meaning of words as effectively as word-level tokens."]},{"cell_type":"markdown","metadata":{"id":"mkfbQe9XjNyv"},"source":["### 18.3.2 Word-level Tokens <a name='18.3.2-word-level-tokens'></a>"]},{"cell_type":"markdown","metadata":{"id":"uemMRwYrGwpb"},"source":["To use the `Tokenizer` function for tokenizing words instead of characters, we need to just change the argument `char_level` to `False`, which is the default setting, so we may as well just omit it."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"CmuIbnlKjNyv","tags":[],"executionInfo":{"status":"ok","timestamp":1761958635284,"user_tz":360,"elapsed":18,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["# Sample sentences\n","sentences = ['TensorFlow is a Machine Learning framework.',\n","             'Keras is a well designed deep learning API!',\n","             'Keras is built on top of TensorFlow!']"]},{"cell_type":"markdown","metadata":{"id":"vk1fTO2yjNyw"},"source":["After the text is broken down into individual words, the `Tokenizer` builds a *vocabulary* of all words that are found in the input text, and assigns a unique integer index to each word in the vocabulary. We can inspect the words by using again the attribute `word_index`."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1761958635284,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"tpIE4S3PHCbY","outputId":"2a3c1cc1-4b28-4cbe-96b9-eefbb6da3d13","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["{'is': 1, 'tensorflow': 2, 'a': 3, 'learning': 4, 'keras': 5, 'machine': 6, 'framework': 7, 'well': 8, 'designed': 9, 'deep': 10, 'api': 11, 'built': 12, 'on': 13, 'top': 14, 'of': 15}\n"]}],"source":["word_tokenizer = Tokenizer(num_words=1000)\n","\n","# Fitting tokenizer on sentences\n","word_tokenizer.fit_on_texts(sentences)\n","\n","word_index = word_tokenizer.word_index\n","print(word_index)"]},{"cell_type":"markdown","metadata":{"id":"NGmvr6I2GwwJ"},"source":["There are 15 unique words in the above sentences. By default, all punctuations are removed and all letters are converted to lowercase.\n","\n","The indices for the above three sentences are shown below. For instance, the first list `[2, 1, 3, 6, 4, 7]` represents the first sentence in the text `TensorFlow is a Machine Learning framework`."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1761958635285,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"9Xz_E4PXjNyx","outputId":"6d08e969-14eb-4896-c5c8-13a96150fd32","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[2, 1, 3, 6, 4, 7], [5, 1, 3, 8, 9, 10, 4, 11], [5, 1, 12, 13, 14, 15, 2]]\n"]}],"source":["print(word_tokenizer.texts_to_sequences(sentences))"]},{"cell_type":"markdown","metadata":{"id":"ZAJYp5sAIstj"},"source":["Also, `word_counts` can return the number of times each word appears in the sentences."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1761958635293,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"XOb-6DGTIIDp","outputId":"a5fd2f45-f04b-41d8-c098-ff99feb4c067","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('tensorflow', 2),\n","             ('is', 3),\n","             ('a', 2),\n","             ('machine', 1),\n","             ('learning', 2),\n","             ('framework', 1),\n","             ('keras', 2),\n","             ('well', 1),\n","             ('designed', 1),\n","             ('deep', 1),\n","             ('api', 1),\n","             ('built', 1),\n","             ('on', 1),\n","             ('top', 1),\n","             ('of', 1)])"]},"metadata":{},"execution_count":9}],"source":["word_counts = word_tokenizer.word_counts\n","word_counts"]},{"cell_type":"markdown","metadata":{"id":"LZE0QY8NjNyx"},"source":["#### Out of Vocabulary Words"]},{"cell_type":"markdown","metadata":{"id":"e4832s6thIYV"},"source":["To handle the case when the Tokenizer is applied to text that contains words that were not present in the original documents, we can define a special token called `oov_token`. This token will be used to replace the words that are Out Of Vocabulary (OOV).\n","\n","In the example below, we set the `oov_token` to `'Word Out of Vocab'`, which has been assigned the index `1`."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1761958635316,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"EHy4egiNh1dg","outputId":"050b530b-04b5-47eb-9e6a-c639f3afc95e","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["{'Word Out of Vocab': 1, 'is': 2, 'tensorflow': 3, 'a': 4, 'learning': 5, 'keras': 6, 'machine': 7, 'framework': 8, 'well': 9, 'designed': 10, 'deep': 11, 'api': 12, 'built': 13, 'on': 14, 'top': 15, 'of': 16}\n"]}],"source":["word_tokenizer_2 = Tokenizer(num_words=1000, oov_token='Word Out of Vocab')\n","word_tokenizer_2.fit_on_texts(sentences)\n","word_index = word_tokenizer_2.word_index\n","print(word_index)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1761958635324,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"9tT78VLKjNyy","outputId":"dc09eb97-b147-4b89-8a0b-43ca1a84ec71","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[3, 2, 4, 7, 5, 8], [6, 2, 4, 9, 10, 11, 5, 12], [6, 2, 13, 14, 15, 16, 3]]\n"]}],"source":["# Converting text to sequences\n","print(word_tokenizer_2.texts_to_sequences(sentences))"]},{"cell_type":"markdown","metadata":{"id":"2Bnw7dUui5Kv"},"source":["Next, if we pass text with new words that the tokenizer was not fit to, the new words will be replaced with the `oov_token`. In the example, the words 'I', 'like', and 'superb' have been assigned the index 1."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1761958635331,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"x7mHGlEjjOG_","outputId":"aa3135fc-b029-4e17-bffa-9e9ff1720494","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 1, 3], [6, 2, 4, 1, 11, 5, 12]]\n"]}],"source":["new_sentences = ['I like TensorFlow', # 'I' and 'like' are new words\n","                'Keras is a superb deep learning API'] # 'superb' is a new word\n","\n","print(word_tokenizer_2.texts_to_sequences(new_sentences))"]},{"cell_type":"markdown","metadata":{"id":"jj4rtaDbjNyz"},"source":["And also, if we work with a large dataset that contains many documents, we can limit the number of words in the vocabulary to 20,000 or 30,000, and consider the rare words as out-of-vocabulary words. This can reduce the input space of the model, by ignoring those words that are present only once or twice in the large database."]},{"cell_type":"markdown","metadata":{"id":"zt7W6Gdflok5"},"source":["### 18.3.3 Padding Word Sequences <a name='18.3.3-padding-word-sequences'></a>"]},{"cell_type":"markdown","metadata":{"id":"lUfBkgJimHpo"},"source":["Most machine learning models require the input samples to have the same length/size. In TensorFlow-Keras, the function `pad_sequences()` can be used to pad the text sequences with predefined values, so that they have the same length.\n","\n","The function `pad_sequences()` accepts the following arguments:\n","\n","- *sequence*: a list of integer indices (i.e., tokenized text).\n","- *maxlen*: maximum length of all sequences; if not provided, sequences will be padded to the length of the longest sequence.\n","- *padding*: 'pre' (default) or 'post', whether to pad before the sequence or after the sequence.\n","- *truncating*: 'pre' (default) or post', whether to remove the values from sequences larger than maxlen at the beginning or at the end of the sequences.\n","- *value*: a float or a string to use as a padding value. By default, the sequences are padded with 0.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1761958635365,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"-gtdBQdVRHEc","outputId":"22d6bc86-5b09-443f-cb87-99c37c55f765","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[3, 2, 4, 7, 5, 8], [6, 2, 4, 9, 10, 11, 5, 12], [6, 2, 13, 14, 15, 16, 3]]\n"]}],"source":["# First, let's diplay the original non-padded sequences\n","tokenized_sentences = word_tokenizer_2.texts_to_sequences(sentences)\n","\n","print(tokenized_sentences)"]},{"cell_type":"markdown","metadata":{"id":"qgLaHzlYrf3j"},"source":["The next cell shows the above sequences pre-padded with 0 to sequences with length 10."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1761958635365,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"7i5cMbLQrthF","outputId":"5e5137b6-e2c5-43ef-ef0c-6655e9b911a2","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0  0  0  0  3  2  4  7  5  8]\n"," [ 0  0  6  2  4  9 10 11  5 12]\n"," [ 0  0  0  6  2 13 14 15 16  3]]\n"]}],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Pad the sequences\n","padded_sequences = pad_sequences(tokenized_sentences, maxlen=10)\n","\n","print(padded_sequences)"]},{"cell_type":"markdown","metadata":{"id":"OdhmWKLAooyP"},"source":["## 18.4 Representation of Groups of Words <a name='18.4-representation-of-groups-of-words'></a>"]},{"cell_type":"markdown","metadata":{"id":"acxXXFMPopRA"},"source":["Representation of groups of words in Machine Learning models for text processing is based on two categories of approaches:\n","\n","- **Set models** approach: the text is represented as unordered collection of words. Such approaches include *bag-of-words* models.\n","- **Sequence models** approach: the text is represented as ordered sequences of words. These methods preserve the order of the words in the text. Representatives of these approaches are Recurrent Neural Networks, and Transformer Networks.\n","\n","The order of words in natural language is not necessarily fixed, and sentences with different orders of the words can have the same meaning. Also, different languages use different ways to order the words. As a result, defining the order of the words in text in NLP tasks is not straightforward.\n","\n","### Bag-of-Words Models\n","\n","**Bag-of-words** models discard the information about the order of the words, where the term *bag* implies that the structure of the text is lost. A depiction of a bag-of-words is shown below, where the initial text is separated into word-level tokens, and a bag is created from all words in the text. Also, instead of individual words, these models often employ N-gram representations. This type of models typically consider the frequency of occurrence of each word in the training data, and a classifier is trained by using the word counts as inputs.\n","\n","For instance, to create a spam filtering classifier, two bags-of-words can be created from the words in spam and non-spam emails. Presumably, the spam bag will contain trigger words (such as cheap, buy, stock) more frequently than the bag with words from non-spam emails. A classifier will be trained using the two bags-of-words and learn to differentiate trigger words from regular words. After the training, the classifier will analyze the words in new unseen messages, and predict the probability that these words belong to the spam or non-spam bag-of-words."]},{"cell_type":"markdown","metadata":{"id":"4qBsT78wDv2O"},"source":["<img src=\"images/bag_of_words.png\" width=\"600\">\n","\n","*Figure: Bag-of-words representation.*"]},{"cell_type":"markdown","metadata":{"id":"GGmSvyoAEvHy"},"source":["The early applications of machine learning in NLP relied on bag-of-words models. Modern applications, especially those related to Large Language Models, rely predominantly on sequence models."]},{"cell_type":"markdown","metadata":{"id":"iJ2D6GsMsAiB"},"source":["## 18.5 Sequence Models Approach <a name='18.5-sequence-models-approach'></a>"]},{"cell_type":"markdown","metadata":{"id":"R-6vSeeksNq-"},"source":["**Sequence models** process the entire text sequence at once, which allows preserving the order of words in the input text. Typical implementation of sequence models includes representing the words in text data with integer indices, mapping the integers to vector representations, and passing the vectors to a machine learning model, where the layers in the model will account for the ordering of input vectors.\n","\n","The input vectors to sequence models can be in the form of:\n","\n","- One-hot word vector representation, or\n","- Word embeddings representation."]},{"cell_type":"markdown","metadata":{"id":"6fGeH8L8t67N"},"source":["### One-Hot Word Vector Representation"]},{"cell_type":"markdown","metadata":{"id":"SFjNamLou4pe"},"source":["**One-hot word vector** representation is similar to encoding categorical features with one-hot encoding matrix. That is, the index for each word is converted to one-hot vector, having `1 (hot)` for that word and `0 (cold)` for all other words. An example is shown in the left-hand figure, where we created a zero vector with length of 4, and assigned 1 for the index that corresponds to  every word. Another example is shown in the right-hand figure.\n","\n","<img src=\"images/one_hot_encodding.png\" width=\"800\">\n","\n","*Figure: One-hot word vector encoding.*"]},{"cell_type":"markdown","metadata":{"id":"jGcbgsmjyho6"},"source":["One-hot word vector representation is not an efficient way to represent text, because for large text datasets the input vectors can become quite large. For instance, a training set with 20,000 words will need to use one-hot vectors of size 20,000 to represent each word, and this results in slow training, as well as this type of word representation takes a lot of memory space.\n","\n","Using word embeddings is more efficient, since the vectors for word representation are much smaller than the size of the vocabulary, and more importantly, embedding vectors can capture important semantic meaning of the words. Hence, most modern NLP models rely on word embeddings for representing words in text."]},{"cell_type":"markdown","metadata":{"id":"7A4VE9IqyAFd"},"source":["### 18.5.1 Word Embeddings <a name='18.5.1-word-embeddings'></a>"]},{"cell_type":"markdown","metadata":{"id":"rI04lGrByAZH"},"source":["**Word embeddings** representation is used to convert each word into a vector (also referred to as *embedding vector*), in such as way that the embedding vectors of words that have similar semantic meaning have close spatial positions in the embeddings space.\n","\n","The embeddings space consists of the set of vectors, where each word in the vocabulary is represented with one vector. For calculating the distance between the vectors in the embeddings space, typically cosine similarity is used as a distance metric. For two vectors $u$ and $v$, *cosine similarity* is calculated as the dot (scalar, inner) product of the vectors divided by the norm of the vectors, i.e., $\\dfrac{u\\cdot v}{||u||\\cdot ||v||}$.\n","\n","Typical vectors for representing word embeddings have between 256 to 1,024 dimensions. For instance, the following figure shows the embedding vector for the word 'work'. The embedding vector has many values, and each value represents some aspect of the meaning of that word.\n","\n","<img src=\"images/embedding_vector.png\" width=\"700\">\n","\n","*Figure: Embedding vector for the word 'work'.* Source: [link](https://ig.ft.com/generative-ai/)\n","\n","\n","The embedding vectors of words that have similar meanings are also similar. In the following figure, we can see that the embedding vectors of the words 'football' and 'soccer' are more similar to each other, than the embedding vectors of the words 'sea' or 'we'.\n","\n","<img src=\"images/embeddings_comparison.png\" width=\"500\">\n","\n","*Figure: Embedding vectors for words with similar meanings are also similar.* Source: [link](https://ig.ft.com/generative-ai/)\n","\n","\n","A simple example of word embeddings space is shown below, where similar words are positioned closer to each other. Therefore, the spatial distance between the vectors is dependent on the semantic meaning of the words.\n","\n","<img src=\"images/word_embeddings.png\" width=\"500\">\n","\n","*Figure: Word embeddings space.* Source: [link](https://ig.ft.com/generative-ai/)\n","\n","\n","Two popular methods for generating word embeddings are *word2vec* and *Glove*. These methods use Neural Networks to learn embedding vectors from a large corpus of text. The resulting vectors learned by these techniques can be imported as pretrained word embeddings and applied to downstream tasks with smaller training datasets.\n","\n","For example, the website  [Embedding Projector](http://projector.tensorflow.org) provides visualizations of word embeddings, and for an entered word displays other words that are adjacent in the embeddings space.\n","\n","To demonstrate the use of word embeddings with TensorFlow-Keras, we will implement it for a sentiment analysis task, to classify movie reviews using the IMDB Reviews dataset."]},{"cell_type":"markdown","metadata":{"id":"OpTdRWWgzs0p"},"source":["#### Loading the IMDB Reviews Dataset"]},{"cell_type":"markdown","metadata":{"id":"TMO8oqNCzwsL"},"source":["IMDB Reviews Dataset can be downloaded from the built-in datasets in TensorFlow-Keras.  There are 25,000 samples of movie reviews for training and 25,000 samples for validation. Setting `max_features` to 20,000 means we are only considering the first 20,000 words and the rest of the words will have the out-of-vocabulary token. Each movie review has a positive or negative label.\n","\n","The training and validation datasets will be loaded as lists with 25,000 elements."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"hvfoy6M82QI9","tags":[],"executionInfo":{"status":"ok","timestamp":1761958638450,"user_tz":360,"elapsed":3087,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["max_features = 20000\n","\n","(train_data, train_labels), (val_data, val_labels) = tf.keras.datasets.imdb.load_data(num_words=max_features)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74,"status":"ok","timestamp":1761958638472,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"xx-Hxo8m_oc4","outputId":"201e0ae8-f6da-4c02-ae1e-3df85d472d20","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["25000\n","25000\n"]}],"source":["print(len(train_data))\n","print(len(val_data))"]},{"cell_type":"markdown","metadata":{"id":"-DmqzQ-eAU1e"},"source":["Displayed below is one example of a movie review. It is a list of indices, it contains 141 words, and as we can see the words in the dataset are already converted to integer indices."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57,"status":"ok","timestamp":1761958638472,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"MXKWTG9Pai4C","outputId":"9837b405-f4db-4d86-ebf4-2a665aa0c53f","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of words in the third review 141\n","[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n"]}],"source":["# Display the third movie review\n","print('Number of words in the third review', len(train_data[2]))\n","print(train_data[2])"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1761958638483,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"Prz3AruWmAb5","outputId":"7ae5bc0c-64f6-4d8c-dd14-dfb8fe0cbac5","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0])"]},"metadata":{},"execution_count":18}],"source":["# Display the first 10 train labels\n","train_labels[:10]"]},{"cell_type":"markdown","metadata":{"id":"xjd3rQSVaMhR"},"source":["#### Preparing the Dataset"]},{"cell_type":"markdown","metadata":{"id":"qgqbiz6p2liy"},"source":["Let's pad the data using the `pad_sequences` function in TensorFlow-Keras. Setting `maxlen` indicates to use the first 200 words in each movie review, and ignore the rest. Most movie reviews in the dataset are shorter than 200 words, however for those that are longer than 200 words some information will be lost. That is a tradeoff between computational expense and model performance.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"R85gam_ua92K","tags":[],"executionInfo":{"status":"ok","timestamp":1761958639066,"user_tz":360,"elapsed":617,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["train_data = pad_sequences(train_data, maxlen=200)\n","val_data = pad_sequences(val_data, maxlen=200)"]},{"cell_type":"code","source":["# Print the shape of the padded train dataset\n","print('Shape of the train data:', train_data.shape)"],"metadata":{"id":"XWKlUviXcQqw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761958639073,"user_tz":360,"elapsed":6,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"2da5e9da-1c32-42ae-826e-02e2b16e3bfe"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the train data: (25000, 200)\n"]}]},{"cell_type":"markdown","source":["We can see in the next cell that for the third review, which has a length of 141 words, the first 59 words are now 0, and the length is 200."],"metadata":{"id":"4nnhf58jcnhu"}},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1761958639110,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"KqHhsOm5BhtH","outputId":"a23ea51d-3c86-49d1-bc32-55ebfce37e81","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    1   14   47    8   30   31    7    4  249  108    7\n","    4 5974   54   61  369   13   71  149   14   22  112    4 2401  311\n","   12   16 3711   33   75   43 1829  296    4   86  320   35  534   19\n","  263 4821 1301    4 1873   33   89   78   12   66   16    4  360    7\n","    4   58  316  334   11    4 1716   43  645  662    8  257   85 1200\n","   42 1228 2578   83   68 3912   15   36  165 1539  278   36   69    2\n","  780    8  106   14 6905 1338   18    6   22   12  215   28  610   40\n","    6   87  326   23 2300   21   23   22   12  272   40   57   31   11\n","    4   22   47    6 2307   51    9  170   23  595  116  595 1352   13\n","  191   79  638   89    2   14    9    8  106  607  624   35  534    6\n","  227    7  129  113]\n"]}],"source":["# Display the third movie review\n","print(train_data[2])"]},{"cell_type":"markdown","metadata":{"id":"qlyWGixEViI_"},"source":["#### Embedding Layer in TensorFlow-Keras\n","\n","TensorFlow-Keras has `Embedding` layer, which we will use to project the input tokens into vectors in an embedding space. The `Embedding` layer requires at the minimum to specify the number of tokens in the data sequences, and the dimensionality of the vectors in the embeddings space. The layer takes  integer indices as inputs, and outputs embedding vectors. It can be considered as a look-up table, which maps each integer index to an embedding vector.\n","\n","To understand how the Embedding layer works, let's consider a dataset with the maximum number of words set to 100, and our aim is to represent the words with 5-dimensional vectors. In the cell below, the Embedding layer assigned random values to the list of indices 1, 2, and 3, and we can see that to each index a 5-dimensional vector is assigned. However, the embedding vectors are trainable, and when we include the Embedding layer in a model, as we train the model, words that are similar will get closer in the embeddings space."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"6xIZ5fkxQrB8","tags":[],"executionInfo":{"status":"ok","timestamp":1761958639113,"user_tz":360,"elapsed":1,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["from tensorflow.keras.layers import Embedding\n","\n","# Embedding layer: represent a dataset with a vocabulary of 100 words with 5 dimensional vectors\n","embedding_layer = Embedding(input_dim=100, output_dim=5)"]},{"cell_type":"code","source":["# Input layer: list [1, 2, 3] coverted to TensorFlow tensor\n","input_layer = tf.convert_to_tensor([1, 2, 3])\n","\n","# Inspect the input_layer\n","input_layer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rser1qdj_RnA","executionInfo":{"status":"ok","timestamp":1761958639308,"user_tz":360,"elapsed":148,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6ed84f07-4e56-4660-b848-476da4f2a435"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# Apply the Embedding layer\n","output_layer = embedding_layer(input_layer)\n","\n","# Print the embedding vectors\n","output_layer.numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0NAJvq7_ldN","executionInfo":{"status":"ok","timestamp":1761958641033,"user_tz":360,"elapsed":1724,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"0016797b-c260-48ea-f9f3-64c7fc420b23"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.04237379,  0.04802616, -0.04056418, -0.04120548, -0.00101371],\n","       [ 0.02238672,  0.04152756, -0.02390041,  0.03987378,  0.02417493],\n","       [-0.00587286,  0.02061386, -0.00715909,  0.0491767 , -0.02104199]],\n","      dtype=float32)"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"9SA5YTIfQ1_1"},"source":["#### Define, Compile, and Train the Model\n","\n","Next, we will define a model that uses an `Embedding` layer to project the words in input sequences into 8-dimensional vectors. These vectors will be further processed through dense layers, and the last layer will predict the label of movie reviews. There are two labels: positive and negative movie review, therefore this is a binary classification problem."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"PnBvxyc4UaIA","tags":[],"executionInfo":{"status":"ok","timestamp":1761958641036,"user_tz":360,"elapsed":2,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n","from tensorflow.keras.models import Model\n","\n","max_features = 20000   # Vocabulary size\n","embedding_dim = 8      # Embedding dimension\n","\n","# Define the layers in the model\n","input_layer = Input(shape=(200,))\n","embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim)(input_layer)\n","flatten_layer = Flatten()(embedding_layer)\n","dense_layer = Dense(32, activation='relu')(flatten_layer)\n","dropout_layer = Dropout(0.5)(dense_layer)\n","output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n","\n","# Create the model with inputs and outputs\n","model = Model(inputs=input_layer, outputs=output_layer)"]},{"cell_type":"markdown","metadata":{"id":"m3w5rY1jWc07"},"source":["We will compile the model with `binary_crossentropy` loss (two labels: positive and negative review) and `adam` optimizer."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Pa9wGfvlWYtB","tags":[],"executionInfo":{"status":"ok","timestamp":1761958641038,"user_tz":360,"elapsed":1,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"l4lU4IG4W1K6"},"source":["Before training the model, we can see the model summary."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1761958641071,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"LNJhXZIMWzmZ","outputId":"cd3e4d7c-1e87-44b4-a691-3c89de85a0f5","tags":[]},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │       \u001b[38;5;34m160,000\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m51,232\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,000</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,232</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m211,265\u001b[0m (825.25 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,265</span> (825.25 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m211,265\u001b[0m (825.25 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,265</span> (825.25 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["model.summary()"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23855,"status":"ok","timestamp":1761958664927,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"N_um8S3dW4-j","outputId":"dc9fc4ad-a97a-482a-d706-d40d80052b5b","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.6360 - loss: 0.5894 - val_accuracy: 0.8681 - val_loss: 0.3075\n","Epoch 2/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9160 - loss: 0.2284 - val_accuracy: 0.8614 - val_loss: 0.3275\n","Epoch 3/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9686 - loss: 0.1046 - val_accuracy: 0.8553 - val_loss: 0.3994\n","Epoch 4/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9888 - loss: 0.0461 - val_accuracy: 0.8473 - val_loss: 0.5096\n","Epoch 5/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9938 - loss: 0.0237 - val_accuracy: 0.8492 - val_loss: 0.6085\n"]}],"source":["history = model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=5)"]},{"cell_type":"markdown","metadata":{"id":"R45E02hfskwZ"},"source":["### 18.5.2 Using TextVectorization Layer <a name='18.5.2-using-textvectorization-layer'></a>"]},{"cell_type":"markdown","metadata":{"id":"BaIlzPVGh54c"},"source":["TensorFlow-Keras also provides another way to preprocess text by using a `TextVectorization` layer, which can be included directly as a layer in Neural Networks to tokenize and vectorize text.\n","\n","This layer performs the following preprocessing steps:\n","\n","* Standardize text by removing punctuations and lowering the text case.\n","* Split sentences into individual tokens.\n","* Convert the tokens into a numerical representation.\n","\n","The functionality of `TextVectorization` is similar to the `Tokenizer` function in TensorFlow-Keras. However, unlike the `Tokenizer` function which is commonly used for preprocessing text before feeding it into a model, the `TextVectorization` layer can be directly integrated into a TensorFlow-Keras model, making it more suitable for end-to-end model pipelines.\n","\n","The arguments in `TextVectorization` layer are:\n","\n","- *max_tokens*: maximum number of tokens in the vocabulary, where vocabulary is comprised of unique text units (words) in the data. E.g., if `max_tokens=1000` the layer will only consider the 1000 most frequent tokens from the input text data when building the vocabulary.\n","- *standardize*: denotes the standardization specifics to be applied to input data; by default, it is `lower_and_strip_punctuation` meaning to convert to lowercase and remove punctuations.\n","- *split*: denotes what will be considered while splitting the input text; by default it is whitespace `\" \"`.\n","- *output_sequence_length*: the length to which the sequences will be padded (if shorter than the length) or truncated (if longer than the length)."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"QfkdnmQkrSYw","tags":[],"executionInfo":{"status":"ok","timestamp":1761958664961,"user_tz":360,"elapsed":3,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["from tensorflow.keras.layers import TextVectorization\n","\n","text_vect_layer = TextVectorization(max_tokens=1000, output_sequence_length=10)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"7eYMFefqscFA","tags":[],"executionInfo":{"status":"ok","timestamp":1761958664984,"user_tz":360,"elapsed":21,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["# Sample sentences\n","sentences = ['TensorFlow is a deep learning library!',\n","             'Is TensorFlow powered by Keras API?']"]},{"cell_type":"markdown","metadata":{"id":"aaOetzBexcPA"},"source":["The `adapt()` method is used to fit the sentences to the TextVectorization layer. The `adapt()` method will create a vocabulary of the most frequent tokens, and it will create a mapping from tokens to integer indices that will be used later for converting text into a numerical representation."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"GdYJMqYGuf4L","tags":[],"executionInfo":{"status":"ok","timestamp":1761958665013,"user_tz":360,"elapsed":27,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["text_vect_layer.adapt(sentences)"]},{"cell_type":"code","source":["# Vectorize the above sentences and display the output\n","vectorized_sentences = text_vect_layer(sentences)\n","print(vectorized_sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSdGV0oNDNYQ","executionInfo":{"status":"ok","timestamp":1761958665844,"user_tz":360,"elapsed":834,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"64e5d481-20ab-4aa7-a8c0-3f254fe22d00"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ 2  3 11  8  6  5  0  0  0  0]\n"," [ 3  2  4  9  7 10  0  0  0  0]], shape=(2, 10), dtype=int64)\n"]}]},{"cell_type":"code","source":["# Get the vocabulary from the TextVectorization layer\n","vocab = text_vect_layer.get_vocabulary()\n","\n","# Print each word and its corresponding index\n","for i, word in enumerate(vocab):\n","    print(f\"{word}: {i}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RobsgppoTWtU","executionInfo":{"status":"ok","timestamp":1761958665872,"user_tz":360,"elapsed":13,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"84bc49ca-acf6-447f-b713-30e937ec3584"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":[": 0\n","[UNK]: 1\n","tensorflow: 2\n","is: 3\n","powered: 4\n","library: 5\n","learning: 6\n","keras: 7\n","deep: 8\n","by: 9\n","api: 10\n","a: 11\n"]}]},{"cell_type":"markdown","metadata":{"id":"UXT4hQTQxiZP"},"source":["Let's pass a sample sentence to inspect the output."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"WyQWnWL-xDV0","tags":[],"executionInfo":{"status":"ok","timestamp":1761958665874,"user_tz":360,"elapsed":8,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["sample_sentence = 'Tensorflow is a machine learning framework!'\n","\n","vectorized_sentence = text_vect_layer([sample_sentence])"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1761958665876,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"1bnFLF8xZTC0","outputId":"259eab6b-fa5e-4e65-cda0-721f09e07815","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Orginal sentence: Tensorflow is a machine learning framework!\n","Vectorized sentence: tf.Tensor([[ 2  3 11  1  6  1  0  0  0  0]], shape=(1, 10), dtype=int64)\n"]}],"source":["print('Orginal sentence:', sample_sentence)\n","print('Vectorized sentence:', vectorized_sentence)"]},{"cell_type":"markdown","metadata":{"id":"No7WUUUhylIG"},"source":["Since the words `'machine'` and `'framework'` were not part of the `sentences` that we passed to the layer and hence are not in the vocabulary, they are both represented by `1` in the vectorized output, since the index 1 is reserved for words that are out of vocabulary(`oov_token`).\n","\n","The output is padded with 0, and the length of the output sequence size is 10.\n","\n","The `TextVectorization` layer performs all required text preprocessing steps at once, and another advantage of this layer is that it can be used inside a model."]},{"cell_type":"markdown","metadata":{"id":"ztGDcuYujNzp"},"source":["### 18.5.3 Sequence Modeling with Recurrent Neural Networks <a name='18.5.3-sequence-modeling-with-recurrent-neural-networks'></a>"]},{"cell_type":"markdown","metadata":{"id":"b6ABwz_Am75n"},"source":["**Recurrent Neural Networks (RNN)** is a  neural network architecture that is designed for handling sequential data. Examples of sequential data are time-series, texts (sequence of words or characters), audio (sequence of sound waves), video (sequences of images), genetic sequences (DNA sequence), etc.\n","\n","Working with sequential data requires to preserve the sequence of the information flow in the data. For example, given the sentence `Today, I took my cat for a [....]`, to predict the next word, there should be a way to capture and preserve the flow from the beginning to the end of the sequence.\n","\n","In conventional feedforward networks (such as networks composed of fully-connected or convolutional layers), the information flows from the input layer to the output layer. Conversely, in RNNs, there is a feedback loop at each time step, which creates the *recurrence*. This is shown in the next figure, where at each time step of the RNN model, an input (e.g., word) is processed, then in the next step the succeeding word is processed based on the information from the previous word, etc. This way, the network can learn dependencies between words that are not adjacent.\n","\n","<img src=\"images/rnn.png\" width=\"600\">\n","\n","*Figure: Recurrent Neural Network.*\n"]},{"cell_type":"markdown","metadata":{"id":"2xBfAgEAzEm2"},"source":["There are three major types of RNN layers: conventional (a.k.a. basic, simple, vanilla) RNN, LSTM (Long Short-Term Memory), and GRU (Gated Recurrent Units). They are implemented in TensorFlow-Keras and PyTorch, and can be conveniently imported and used for creating models. In TensorFlow-Keras, the conventional (basic) RNN is called SimpleRNN, and LSTM and GRU are called as they are written.\n","\n","While SimpleRNN has difficulty in handling long sequences, LSTM and GRU have the ability to store and preserve long-term dependencies over many time steps. Consequently, SimpleRNNs are rarely used at present.\n","\n","Both LSTM and GRU layers use multiple gates to control to flow of information between the time steps. For instance, LSTM layers include an input gate and an output gate to control the input and output information for each time step, a forget gate that removes irrelevant information, and a memory cell that saves important information.\n","\n","Next, we will apply an RNN model with LSTM layers for classification of text data."]},{"cell_type":"markdown","metadata":{"id":"OyjIIjZzRlev"},"source":["#### Loading the Data"]},{"cell_type":"markdown","metadata":{"id":"0DAL7PtzV7gg"},"source":["We are going to use the `ag_news_subset` dataset that is available in TensorFlow datasets. AG is a collection of news articles gathered from more than 2,000 news sources. The news articles are classified into 4 classes: World(0), Sports(1), Business(2), and Sci/Tech(3). The total number of training samples is 120,000 and testing 7,600.\n","\n","Let's get the dataset from TensorFlow datasets. In the load function, `with_info=True` will return various information about the dataset (as shown in the next cells), and `as_supervised=True` indicates that the data will be loaded as 2-element tuples consisting of (input, target) pairs."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"U8fnaQT4CE9G","tags":[],"executionInfo":{"status":"ok","timestamp":1761958665981,"user_tz":360,"elapsed":106,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["import tensorflow_datasets as tfds\n","import pandas as pd"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"6JIKIZDRQwVV","executionInfo":{"status":"ok","timestamp":1761958666492,"user_tz":360,"elapsed":508,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["(train_data, val_data), info = tfds.load('ag_news_subset:1.0.0', #version 1.0.0\n","                                         split=['train', 'test'],\n","                                         with_info=True,\n","                                         as_supervised=True)"]},{"cell_type":"markdown","metadata":{"id":"378IQW1eSVlz"},"source":["We can use `info` to check basic information about the dataset."]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1761958666518,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"03IBh4d1S1eJ","outputId":"35238175-648a-4b07-934e-a849280d8341"},"outputs":[{"output_type":"stream","name":"stdout","text":["['World', 'Sports', 'Business', 'Sci/Tech']\n"]}],"source":["# Displaying the classes\n","class_names = info.features['label'].names\n","print(class_names)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1761958666523,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"uUzqSrGwY7dU","outputId":"e47b4c25-52fc-4f54-cf14-bdc6a836b18c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training samples: 120000\n","Number of validation samples: 7600\n"]}],"source":["print('Number of training samples:', info.splits['train'].num_examples)\n","print('Number of validation samples:', info.splits['test'].num_examples)"]},{"cell_type":"markdown","metadata":{"id":"_AGay5wWZ_xs"},"source":["We can use `tfds.as_dataframe` to display the first 10 news articles as  Pandas DataFrame.\n"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":86,"status":"ok","timestamp":1761958666621,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"kGj0TRaXaHG1","outputId":"eb7fb7cc-eb6d-4cdc-efdb-2a98a1e9a006"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                         description  label\n","0  b'AMD #39;s new dual-core Opteron chip is desi...      3\n","1  b'Reuters - Major League Baseball\\\\Monday anno...      1\n","2  b'President Bush #39;s  quot;revenue-neutral q...      2\n","3  b'Britain will run out of leading scientists u...      3\n","4  b'London, England (Sports Network) - England m...      1\n","5  b'TOKYO - Sony Corp. is banking on the \\\\$3 bi...      0\n","6  b'Giant pandas may well prefer bamboo to lapto...      3\n","7  b'VILNIUS, Lithuania - Lithuania #39;s main pa...      0\n","8  b'Witnesses in the trial of a US soldier charg...      0\n","9  b'Dan Olsen of Ponte Vedra Beach, Fla., shot a...      1"],"text/html":["\n","  <div id=\"df-96fe7c40-810f-45df-a4f5-34d16714ee30\" class=\"colab-df-container\">\n","    <style type=\"text/css\">\n","</style>\n","<table id=\"T_905b3\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_905b3_level0_col0\" class=\"col_heading level0 col0\" >description</th>\n","      <th id=\"T_905b3_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_905b3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_905b3_row0_col0\" class=\"data row0 col0\" >AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.</td>\n","      <td id=\"T_905b3_row0_col1\" class=\"data row0 col1\" >3 (Sci/Tech)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_905b3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_905b3_row1_col0\" class=\"data row1 col0\" >Reuters - Major League Baseball\\Monday announced a decision on the appeal filed by Chicago Cubs\\pitcher Kerry Wood regarding a suspension stemming from an\\incident earlier this season.</td>\n","      <td id=\"T_905b3_row1_col1\" class=\"data row1 col1\" >1 (Sports)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_905b3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_905b3_row2_col0\" class=\"data row2 col0\" >President Bush #39;s quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.</td>\n","      <td id=\"T_905b3_row2_col1\" class=\"data row2 col1\" >2 (Business)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_905b3_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_905b3_row3_col0\" class=\"data row3 col0\" >Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.</td>\n","      <td id=\"T_905b3_row3_col1\" class=\"data row3 col1\" >3 (Sci/Tech)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_905b3_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_905b3_row4_col0\" class=\"data row4 col0\" >London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.</td>\n","      <td id=\"T_905b3_row4_col1\" class=\"data row4 col1\" >1 (Sports)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_905b3_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n","      <td id=\"T_905b3_row5_col0\" class=\"data row5 col0\" >TOKYO - Sony Corp. is banking on the \\$3 billion deal to acquire Hollywood studio Metro-Goldwyn-Mayer Inc...</td>\n","      <td id=\"T_905b3_row5_col1\" class=\"data row5 col1\" >0 (World)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_905b3_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n","      <td id=\"T_905b3_row6_col0\" class=\"data row6 col0\" >Giant pandas may well prefer bamboo to laptops, but wireless technology is helping researchers in China in their efforts to protect the engandered animals living in the remote Wolong Nature Reserve.</td>\n","      <td id=\"T_905b3_row6_col1\" class=\"data row6 col1\" >3 (Sci/Tech)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_905b3_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n","      <td id=\"T_905b3_row7_col0\" class=\"data row7 col0\" >VILNIUS, Lithuania - Lithuania #39;s main parties formed an alliance to try to keep a Russian-born tycoon and his populist promises out of the government in Sunday #39;s second round of parliamentary elections in this Baltic country.</td>\n","      <td id=\"T_905b3_row7_col1\" class=\"data row7 col1\" >0 (World)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_905b3_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n","      <td id=\"T_905b3_row8_col0\" class=\"data row8 col0\" >Witnesses in the trial of a US soldier charged with abusing prisoners at Abu Ghraib have told the court that the CIA sometimes directed abuse and orders were received from military command to toughen interrogations.</td>\n","      <td id=\"T_905b3_row8_col1\" class=\"data row8 col1\" >0 (World)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_905b3_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n","      <td id=\"T_905b3_row9_col0\" class=\"data row9 col0\" >Dan Olsen of Ponte Vedra Beach, Fla., shot a 7-under 65 Thursday to take a one-shot lead after two rounds of the PGA Tour qualifying tournament.</td>\n","      <td id=\"T_905b3_row9_col1\" class=\"data row9 col1\" >1 (Sports)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96fe7c40-810f-45df-a4f5-34d16714ee30')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-96fe7c40-810f-45df-a4f5-34d16714ee30 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-96fe7c40-810f-45df-a4f5-34d16714ee30');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-6533152e-2984-4224-b5f3-209f075e7de9\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6533152e-2984-4224-b5f3-209f075e7de9')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-6533152e-2984-4224-b5f3-209f075e7de9 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_174f8eb1-3f0e-44c7-b322-bef9628d6032\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('news_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_174f8eb1-3f0e-44c7-b322-bef9628d6032 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('news_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"news_df","summary":"{\n  \"name\": \"news_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"b'Witnesses in the trial of a US soldier charged with abusing prisoners at Abu Ghraib have told the court that the CIA sometimes directed abuse and orders were received from military command to toughen interrogations.'\",\n          \"b'Reuters - Major League Baseball\\\\\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\\\\\incident earlier this season.'\",\n          \"b'TOKYO - Sony Corp. is banking on the \\\\\\\\$3 billion deal to acquire Hollywood studio Metro-Goldwyn-Mayer Inc...'\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":40}],"source":["news_df = tfds.as_dataframe(train_data.take(10), info)\n","\n","news_df"]},{"cell_type":"markdown","metadata":{"id":"F-jPA6TEd3sw"},"source":["Now that we understand the data, let's prepare it before we can use LSTMs to classify the news.  "]},{"cell_type":"markdown","metadata":{"id":"EKjyyw21bRzM"},"source":["#### Preparing the Data"]},{"cell_type":"markdown","metadata":{"id":"w-KMfmo0dOWc"},"source":["Note again that the variables `train_data` and `val_data` contain pairs of input text sequences and labels. First, we will separate the input text sequences and labels, and we will convert them to TensorFlow tensors with the `convert_to_tensor` function."]},{"cell_type":"code","source":["# Function to load the data as TensorFlow tensors\n","def load_data(dataset):\n","    inputs, labels = [], []\n","    for input_text, label in tfds.as_numpy(dataset):\n","        inputs.append(input_text)\n","        labels.append(label)\n","    return tf.convert_to_tensor(inputs, dtype=tf.string), tf.convert_to_tensor(labels)\n","\n","# Load training and validation data directly as tensors\n","train_inputs, train_labels = load_data(train_data)\n","val_inputs, val_labels = load_data(val_data)"],"metadata":{"id":"qMAQ2A3yVb9x","executionInfo":{"status":"ok","timestamp":1761958711661,"user_tz":360,"elapsed":45039,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["Let's check the shape of the train and validation inputs and labels."],"metadata":{"id":"KVQV_3hfWZV-"}},{"cell_type":"code","source":["print(\"Training inputs shape:\", train_inputs.shape)\n","print(\"Training labels shape:\", train_labels.shape)\n","print(\"Validation inputs shape:\", val_inputs.shape)\n","print(\"Validation labels shape:\", val_labels.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qCx9Nc-kWXFB","executionInfo":{"status":"ok","timestamp":1761958711683,"user_tz":360,"elapsed":6,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"8079e376-3d98-48db-8148-30624cd106f9"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Training inputs shape: (120000,)\n","Training labels shape: (120000,)\n","Validation inputs shape: (7600,)\n","Validation labels shape: (7600,)\n"]}]},{"cell_type":"markdown","source":["In the next cell, we can observe that the data type of the inputs is `tf.string`."],"metadata":{"id":"1M1tilyTXHrG"}},{"cell_type":"code","source":["train_inputs.dtype"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rErWtnwRWzjk","executionInfo":{"status":"ok","timestamp":1761958711689,"user_tz":360,"elapsed":5,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"702fc136-ad0f-4aae-a579-275d641b8371"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tf.string"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"MEIRO6ZlphLl"},"source":["To convert the text data into tokens, we will use the TextVectorizer layer in TensorFlow-Keras. Afterward, we will apply the `adapt()` method to preprocess the training data."]},{"cell_type":"code","execution_count":44,"metadata":{"id":"tu8u1i612_sd","executionInfo":{"status":"ok","timestamp":1761958711691,"user_tz":360,"elapsed":1,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["vocab_size = 20000\n","\n","text_vect_layer = TextVectorization(max_tokens=vocab_size)"]},{"cell_type":"code","source":["text_vect_layer.adapt(train_inputs)"],"metadata":{"id":"Fy2qeJXHYGhE","executionInfo":{"status":"ok","timestamp":1761958712930,"user_tz":360,"elapsed":1238,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GfiumAc0rriT"},"source":["Let's pass two news articles to `text_vect_layer`. The vectorized sequences will be padded to the sentence with the maximum length, but if we wanted to have fixed size of padded sequences, we could set the `output_sequence_length` to another value in the layer initialization."]},{"cell_type":"code","execution_count":46,"metadata":{"id":"Hb4d3EqrrMuU","executionInfo":{"status":"ok","timestamp":1761958712933,"user_tz":360,"elapsed":1,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["sample_news = ['This weekend there is a sport match between Man U and Fc Barcelona',\n","               'Tesla has unveiled its humanoid robot that appeared dancing during the show!']"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1761958713004,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"B7cQZbSIsjhA","outputId":"13c7091b-52e0-47a0-f632-f8559396a72e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[   40,   491,   185,    16,     3,  1559,   560,   163,   362,\n","        13418,     7,  7381,  2517],\n","       [    1,    20,   878,    14,     1,  4663,    10,  1249, 11657,\n","          159,     2,   541,     0]])"]},"metadata":{},"execution_count":47}],"source":["vectorized_news = text_vect_layer(sample_news)\n","vectorized_news.numpy()"]},{"cell_type":"markdown","metadata":{"id":"QDiIP1tFtRYs"},"source":["Note that the second sentence was padded with 0. Also the words `Tesla` and `humanoid` have an index of 1 because they were not a part of the training data."]},{"cell_type":"markdown","metadata":{"id":"TUNqJkjpusFA"},"source":["#### Creating and Training the Model"]},{"cell_type":"markdown","metadata":{"id":"e7Vobpziu4Zw"},"source":["We are going to create a TensorFlow-Keras model that takes the sequences of text as input and outputs the class of the news articles.\n","\n","The model has the following layers:\n","\n","* `Input layer` that takes input text sequences having `tf.string` type.\n","* `TextVectorization layer` for converting input texts into tokens.\n","* `Embedding layer` for representing the tokens with trainable embedding vectors. Because the embedding vectors are trainable, words that have similar semantic meaning will be represented by vectors that are close in the embeddings space.\n","* `LSTM layer` for processing the sequences. The layer is wrapped into a Bidirectional layer, which will process the sequences from both directions (forward and backward), i.e., one LSTM layer will process the sequences forward, another layer will process the sequences backward, and the outputs of the two LSTMs will be combined.\n","* `Dense layer` for classification purpose."]},{"cell_type":"code","execution_count":48,"metadata":{"id":"ApxFc7DouIAX","executionInfo":{"status":"ok","timestamp":1761958713155,"user_tz":360,"elapsed":132,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["from tensorflow.keras.layers import Bidirectional, LSTM\n","\n","embedding_dim = 64\n","\n","# Define model layers\n","inputs = Input(shape=(1,), dtype=tf.string)\n","x = text_vect_layer(inputs)\n","x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(x)\n","x = Bidirectional(LSTM(64))(x)\n","x = Dense(64, activation='relu')(x)\n","outputs = Dense(4, activation='softmax')(x)\n","\n","# Define the model\n","model = Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"E3dtgRIH1BL_","executionInfo":{"status":"ok","timestamp":1761958713161,"user_tz":360,"elapsed":3,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"outputs":[],"source":["# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125843,"status":"ok","timestamp":1761958839002,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"AbJ0GXUi1ToF","outputId":"2ee0c4d0-40ba-4fe2-ddb8-b623eccc6441"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 11ms/step - accuracy: 0.8075 - loss: 0.5057 - val_accuracy: 0.9111 - val_loss: 0.2633\n","Epoch 2/3\n","\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9314 - loss: 0.2024 - val_accuracy: 0.9139 - val_loss: 0.2543\n","Epoch 3/3\n","\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 11ms/step - accuracy: 0.9490 - loss: 0.1498 - val_accuracy: 0.9093 - val_loss: 0.2770\n"]}],"source":["# Train the model\n","history = model.fit(train_inputs, train_labels, validation_data=(val_inputs, val_labels), epochs=3, batch_size=32)"]},{"cell_type":"markdown","source":["Let's evaluate the model on news articles."],"metadata":{"id":"vCGgx0-wbLLK"}},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1761958839313,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"d7ZdyxK77qfW","outputId":"ed288a12-e4c6-421c-9a8b-e95521ac59f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n","[[0.01865892 0.03239412 0.26387298 0.68507403]]\n"]}],"source":["# Predicting the class of new news articles\n","sample_news_1 = tf.convert_to_tensor(['The self driving car company Tesla has unveiled its humanoid robot at a recent event!'])\n","\n","# make predictions on the sample_news 1\n","predictions_1 = model.predict(sample_news_1)\n","print(predictions_1)"]},{"cell_type":"markdown","metadata":{"id":"ZdAqAXUzN2Xe"},"source":["The model correctly predicted that the news article is related to tech or science."]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1761958839313,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"XOzT9yra8K1o","outputId":"3bbc325c-9de9-44da-8b11-88cda3025b2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: 3\n","Predicted class name: Sci/Tech\n"]}],"source":["# find the index of the predicted class\n","predicted_class_1 = np.argmax(predictions_1)\n","\n","print('Predicted class:', predicted_class_1)\n","print('Predicted class name:', class_names[predicted_class_1])"]},{"cell_type":"markdown","metadata":{"id":"XKxv86w5OWhG"},"source":["One more example is provided in the next cell."]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96,"status":"ok","timestamp":1761958839406,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":360},"id":"NDXFamqkOHAD","outputId":"a6f91573-d473-482b-8d7f-fafedbcbc82e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","Predicted class: 1\n","Predicted class name: Sports\n"]}],"source":["# Predicting the class of a new sample\n","sample_news_2 = tf.convert_to_tensor(['This weekend there is a match between two big footbal teams in the national league'])\n","\n","predictions_2 = model.predict(sample_news_2)\n","\n","predicted_class_2 = np.argmax(predictions_2)\n","\n","print('Predicted class:', predicted_class_2)\n","print('Predicted class name:', class_names[predicted_class_2])"]},{"cell_type":"markdown","metadata":{"id":"vweobvFVe4RB"},"source":["## References <a name='references'></a>\n","\n","1. Complete Machine Learning Package, Jean de Dieu Nyandwi, available at: [https://github.com/Nyandwi/machine_learning_complete](https://github.com/Nyandwi/machine_learning_complete).\n","2. Deep Learning with Python, Francois Chollet, Second Edition, Manning Publications, 2021."]},{"cell_type":"markdown","metadata":{"id":"bDbzcXmWe4RB"},"source":["[BACK TO TOP](#top)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}