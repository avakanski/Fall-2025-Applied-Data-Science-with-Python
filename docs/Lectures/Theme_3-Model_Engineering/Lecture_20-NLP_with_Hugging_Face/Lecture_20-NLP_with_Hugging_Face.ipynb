{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JhHIQqNe4Qs"
   },
   "source": [
    "# Lecture 20 - NLP with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPG5INqg-_qn"
   },
   "source": [
    "[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_20-NLP_with_Hugging_Face/Lecture_20-NLP_with_Hugging_Face.ipynb)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_20-NLP_with_Hugging_Face/Lecture_20-NLP_with_Hugging_Face.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='top'></a>"
   ],
   "metadata": {
    "id": "zk7UKxk5P8eH"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEkmemKte4Qv"
   },
   "source": [
    "- [20.1 Introduction to Hugging Face](#20.1-introduction-to-hugging-face)\n",
    "- [20.2 Hugging Face Pipelines](#20.2-hugging-face-pipelines)\n",
    "- [20.3 Pipelines for NLP Tasks](#20.3-pipelines-for-nlp-tasks)\n",
    "    - [20.3.1 Sentiment Analysis](#20.3.1-sentiment-analysis)\n",
    "    - [20.3.2 Question Answering](#20.3.2-question-answering)\n",
    "    - [20.3.3 Machine Translation](#20.3.3-machine-translation)\n",
    "    - [20.3.4 Text Summarization](#20.3.4-text-summarization)\n",
    "    - [20.3.5 Text Generation](#20.3.5-text-generation)\n",
    "    - [20.3.6 Named Entity Recognition](#20.3.6-named-entity-recognition)\n",
    "    - [20.3.7 Zero-shot Classification](#20.3.7-zero-shot-classification)\n",
    "    - [20.3.8 Mask Filling](#20.3.8-mask-filling)\n",
    "- [20.4 Tokenizers](#20.4-tokenizers)\n",
    "- [20.5 Datasets](#20.5-datasets)    \n",
    "- [20.6 Models](#20.6-models)\n",
    "    - [20.6.1 Importing a Pretrained Model](#20.6.1-importing-a-pretrained-model)\n",
    "    - [20.6.2 Generation Strategies](#20.6.2-generation-strategies)\n",
    "    - [20.6.3 Fine-tuning a Pretrained Model](#20.6.3-fine-tuning-a-pretrained-model)\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTFgm_bJe4Qw"
   },
   "source": [
    "## 20.1 Introduction to Hugging Face <a name='20.1-introduction-to-hugging-face'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFMFGCdwiw5k"
   },
   "source": [
    "**Hugging Face** ([link](https://huggingface.co/)) is a platform for Machine Learning and AI created in 2016, with the aim to \"build, train, and deploy state of the art models powered by the reference open source in machine learning\". Since then, Hugging Face has established itself as the main source for NLP and other Machine Learning tasks, providing open access to over 2 million pre-trained models, datasets, and pertinent tools and resources. Hugging Face focuses on community-building around open-source machine learning tools and data. They also developed several [courses](https://huggingface.co/course/chapter1/1) on how to use their libraries for various tasks. Also note that while open access is provided to the core NLP libraries, Hugging Face also offers pricing options for access to AutoNLP libraries.\n",
    "\n",
    "<img src=\"images/hf_icon.png\" width=\"500\">\n",
    "\n",
    "*Figure: Hugging Face webpage.*"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hugging Face initially focused on Transformer Networks and NLP, and afterward they expanded their libraries and tools to cover machine learning models and tasks, in general. State-of-the-art Transformer Networks are very large models, and hence, training such models from scratch is expensive and not affordable for many organizations. For example, the cost of training the GPT-4 model is estimated to be over USD $100 million. Providing access to pre-trained models for transfer learning and fine-tuning to specific tasks by Hugging Face has been a significant resource.\n",
    "\n",
    "The core Hugging Face libraries include Transformer models, Tokenizers, Datasets, and Accelerate. Accelerate library enables distributed training with hardware acceleration devices, such as using multiple GPUs, or cloud accelerators with TPUs. In addition to these core libraries, Hugging Face provides various community resources, which include a platform for sharing models, code versioning, Spaces allow sharing apps developed with Hugging Face libraries and browsing apps created by others, etc.\n",
    "\n",
    "<img src=\"images/hf_libraries.png\" width=\"400\">\n",
    "\n",
    "*Figure: Hugging Face libraries.*\n",
    "\n",
    "The key characteristics of these libraries include:\n",
    "\n",
    "- Ease of use and simplicity, where downloading and using state-of-the-art NLP models can be done with a few lines of code.\n",
    "- Flexibility, since all models are implemented either using the `nn.Module` in PyTorch or `tensorflow.keras.Model` in TensorFlow-Keras, allowing for easy model integration with these popular frameworks.\n"
   ],
   "metadata": {
    "id": "FplNgLi89-__"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 20.2 Hugging Face Pipelines <a name='20.2-hugging-face-pipelines'></a>"
   ],
   "metadata": {
    "id": "aJIOCFH_GeLa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hugging Face uses **Pipelines** as an API allowing to perform a a variety of NLP tasks through the `pipeline()` method.\n",
    "\n",
    "The `pipeline()` method has the following syntax:\n",
    "\n",
    "```\n",
    "from transformers import pipeline\n",
    "\n",
    "# Pipeline to use a default model & tokenizer for a given task\n",
    "pipeline(\"<task-name>\")\n",
    "\n",
    "# Pipeline to use an existing or custom model\n",
    "pipeline(\"<task-name>\", model=\"<model_name>\")\n",
    "\n",
    "# Pipeline to use an existing or custom model and tokenizer\n",
    "pipeline('<task-name>', model='<model name>', tokenizer='<tokenizer_name>')\n",
    "```"
   ],
   "metadata": {
    "id": "2aMOkua_GomJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Among the currently available task pipelines are:\n",
    "\n",
    "- Sentiment analysis\n",
    "- Question answering\n",
    "- Translation\n",
    "- Summarization\n",
    "- Text generation\n",
    "- NER (named entity recognition)\n",
    "- Zero shot classification\n",
    "- Fill mask"
   ],
   "metadata": {
    "id": "bYGQQd46IBlT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 20.3 Pipelines for NLP Tasks <a name='20.3-pipelines-for-nlp-tasks'></a>"
   ],
   "metadata": {
    "id": "dlAygyVAIB3v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, we will examine examples of using the `pipeline(\"<task-name>\")` method with different NLP tasks. As we mentioned, if we don't provide the names for the used model and tokenizer, the pipeline will assign a default  model and tokenizer to complete the task, and it will download the model parameters and other required elements to perform the task.\n",
    "\n",
    " The Transformers library by Hugging Face is preinstalled in Google Colab. However, if you don't run this notebook in Google Colab, you will need to first install the Transformers library (`!pip install transformers`)."
   ],
   "metadata": {
    "id": "9e49MdXTICIg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.3.1 Sentiment Analysis <a name='20.3.1-sentiment-analysis'></a>"
   ],
   "metadata": {
    "id": "-MC9UBiqyPwV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first example uses `pipeline()` for sentiment analysis. We saw examples of sentiment analysis in the previous lectures, where the goal was to classify the sentiment in movie reviews text as positive or negative.\n",
    "\n",
    "When the cell is executed, the pipeline will select a default pretrained model for sentiment analysis in English, it will download the model and the related tokenizer, and it will instantiate a text classifier object. In this case, we can see in the cell output that the used default pretrained model is `distilbert-base-uncased-finetuned-sst-2-english`. The argument `device=0` in the pipeline assigns the pipeline to a GPU device if it is available. For running the model on CPU use `device=-1`."
   ],
   "metadata": {
    "id": "d_7dDPXJWlLn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", device=0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtvntkaCyHJs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661466748,
     "user_tz": 420,
     "elapsed": 50400,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "6dcaf306-64db-4e24-b8c9-be14e88d10e3"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next cell, the classifier is applied to a sentence. The output is the predicted label and the confidence score."
   ],
   "metadata": {
    "id": "8H7Isab8BWWl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "classifier(\"I fully understand what you are saying.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJ_4Y39uyHM0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661467429,
     "user_tz": 420,
     "elapsed": 679,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "5dac8987-9452-4771-cb17-ecfb36f65fcb"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996806383132935}]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pipeline allows to pass multiple sentences, and it will return a sentiment label and confidence score for each sentence."
   ],
   "metadata": {
    "id": "l32uW3Ueyevh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyDIrSpryWiL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661467490,
     "user_tz": 420,
     "elapsed": 60,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "4bb9e752-a4ff-4e6c-b241-46fe46182ad9"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598046541213989},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.3.2 Question Answering <a name='20.3.2-question-answering'></a>"
   ],
   "metadata": {
    "id": "NiJC0oCdXZAT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This pipeline answers questions using information from a given context. Such pipeline can be very useful when we are dealing with long text data and finding answers to questions in the document can take time."
   ],
   "metadata": {
    "id": "SavxGni5XZTV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "question_answerer = pipeline(\"question-answering\", device=0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5u9R3j_XmbS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661472902,
     "user_tz": 420,
     "elapsed": 5410,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "67ecf1a4-8c76-4096-a8dd-92a830052976"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can provide inputs to the pipeline as a dictionary with `question` and `context` as keys. The model extracts information from the provided context and returns a dictionary with a confidence score, start and end characters of the answer in the context, and the answer. Also note that the model does not generate new text to answer the questions, but instead it searches for the answer in the supplied context sequence."
   ],
   "metadata": {
    "id": "4alMqqSEZd01"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "input_1 = {\n",
    "    \"question\" : \"What didn't cross the street?\",\n",
    "    \"context\" : \"The animal didn't cross the street because it was too tired\",\n",
    "    }\n",
    "\n",
    "question_answerer(input_1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iP_KG6jqYZtU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661472983,
     "user_tz": 420,
     "elapsed": 77,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "9f41e4c3-ed13-457c-c41f-623a75d965f6"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.755366325378418, 'start': 0, 'end': 10, 'answer': 'The animal'}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "input_2 = {\n",
    "    \"question\" : \"Why the animal didn't cross the street?\",\n",
    "    \"context\" : \"The animal didn't cross the street because it was too wide\",\n",
    "    }\n",
    "\n",
    "question_answerer(input_2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fn9PVBqCZHaC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661473011,
     "user_tz": 420,
     "elapsed": 27,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "d081649c-707d-46df-a1f6-d08d0ec2a9da"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.6076138019561768,\n",
       " 'start': 43,\n",
       " 'end': 58,\n",
       " 'answer': 'it was too wide'}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.3.3 Machine Translation <a name='20.3.3-machine-translation'></a>\n",
    "\n",
    "For machine translation, we can provide source and target languages in the pipeline, as in the next cell where the task `\"translation_en_to_fr\"` is to translate text from English to French. Although this pipeline can work with several languages, most often, machine translation requires to specify the name of the used language model, and only for several special cases it can work by specifying only the task name."
   ],
   "metadata": {
    "id": "TfOHAyjCgTPB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "translator = pipeline(\"translation_en_to_fr\", device=0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJvDGaAqgfZN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661481805,
     "user_tz": 420,
     "elapsed": 8793,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "ae34f46e-8db7-4ac5-b2a7-4b1b27961214"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "translator(\"I am a student\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab1zUwkSjcf7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661483599,
     "user_tz": 420,
     "elapsed": 1791,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "cb67b114-b56c-4447-c841-3346608bd947"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'translation_text': 'Je suis un étudiant'}]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "translator(\"Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XV3J4sQkjfub",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661485049,
     "user_tz": 420,
     "elapsed": 1448,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "c1b4ed2f-27f7-4961-fa14-ac1e4c720b31"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'translation_text': 'Peyton Manning est devenu le premier quarterback à conduire deux équipes différentes à plusieurs Super Bowls.'}]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.3.4 Text Summarization <a name='20.3.4-text-summarization'></a>\n",
    "\n",
    "Text summarization reduces a longer text into a shorter summary."
   ],
   "metadata": {
    "id": "1Z82y4RvXZmv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "summarizer = pipeline(\"summarization\", device=0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8uuy0L_afDZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661495116,
     "user_tz": 420,
     "elapsed": 10065,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "a1585d46-f757-4990-dc08-212a5dff6e2c"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science. As a result, there\n",
    "    are declining offerings in engineering subjects dealing with infrastructure,\n",
    "    the environment, and related issues, and greater concentration on high\n",
    "    technology subjects, largely supporting increasingly complex scientific\n",
    "    developments. While the latter is important, it should not be at the expense\n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance\n",
    "    the teaching of engineering. Both China and India, respectively, graduate\n",
    "    six and eight times as many traditional engineers as does the United States.\n",
    "    Other industrial countries at minimum maintain their output, while America\n",
    "    suffers an increasingly serious decline in the number of engineering graduates\n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2XgEDYVafHp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661496630,
     "user_tz": 420,
     "elapsed": 1513,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "4d3388a9-72c8-4e90-d659-c3dfcd83d1e7"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'summary_text': ' The number of engineering graduates in the United States has declined in recent years . China and India graduate six and eight times as many traditional engineers as the U.S. does . Rapidly developing economies such as China continue to encourage and advance the teaching of engineering . There are declining offerings in engineering subjects dealing with infrastructure, infrastructure, the environment, and related issues .'}]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Specifying the `min_length` and `max_length` arguments allows to control the length of the summary."
   ],
   "metadata": {
    "id": "y3A7Zx5hdKed"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "summarizer(\n",
    "    \"\"\"\n",
    "    Flooding on the Yangtze river remains serious although water levels on parts of the river decreased\n",
    "    today, according to the state headquarters of flood control and drought relief .\n",
    "    \"\"\", min_length=8, max_length=20)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLwEfNhpbtY_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661497084,
     "user_tz": 420,
     "elapsed": 452,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "32efa830-4a84-457d-8ea9-f86fca2ee259"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'summary_text': ' Flooding on the Yangtze river remains serious although water levels on parts of the'}]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "summarizer(\n",
    "    \"\"\"BAGHDAD -- Archaeologists in northern Iraq last week unearthed 2,700-year-old rock carvings featuring war scenes and trees from the Assyrian Empire, an archaeologist said Wednesday.\n",
    "    The carvings on marble slabs were discovered by a team of experts in Mosul, Iraq’s second-largest city, who have been working to restore the site of the ancient Mashki Gate, which was bulldozed by Islamic State group militants in 2016.\n",
    "    Fadhil Mohammed, head of the restoration works, said the team was surprised by discovering “eight murals with inscriptions, decorative drawings and writings.”\n",
    "    Mashki Gate was one of the largest gates of Nineveh, an ancient Assyrian city of this part of the historic region of Mesopotamia.\n",
    "    The discovered carvings show, among other things, a fighter preparing to fire an arrow while others show palm trees.\n",
    "    \"\"\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQM9vSHgc5vB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661498647,
     "user_tz": 420,
     "elapsed": 1562,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "711f55cf-7994-4fbc-98ff-fbb1a93448cc"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'summary_text': ' The carvings on marble slabs were discovered by a team of experts in Mosul, Iraq’s second-largest city . They have been working to restore the site of the ancient Mashki Gate, which was bulldozed by Islamic State group militants in 2016 . Mashki gate was one of the largest gates of Nineveh, an ancient Assyrian city of this part of Mesopotamia .'}]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.3.5 Text Generation <a name='20.3.5-text-generation'></a>"
   ],
   "metadata": {
    "id": "Dau77YkzJriW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we will use the `\"text-generation\"` pipeline to generate text based on a provided prompt."
   ],
   "metadata": {
    "id": "TcylhfOxIcC4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "generator = pipeline(\"text-generation\", device=0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-W_ArxleIC0C",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661503986,
     "user_tz": 420,
     "elapsed": 5336,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "d1e470fa-bb57-43d5-b517-c9de9f3bcde8"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Now let’s provide a prompt text to the generator object, and the generator will continue the text. Note that text generation involves randomness, so some of the outputs will not be perfect. And admittedly, this is one of the most difficult NLP tasks."
   ],
   "metadata": {
    "id": "HJ_Jv-SMVTX2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "outputs_1 = generator(\"In this course, we will teach you how to\")\n",
    "\n",
    "print(outputs_1[0]['generated_text'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kN1bhbjQIC2F",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661508735,
     "user_tz": 420,
     "elapsed": 4748,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "25766447-efd9-4635-bc2e-1a71d143a3ee"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In this course, we will teach you how to successfully write your own program for the Raspberry Pi. In the course, you will learn about building and booting your own Linux/OSX/Windows operating system.\n",
      "\n",
      "The first three weeks of the course will cover basic programming concepts, including how to write programs for your favourite operating system, how to write your own program for a desktop computer, how to write your own program for a Raspberry Pi, how to build and install your own software, and how to install and configure your own software. In the course, we'll also cover how to write your own programs for the Raspberry Pi.\n",
      "\n",
      "We will also take you through the basics of building a Raspberry Pi. In the next three weeks, we'll cover the basics of building a Debian-based Linux/OSX/Windows operating system, which will be designed to run on a Raspberry Pi.\n",
      "\n",
      "The final three weeks of the course include the basics of how to install a Debian-based Linux/OSX/Windows operating system on the Raspberry Pi, how to compile your own program for a desktop computer, how to build your own program for a Raspberry Pi, how to load and run your program, and how to configure your own software.\n",
      "\n",
      "Before we begin, we'll take you through the basic\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "outputs_2 = generator(\"Niagara Falls is a city located in\")\n",
    "\n",
    "print(outputs_2[0]['generated_text'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gp4sTF8pMAKI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661515720,
     "user_tz": 420,
     "elapsed": 6983,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "bbefbeec-13ed-4368-8135-ebc9f1b1db16"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Niagara Falls is a city located in northeastern Ontario and a part of North America.\n",
      "\n",
      "Bridget C. Miller, a professor of geography at the University of Western Ontario, said the city has been known for many years for being a welcoming place to live.\n",
      "\n",
      "\"Because of the community's heritage, we have had a lot of people coming in here, a lot of people from all over the world coming in, and I think that has helped to increase our cultural community. Because of that, we have seen an increase in the number of people coming here,\" Miller said.\n",
      "\n",
      "She said the city is still a vibrant place to live, but it's becoming more prevalent in the city's areas.\n",
      "\n",
      "\"In fact, the city is now on the map for the first time nationally with over 200,000 residents,\" Miller said. \"In fact, we're going to have to start to attract more of those people, especially in the inner city.\"\n",
      "\n",
      "Miller said the city's growing population is a major reason for the increase in the number of people coming into the city.\n",
      "\n",
      "\"This is really the first time that we've seen more than 1 million people come into the city. The number of people coming into the city is about 3.5 million,\" Miller said.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "outputs_3 = generator(\"Niagara Falls is a famous world attractation\")\n",
    "\n",
    "print(outputs_3[0]['generated_text'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJ_ml-s1Oalb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661520325,
     "user_tz": 420,
     "elapsed": 4600,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "70b6448c-eb46-4cb3-870d-c6f7f3319d40"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Niagara Falls is a famous world attractation for a variety of reasons. It has an abundance of natural plants and wildlife, as well as a reputation for a vibrant local community.\n",
      "\n",
      "The town's natural resources are abundant. The Niagara Falls Park is a wonderful spot for wildlife viewing. The water is very clean as well as beautiful. The beach is well maintained and the river is protected.\n",
      "\n",
      "The Falls has been popular for many years. It is a popular place to visit for tourists and it offers a lot of recreational activity. It is also a great place for a relaxing walk or a swim.\n",
      "\n",
      "The Falls is located about five minutes north of Niagara Falls.\n",
      "\n",
      "You can also see the Niagara Falls from the Niagara Falls Point. The Falls is closed to car or boat traffic.\n",
      "\n",
      "The Falls is a popular destination for people who don't drive. There are a few cars and a few bikes in the park.\n",
      "\n",
      "The Falls Falls is open from 9 AM to 5 PM, Saturday to Sundays from 9 AM to 5 PM, and from 8 PM to 5 PM, and from 8 PM to 4 PM.\n",
      "\n",
      "The Falls is a great way to take a walk or walk around the park or a good day's ride. The park is a great place for people to come down to\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.3.6 Named Entity Recognition <a name='20.3.6-named-entity-recognition'></a>\n",
    "\n",
    "Named Entity Recognition (NER), also known as named entity tagging, is a task of identifying parts of the input that represent entities. Examples of entities are:\n",
    "\n",
    "- Location (LOC)\n",
    "- Organizations (ORG)\n",
    "- Persons (PER)\n",
    "- Miscellaneous entities (MISC)"
   ],
   "metadata": {
    "id": "aYISR_J8kWZQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True, device=0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QP0NdU4w-Qz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661529968,
     "user_tz": 420,
     "elapsed": 9637,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "1e079f07-4205-4e6a-9e48-1f467058f17b"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "text_1 = \"Abraham Lincoln was a president who lived in the United States.\"\n",
    "\n",
    "print(ner(text_1))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcAecdhqxGkJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661530023,
     "user_tz": 420,
     "elapsed": 52,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "e3a439f1-a93a-4311-ec9d-645db265d093"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'entity_group': 'PER', 'score': np.float32(0.9988935), 'word': 'Abraham Lincoln', 'start': 0, 'end': 15}, {'entity_group': 'LOC', 'score': np.float32(0.99965084), 'word': 'United States', 'start': 49, 'end': 62}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, we can use Pandas to display the output."
   ],
   "metadata": {
    "id": "cYtM9d4dxgVY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(ner(text_1))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "3no833EHxZsn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661530136,
     "user_tz": 420,
     "elapsed": 113,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "4f520fa7-e56f-4dd6-ab1d-2f8500c54bd9"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  entity_group     score             word  start  end\n",
       "0          PER  0.998893  Abraham Lincoln      0   15\n",
       "1          LOC  0.999651    United States     49   62"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-00b8a095-b425-4d48-b77f-2516c40cf198\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998893</td>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>United States</td>\n",
       "      <td>49</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00b8a095-b425-4d48-b77f-2516c40cf198')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-00b8a095-b425-4d48-b77f-2516c40cf198 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-00b8a095-b425-4d48-b77f-2516c40cf198');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-9a1e6903-a65a-4c9b-95fe-1fca81f5b785\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9a1e6903-a65a-4c9b-95fe-1fca81f5b785')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-9a1e6903-a65a-4c9b-95fe-1fca81f5b785 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"entity_group\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"LOC\",\n          \"PER\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9996508359909058,\n          0.9988934993743896\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"United States\",\n          \"Abraham Lincoln\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 34,\n        \"min\": 0,\n        \"max\": 49,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          49,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33,\n        \"min\": 15,\n        \"max\": 62,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          62,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "text_2 = \"\"\"BAGHDAD -- Archaeologists in northern Iraq last week unearthed 2,700-year-old rock carvings featuring war scenes and trees from the Assyrian Empire, an archaeologist said Wednesday.\n",
    "    The carvings on marble slabs were discovered by a team of experts in Mosul, Iraq’s second-largest city, who have been working to restore the site of the ancient Mashki Gate, which was bulldozed by Islamic State group militants in 2016.\n",
    "    Fadhil Mohammed, head of the restoration works, said the team was surprised by discovering “eight murals with inscriptions, decorative drawings and writings.”\n",
    "    Mashki Gate was one of the largest gates of Nineveh, an ancient Assyrian city of this part of the historic region of Mesopotamia.\n",
    "    The discovered carvings show, among other things, a fighter preparing to fire an arrow while others show palm trees.\n",
    "    \"\"\"\n",
    "\n",
    "pd.DataFrame(ner(text_2))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "ZzZ3GpFCxnAe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661530252,
     "user_tz": 420,
     "elapsed": 115,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "bf4ae665-6a8c-4870-b47c-a2280fa4cfb6"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   entity_group     score             word  start  end\n",
       "0           LOC  0.434807               BA      0    2\n",
       "1           LOC  0.999473             Iraq     38   42\n",
       "2          MISC  0.893630         Assyrian    132  140\n",
       "3           LOC  0.782092           Empire    141  147\n",
       "4           LOC  0.999238            Mosul    255  260\n",
       "5           LOC  0.999156             Iraq    262  266\n",
       "6           LOC  0.971527      Mashki Gate    347  358\n",
       "7           ORG  0.997262    Islamic State    383  396\n",
       "8           PER  0.999300  Fadhil Mohammed    426  441\n",
       "9           LOC  0.974939      Mashki Gate    589  600\n",
       "10          LOC  0.975865          Nineveh    633  640\n",
       "11         MISC  0.994617         Assyrian    653  661\n",
       "12          LOC  0.976547      Mesopotamia    706  717"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-062668a8-46bb-4ab9-aae2-00b77e907fac\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.434807</td>\n",
       "      <td>BA</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999473</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.893630</td>\n",
       "      <td>Assyrian</td>\n",
       "      <td>132</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.782092</td>\n",
       "      <td>Empire</td>\n",
       "      <td>141</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999238</td>\n",
       "      <td>Mosul</td>\n",
       "      <td>255</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999156</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>262</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.971527</td>\n",
       "      <td>Mashki Gate</td>\n",
       "      <td>347</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.997262</td>\n",
       "      <td>Islamic State</td>\n",
       "      <td>383</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>Fadhil Mohammed</td>\n",
       "      <td>426</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.974939</td>\n",
       "      <td>Mashki Gate</td>\n",
       "      <td>589</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.975865</td>\n",
       "      <td>Nineveh</td>\n",
       "      <td>633</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.994617</td>\n",
       "      <td>Assyrian</td>\n",
       "      <td>653</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.976547</td>\n",
       "      <td>Mesopotamia</td>\n",
       "      <td>706</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-062668a8-46bb-4ab9-aae2-00b77e907fac')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-062668a8-46bb-4ab9-aae2-00b77e907fac button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-062668a8-46bb-4ab9-aae2-00b77e907fac');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-de1e6f80-0234-49cb-8d3e-c3ba9a01081e\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de1e6f80-0234-49cb-8d3e-c3ba9a01081e')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-de1e6f80-0234-49cb-8d3e-c3ba9a01081e button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 13,\n  \"fields\": [\n    {\n      \"column\": \"entity_group\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"MISC\",\n          \"PER\",\n          \"LOC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.9946165680885315,\n          0.9749393463134766,\n          0.4348071813583374\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Nineveh\",\n          \"Iraq\",\n          \"Mashki Gate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 239,\n        \"min\": 0,\n        \"max\": 706,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          653,\n          589,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 242,\n        \"min\": 2,\n        \"max\": 717,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          661,\n          600,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.3.7 Zero-shot Classification <a name='20.3.7-zero-shot-classification'></a>\n",
    "\n",
    "Zero-shot classification is a task to classify text documents, where the term\n",
    "*zero-shot* classification refers to tasks for which a language model has not been trained. I.e., the model was not trained to classify documents using the provided type of labels in the next example."
   ],
   "metadata": {
    "id": "gnmswDE02Or_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", device=0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBWehMtL2tTY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661539192,
     "user_tz": 420,
     "elapsed": 8938,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "c4545919-4c23-4252-ca03-bbfb1ba868da"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pipeline allows us to list `candidate labels` to be used for the classification. For this example, the model returned confidence scores for each category, and the highest probability was assigned to the \"sports\" category."
   ],
   "metadata": {
    "id": "8U1RWzt52zgd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "classifier(\n",
    "    \"Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls.\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\", \"sports\"],\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pvf75PSh2tWB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661539353,
     "user_tz": 420,
     "elapsed": 160,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "f4f40bd2-e53f-4364-ae7b-45ff0c2941d7"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sequence': 'Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls.',\n",
       " 'labels': ['sports', 'business', 'education', 'politics'],\n",
       " 'scores': [0.9866245985031128,\n",
       "  0.006729836110025644,\n",
       "  0.003462165826931596,\n",
       "  0.003183396067470312]}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.3.8 Mask Filling <a name='20.3.8-mask-filling'></a>\n",
    "\n",
    "The pipeline with the `fill-mask` task is used to fill in blanks in an input text."
   ],
   "metadata": {
    "id": "NYVSf_mWB5cN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mask_filling = pipeline(\"fill-mask\", device=0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXoJ7ekJB4n5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661541592,
     "user_tz": 420,
     "elapsed": 2237,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "37d53883-6cfa-46ba-a2c0-0416e6f6d599"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can provide the `top_k` argument to indicate the number of returned answers."
   ],
   "metadata": {
    "id": "a0E7QA4CBqDs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mask_filling(\"Abraham Lincoln was a <mask> who lived in the United States.\", top_k=5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eD9Br5uBCWas",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661541631,
     "user_tz": 420,
     "elapsed": 35,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "3a21b039-64d8-4fed-98d7-61132e721db8"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'score': 0.3327236473560333,\n",
       "  'token': 3661,\n",
       "  'token_str': ' Democrat',\n",
       "  'sequence': 'Abraham Lincoln was a Democrat who lived in the United States.'},\n",
       " {'score': 0.18090975284576416,\n",
       "  'token': 1172,\n",
       "  'token_str': ' Republican',\n",
       "  'sequence': 'Abraham Lincoln was a Republican who lived in the United States.'},\n",
       " {'score': 0.03390646353363991,\n",
       "  'token': 16495,\n",
       "  'token_str': ' Jew',\n",
       "  'sequence': 'Abraham Lincoln was a Jew who lived in the United States.'},\n",
       " {'score': 0.02841552160680294,\n",
       "  'token': 24156,\n",
       "  'token_str': ' Presbyterian',\n",
       "  'sequence': 'Abraham Lincoln was a Presbyterian who lived in the United States.'},\n",
       " {'score': 0.02462913654744625,\n",
       "  'token': 11593,\n",
       "  'token_str': ' physician',\n",
       "  'sequence': 'Abraham Lincoln was a physician who lived in the United States.'}]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "mask_filling(\"Flooding on the Yangtze river remains serious although <mask> levels on parts of the river decreased today.\", top_k=2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kH0_LiKeB4lO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661541655,
     "user_tz": 420,
     "elapsed": 23,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "33d1462b-c425-46f4-fceb-e8052251fb1f"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'score': 0.2489064335823059,\n",
       "  'token': 514,\n",
       "  'token_str': ' water',\n",
       "  'sequence': 'Flooding on the Yangtze river remains serious although water levels on parts of the river decreased today.'},\n",
       " {'score': 0.12597206234931946,\n",
       "  'token': 11747,\n",
       "  'token_str': ' oxygen',\n",
       "  'sequence': 'Flooding on the Yangtze river remains serious although oxygen levels on parts of the river decreased today.'}]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 20.4 Tokenizers <a name='20.4-tokenizers'></a>"
   ],
   "metadata": {
    "id": "2XOmz0dFEQxj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tokenizers** library in Hugging Face is used to split input text data into tokens (e.g., words, characters, N-grams), and map the tokens to integers. When we use a pretrained model from Hugging Face for a downstream task, our text data needs to be preprocessed in the same way as the training data used with the model. Therefore, we will need to download the tokenizer for that specific model.\n",
    "\n",
    "Let's consider the model `\"distilbert-base-uncased\"`, which is a version of the BERT transformer model, which takes case-insensitive English text as input data. Next, we will download the tokenizer for this model by using the `AutoTokenizer` class and its method `from_pretrained()`.  By using `AutoTokenizer` we don't need to manually download and manage the tokenizer files.\n",
    "\n",
    "Or, we can also specify the tokenizer name for the model that we wish to use. For instance, for the BERT model, Hugging Face has a `BertTokenizer` that can be directly imported from the `transformers` package."
   ],
   "metadata": {
    "id": "mH66hloLQQdk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ],
   "metadata": {
    "id": "8Jh17O93Ta7G",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661542021,
     "user_tz": 420,
     "elapsed": 365,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now use the `tokenizer` to convert text sentences into a sequence of integers, and display the output.\n",
    "\n",
    "The ouput of the tokenizer is a dictionary consisting of two key-value pairs:\n",
    "\n",
    "- `input_ids`, a list of integers, where each index identifies a token. The indexing is based on the vocabulary of the training data that was used to train the model `\"distilbert-base-uncased\"`.\n",
    "- `attention_mask`, a list of 1's or 0's, to indicate padding of the text sequence. This sentence does not have padding, since all elements have an attention mask of 1's. The attention mask ensures that the attention mechanism in Transformer is applied only to the real tokens, and the padding tokens are ignored."
   ],
   "metadata": {
    "id": "tegBL2hTVv2G"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "output_tokens_1 = tokenizer('Tokenizing text is easy.')\n",
    "print(output_tokens_1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMuXKBUZVtzU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661542046,
     "user_tz": 420,
     "elapsed": 18,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "52c93cc8-f5db-421c-e388-f02a2ced15c0"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 3733, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the above output has 8 tokens, although the input sentence has 4 words and the period mark. To understand better how the tokenization was performed, in the next cell we used the method `covert_ids_to_tokens()` to obtain the text for each integer value. Now we can see that the tokenizer places special tokens at the beginning and end of each sequence. `[CLS]` is placed at the beginning (it stands for Classification), and `[SEP]` is placed at the end of the sequence (it stands for Separate).\n",
    "\n",
    "Also note that the gerund verb \"tokenizing\" is split into `'token'` and `'##izing'`. Using two tokens for the word allows to work with smaller vocabularies. I.e., instead of considering `token` and `tokenization` as two different words, by splitting the word into the root `token` and the suffix `ization`, the model will use two tokens that have a distinct semantic meaning. This approach of decomposing long words into subwords is especially efficient with some languages where one can form very long words by chaining simple subwords."
   ],
   "metadata": {
    "id": "BJ6eWpDKXmqe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.convert_ids_to_tokens(output_tokens_1.input_ids)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AyqUqizTXiSd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661542057,
     "user_tz": 420,
     "elapsed": 10,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "41a56e2b-44be-44a5-90c3-d42367d4fbbc"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]', 'token', '##izing', 'text', 'is', 'easy', '.', '[SEP]']"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most tokenizers in Hugging Face assign integers for the special tokens between 100 and 103.\n",
    "\n",
    "These special tokens include:\n",
    "\n",
    "- \\[PAD\\], padding.\n",
    "- \\[UNK\\], unknown token.\n",
    "- \\[CLS\\], sequence beginning.\n",
    "- \\[SEP\\], sequence end.\n",
    "- \\[MASK\\], masked tokens."
   ],
   "metadata": {
    "id": "w1OSYSYBin1u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.convert_ids_to_tokens([0, 100, 101, 102, 103])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N237SDYjhuk0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661542071,
     "user_tz": 420,
     "elapsed": 13,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "f97160d2-e2dc-40a8-e720-dc5f2d241440"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another simple example is provided next. We can see that the word `'Transformer'` is tokenized as `'transform'` + `'##er'`."
   ],
   "metadata": {
    "id": "TiYWpD4T_tzr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "output_tokens_2 = tokenizer('Using a Transformer network in Hugging Face is simple')\n",
    "print(output_tokens_2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oiJ8JGoB_2Wo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661542084,
     "user_tz": 420,
     "elapsed": 11,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "221b0066-6941-44e5-f6f5-5e1b4c737835"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [101, 2478, 1037, 10938, 2121, 2897, 1999, 17662, 2227, 2003, 3722, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.convert_ids_to_tokens(output_tokens_2.input_ids)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9GzIeB0ABgf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661542097,
     "user_tz": 420,
     "elapsed": 12,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "4778ca83-a80a-4f8e-eb94-2d1b6a8fdaaf"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'using',\n",
       " 'a',\n",
       " 'transform',\n",
       " '##er',\n",
       " 'network',\n",
       " 'in',\n",
       " 'hugging',\n",
       " 'face',\n",
       " 'is',\n",
       " 'simple',\n",
       " '[SEP]']"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenizers can be applied to multiple text sequences. The argument `padding=True` is used below to pad the sequences to the longest sequence. Note below that 0's are added to pad the second sentence."
   ],
   "metadata": {
    "id": "7NFyFBEtjoYz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text_3 =  [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    "\n",
    "output_tokens_3 = tokenizer(text_3, padding=True)"
   ],
   "metadata": {
    "id": "NZUkYBXjhyry",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661542099,
     "user_tz": 420,
     "elapsed": 1,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(output_tokens_3)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkasEZgSiJ5w",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661542110,
     "user_tz": 420,
     "elapsed": 11,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "4d473ae3-37e2-459b-a5de-8be1bc88a0c2"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output is more readable if we print it line by line."
   ],
   "metadata": {
    "id": "aigtTmJxZsCU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Input IDs\")\n",
    "for item in output_tokens_3.input_ids:\n",
    "    print(item)\n",
    "\n",
    "print(\"Attention Mask\")\n",
    "for item in output_tokens_3.attention_mask:\n",
    "    print(item)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrazdOxqh_t3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661542123,
     "user_tz": 420,
     "elapsed": 12,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "b07b6100-3d33-4a83-fb09-f83363e71410"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input IDs\n",
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note also that each sequence of tokens begins with 101 (`'[CLS]'`) and ends with  102 (`'[SEP]'`).\n",
    "\n",
    "If `max_length` is provided, the tokenizer will truncate longer sentences to the specified length, as in the example in the next cell."
   ],
   "metadata": {
    "id": "bbLZbcH5AkOM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "output_tokens_4 = tokenizer(text_3, padding=False, max_length=10)\n",
    "\n",
    "print(\"Input IDs\")\n",
    "for item in output_tokens_4.input_ids:\n",
    "    print(item)\n",
    "\n",
    "print(\"Attention Mask\")\n",
    "for item in output_tokens_4.attention_mask:\n",
    "    print(item)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwD6z_vrGkTe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661542142,
     "user_tz": 420,
     "elapsed": 18,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "0ac7f133-a393-43a4-8892-028569b62fc2"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input IDs\n",
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 102]\n",
      "[101, 1045, 5223, 2023, 2061, 2172, 999, 102]\n",
      "Attention Mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Tokenizers library also allows to train new tokenizers from scratch. For instance, if a large corpus of text is available in another language than English, a new tokenizer will need to be trained to efficiently deal with the differences in the punctuation and use of spaces in that language."
   ],
   "metadata": {
    "id": "KENAhJA-HpDz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 20.5 Datasets <a name='20.5-datasets'></a>"
   ],
   "metadata": {
    "id": "TIQybZu4kliK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hugging Face provides access to a large number of **datasets**. If you wish to check all datasets please follow this [link](https://huggingface.co/datasets).\n",
    "\n",
    "To use the `datasets` library in Google Colab, we need to first install it."
   ],
   "metadata": {
    "id": "N6N0cbbjItLB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q datasets fsspec"
   ],
   "metadata": {
    "id": "xbiQCiZORwxH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661547871,
     "user_tz": 420,
     "elapsed": 5731,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "print(datasets.__version__)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v9fdoLSyteol",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661548332,
     "user_tz": 420,
     "elapsed": 458,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "2da32906-c54b-4eef-db42-dd32ad9daa12"
   },
   "execution_count": 38,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.0.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load the Emotions dataset. It contains Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. We can just use `load_dataset()` to accomplish that."
   ],
   "metadata": {
    "id": "CnNMHQ2nKC7x"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotions = load_dataset(\"emotion\")"
   ],
   "metadata": {
    "id": "-Guq9Z6wK1w3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661551490,
     "user_tz": 420,
     "elapsed": 3157,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see in the next cell that `emotions` dataset is a dictionary object that is split into training, validation, and test data sets, consisting of 16,000, 2,000, and 2,000 messages, respectively."
   ],
   "metadata": {
    "id": "wHT8qOP_K88J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "emotions"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5b12OuKK1zT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661551502,
     "user_tz": 420,
     "elapsed": 9,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "a9371cc4-7f9d-48f8-93a7-c16f718f0ffd"
   },
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first and second training samples are shown next. They contain the text and the corresponding emotion label."
   ],
   "metadata": {
    "id": "4xLmkX3FL-GA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "emotions['train'][0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xH-F4_s0K11C",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661551528,
     "user_tz": 420,
     "elapsed": 14,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "70d96683-6af8-4cac-8723-ee682dd54339"
   },
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "emotions['train'][1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7rETE-2L2cM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661551549,
     "user_tz": 420,
     "elapsed": 21,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "95cf8552-522d-4650-b472-1b39a9a62838"
   },
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       " 'label': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The order of the labels for the emotion categories are shown in the next cell."
   ],
   "metadata": {
    "id": "NeUCFqSqMY9I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "emotions['train'].features"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U8k3Y5oMZQS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661551565,
     "user_tz": 420,
     "elapsed": 13,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "1d9021b0-3b20-409f-b394-a6ff4d11cfb5"
   },
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': Value('string'),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])}"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hugging Face also allows to use the `set_format()` method with datasets, and to define the format of the data. For instance, by setting the type to Pandas, we can obtain the data as Pandas DataFrames."
   ],
   "metadata": {
    "id": "A_GfT28FNmP_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "emotions.set_format(type='pandas')\n",
    "df = emotions[\"train\"][:]\n",
    "df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "m6JsU5svMZS0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661551738,
     "user_tz": 420,
     "elapsed": 171,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "8514fe1a-3ea4-41c9-d171-62d221507218"
   },
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-e211ab96-07e2-4d34-9ede-e5edbba02848\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e211ab96-07e2-4d34-9ede-e5edbba02848')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e211ab96-07e2-4d34-9ede-e5edbba02848 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e211ab96-07e2-4d34-9ede-e5edbba02848');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-2d7d944c-b1e3-4358-8f31-437c870bd5bf\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2d7d944c-b1e3-4358-8f31-437c870bd5bf')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-2d7d944c-b1e3-4358-8f31-437c870bd5bf button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15969,\n        \"samples\": [\n          \"i feel rather imbicilic or at least complacent\",\n          \"i was in the bathroom i had sat down to pee it was to make me feel submissive again per instructions\",\n          \"i am thrilled with the way my skin and hair feel if you are like me you are skeptical\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 44
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use a bar plot to plot the number of values in each category."
   ],
   "metadata": {
    "id": "inoYCH0vQf_u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"label\"].value_counts(ascending=True).plot.barh()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "Cd1cAMoSQCC6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661552172,
     "user_tz": 420,
     "elapsed": 433,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "6f54b212-4cd9-4999-e6e2-0bf11f45f76c"
   },
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply a Tokenizer to a Dataset\n",
    "\n",
    "Let's show next how we can use a tokenizer to convert the first 5 samples in the training dataset to sequences of tokens. Since in the previous cells we set the format of the dataset to `pandas`, we need to first re-set the format to `type=None`."
   ],
   "metadata": {
    "id": "8ThxVoBZSggu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "emotions.set_format(type=None)\n",
    "\n",
    "training_samples_5 = emotions[\"train\"][:5]\n",
    "training_samples_5"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6I-xkx8R6O6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661552211,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "91adc3d7-f807-445f-b989-b15c7e19f70a"
   },
   "execution_count": 46,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': ['i didnt feel humiliated',\n",
       "  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       "  'im grabbing a minute to post i feel greedy wrong',\n",
       "  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
       "  'i am feeling grouchy'],\n",
       " 'label': [0, 0, 3, 2, 3]}"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's extract only the text data, and not the labels."
   ],
   "metadata": {
    "id": "dIyhVhDIS0HF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text_training_5 = training_samples_5['text']\n",
    "text_training_5"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dz8BJ6J1SJm7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661552269,
     "user_tz": 420,
     "elapsed": 56,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "0cd8af5c-41e4-4b58-8c01-26cd361c148c"
   },
   "execution_count": 47,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i didnt feel humiliated',\n",
       " 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       " 'im grabbing a minute to post i feel greedy wrong',\n",
       " 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
       " 'i am feeling grouchy']"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply the tokenizer, and display the sequence of tokens."
   ],
   "metadata": {
    "id": "CRP0h75PTQoV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "output_tokens_5 = tokenizer(text_training_5, padding=True)\n",
    "\n",
    "print(\"Input IDs\")\n",
    "for item in output_tokens_5.input_ids:\n",
    "    print(item)\n",
    "\n",
    "print(\"Attention Mask\")\n",
    "for item in output_tokens_5.attention_mask:\n",
    "    print(item)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cbhfZvcRF8h",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661552279,
     "user_tz": 420,
     "elapsed": 8,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "3b52397f-8568-4258-d34d-00620fbd6e13"
   },
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input IDs\n",
      "[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]\n",
      "[101, 10047, 9775, 1037, 3371, 2000, 2695, 1045, 2514, 20505, 3308, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 1045, 2572, 2412, 3110, 16839, 9080, 12863, 2055, 1996, 13788, 1045, 2097, 2113, 2008, 2009, 2003, 2145, 2006, 1996, 3200, 102, 0]\n",
      "[101, 1045, 2572, 3110, 24665, 7140, 11714, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 20.6 Models <a name='20.6-models'></a>"
   ],
   "metadata": {
    "id": "bFNzAfcc-1kL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.6.1 Importing a Pretrained Model <a name='20.6.1-importing-a-pretrained-model'></a>\n",
    "\n",
    "Instead of using a default model for a task, we can also select a language model from the many available in Hugging Face. Let's use GPT-2 language model, and we will also need to use the corresponding tokenizer for GPT-2.\n",
    "\n",
    "The parameter `pad_token_id` is optional, and its purpose is to define the ID (assigned integer value) for the token used for padding the text sequences. In this case, the ID of the padding token is set to the ID of the end-of-sequence token (`eos_token_id`), which is a common choice for padding in language models."
   ],
   "metadata": {
    "id": "OgvhTDO2EOXI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ],
   "metadata": {
    "id": "bcZLtN4iDM9u",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661554142,
     "user_tz": 420,
     "elapsed": 1862,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.6.2 Generation Strategies <a name='20.6.2-generation-strategies'></a>"
   ],
   "metadata": {
    "id": "U4ld12kdJrpg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To generate text with a pretrained sequence-to-sequence model using the Hugging Face library, we will use the `model.generate` method. The generation process involves making several choices, each with its own set of advantages and trade-offs. Based on the selected generation strategy, the model output can vary significantly.\n",
    "\n",
    "Let's consider the sentence `input_string` in the next cell. We will first preprocess it with the tokenizer to `encode` it into a sequence of integers. The argument `return_tensors=\"pt\"` specifies that the output should be returned as PyTorch tensors, where `\"pt\"` stands for PyTorch."
   ],
   "metadata": {
    "id": "1Zk74QuN-glv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "input_string = \"Yesterday I spent several hours in the library, studying\"\n",
    "input_tokens = tokenizer.encode(input_string, return_tensors=\"pt\")"
   ],
   "metadata": {
    "id": "WLmAGyZBrFN4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661554153,
     "user_tz": 420,
     "elapsed": 9,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will use the GPT-2 model for text generation with the `model.generate` method, to generate text that continues the input sentences. Afterward, we will use the tokenizer with the `tokenizer.decode` method to convert the generated sequence by the model into a text string.\n",
    "\n",
    "You can notice that the generated text sequence is not the best, since after the initial sequence of words, the model got stuck into a loop, and began repeating the same sequence. To deal with repetitive text generation, we can apply a different sampling strategy, as explained next."
   ],
   "metadata": {
    "id": "eSYAe7uWhNwf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "output_greedy = model.generate(input_tokens, max_length=64)\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5j_LP1tCIdP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661561597,
     "user_tz": 420,
     "elapsed": 7452,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "0d3d8f12-0c88-45c0-db62-af4d142867a0"
   },
   "execution_count": 51,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Yesterday I spent several hours in the library, studying the books, and I was amazed at how much I had learned. I was amazed at how much I had learned. I was amazed at how much I had learned. I was amazed at how much I had learned. I was amazed at how much I had learned.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Search Methods\n",
    "\n",
    "Two common methods for generating text with Language Models are:\n",
    "\n",
    " - **Greedy search**, selects the word with the highest probability as the next word. The major drawback of greedy search is that it can miss potentially high-probability words that follow a low-probability word. Therefore, although each individual word may be the best fit when generating a response, the entire generated text can be less relevant for the query.\n",
    " - **Beam search**, selects a sequence of words (beam) that has the overall highest probability. This approach reduces the risk of missing high-probability words, because rather than focusing only on the next word in a sequence, beam search looks at the probability of the entire response.\n",
    "\n",
    "Beam search is typically preferred over greedy search, because with beam search, the model can consider multiple routes and find the best option.\n",
    "\n",
    "For example, in the following figure, the input query to the model is \"The Financial Times is ...\". The model created four possible beams with a potential response, and out of the four beams, the third beam \"a newspaper founded in 1888\" was selected as the most coherent human-like response.\n",
    "\n",
    "<img src='images/beam_search.png' width=500px>\n",
    "\n",
    "*Figure: Beam search example.* Source: [link](https://ig.ft.com/generative-ai/)"
   ],
   "metadata": {
    "id": "uQgLuILYJaC_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To deal with repetitive text generation in the above example which uses greedy search, let's apply a beam search strategy. This is shown in the following cell, where beam search examines multiple probable solutions (beams) for the generated text, that is set by the argument `num_beams=32`. We also applied a penalty term for repeating the same two consecutive words with the argument `no_repeat_ngram_size=2`. By considering several possible solutions, the beam search algorithm tries to improve the generated output text by the model."
   ],
   "metadata": {
    "id": "b7RD1p8O_Hf1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "output_beam = model.generate(input_tokens, max_length=64, num_beams=32, no_repeat_ngram_size=2)\n",
    "\n",
    "print(tokenizer.decode(output_beam[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEhwRR2ADdd8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661586668,
     "user_tz": 420,
     "elapsed": 25065,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "b4451cad-e96b-4606-f956-356536e6364b"
   },
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Yesterday I spent several hours in the library, studying all the books I could get my hands on, and trying to figure out what I wanted to do with them. I didn't know what to expect, but I did know that it was going to be a lot of fun.\n",
      "\n",
      "When I got home, I\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sampling-based Methods\n",
    "\n",
    "Greedy search and beam search aim to find the best sequence of tokens that maximizes the likelihood of the generated text. Beside search methods,  modern language models also employ **sampling-based methods** that introduce controlled randomness to generate more diverse and creative text. Instead of always choosing the most likely tokens, these methods sample from the model's probability distribution, allowing for variability in the generated output.\n",
    "\n",
    "Sampling-based methods are especially useful for tasks like creative writing, dialogue generation, and open-ended text completion.\n",
    "\n",
    "Sampling-based strategies include the following.\n",
    "\n",
    "1. Temperature Sampling\n",
    "\n",
    "**Temperature** controls the randomness of predictions by scaling the logits (raw prediction scores) before the softmax activation function in the output layer. A typical defaul temperature value is 1.0. Lower temperature values in the range 0.1 - 0.7 make the model more confident and deterministic, whereas higher temperature values in the range 1.0 - 2.0 increase randomness and creativity in the generated text.\n",
    "\n",
    "The next example shows two different responses with a low and high temperature. The argument `do_sample=True` specifies that the model should sample from the probability distribution of possible next tokens."
   ],
   "metadata": {
    "id": "XO0xLEzRKpcl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "input_string = \"The secret door opened, revealing\"\n",
    "input_tokens = tokenizer.encode(input_string, return_tensors=\"pt\")"
   ],
   "metadata": {
    "id": "PMZ-rIqVTxlO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661586670,
     "user_tz": 420,
     "elapsed": 1,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Low temperature - more focused and deterministic\n",
    "output_low_temp = model.generate(input_tokens, max_length=80, do_sample=True, temperature=0.3, num_return_sequences=1)\n",
    "\n",
    "# High temperature - more random and creative\n",
    "output_high_temp = model.generate(input_tokens, max_length=80, do_sample=True, temperature=1.5, num_return_sequences=1)\n",
    "\n",
    "print(\"Low Temperature Output:\")\n",
    "print(tokenizer.decode(output_low_temp[0], skip_special_tokens=True))\n",
    "print(\"\\nHigh Temperature Output:\")\n",
    "print(tokenizer.decode(output_high_temp[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Leqp0ezYLiZh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661601497,
     "user_tz": 420,
     "elapsed": 14825,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "fbaa9f17-f60f-4ecd-c5e1-7012221fa86b"
   },
   "execution_count": 54,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Low Temperature Output:\n",
      "The secret door opened, revealing a man in a white suit and a black suit. He was dressed in a white suit, black tie, and a black shirt. He was wearing a black t-shirt and black pants. He was wearing a black T-shirt and black pants. He was wearing a black T-shirt and black pants. He was wearing a black T-shirt and black pants\n",
      "\n",
      "High Temperature Output:\n",
      "The secret door opened, revealing Jannette, the blonde-haired queen's favorite model, with her black hair cascading in two. She smiled out loud, a look of delight that she held before her.\n",
      "\n",
      "The other two watched in silence as one of the girls looked at the queen. She reached up toward the queen of the room and placed a hand over her mouth. She placed\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Top-K Sampling\n",
    "\n",
    "**Top-K sampling** limits the model to selecting from only the K most likely next tokens at each step. At each generation step, the model keps only the top K tokens (those with the highest probability), and samples randomly from this reduced set of tokens. Instead of selecting the token with the highest probability as in greedy search, top-K sampling considers several most likely tokens at each step.\n",
    "\n",
    "A typical default value for K is 50. Small K values in the range 10-20 result in more focused but less diverse response, and large K values over 100 produce more diverse but potentially less coherent text."
   ],
   "metadata": {
    "id": "BoPrAtcNKph7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Top-K sampling with K=10\n",
    "output_low_topk = model.generate(input_tokens, max_length=80, do_sample=True, top_k=10, num_return_sequences=1)\n",
    "\n",
    "# Top-K sampling with K=100\n",
    "output_high_topk = model.generate(input_tokens, max_length=80, do_sample=True, top_k=100, num_return_sequences=1)\n",
    "\n",
    "print(\"Low Top-K Output:\")\n",
    "print(tokenizer.decode(output_low_topk[0], skip_special_tokens=True))\n",
    "print(\"\\nHigh Top-K Output:\")\n",
    "print(tokenizer.decode(output_high_topk[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQAFCVIiMhMw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661622073,
     "user_tz": 420,
     "elapsed": 20575,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "a76bb38e-ef69-4dfa-deb4-a18664e02f18"
   },
   "execution_count": 55,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Low Top-K Output:\n",
      "The secret door opened, revealing a man with a gun drawn. The man turned around and pointed a handgun at the woman with the rifle at her side and told her that he had a gun. When the woman looked back at her assailant and said that he would kill her he shot her in the neck. The woman was hit and was taken to a hospital for treatment.\n",
      "\n",
      "The man said the\n",
      "\n",
      "High Top-K Output:\n",
      "The secret door opened, revealing the body\n",
      "\n",
      "of the former assassin of the city's great spy,\n",
      "\n",
      "Dr. Zemma.\n",
      "\n",
      "Dr. Zemma, the former assassin of San Ramon who's now\n",
      "\n",
      "going to have his own trial.\n",
      "\n",
      "\"Shall we hold a new trial?\" he exclaimed, his voice trembling.\n",
      "\n",
      "The old assassin,\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Top-P (Nucleus) Sampling\n",
    "\n",
    "**Top-P sampling** (also called nucleus sampling) selects the smallest set of tokens whose cumulative probability exceeds a chosen threshold P. It ranks all tokens by probability and samples from a reduced \"nucleus\" of tokens with cumulative probability greater than P.\n",
    "\n",
    "When the model is confident fewer tokens are considered, and when the model is uncertain more tokens are included. A typical default value for P is 0.9. Lower values for P in the range 0.5-0.7 produce more deterministic and focused outputs, while higher values for P in the range 0.95-1.0 result in more diverse and creative outputs."
   ],
   "metadata": {
    "id": "41gnBu1fKpuK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Top-P sampling with P=0.1\n",
    "output_low_topp = model.generate(input_tokens, max_length=80, do_sample=True, top_p=0.5, num_return_sequences=1)\n",
    "\n",
    "# Top-K sampling with P=0.9\n",
    "output_high_topp = model.generate(input_tokens, max_length=80, do_sample=True, top_p=0.95, num_return_sequences=1)\n",
    "\n",
    "print(\"Low Top-P Output:\")\n",
    "print(tokenizer.decode(output_low_topp[0], skip_special_tokens=True))\n",
    "print(\"\\nHigh Top-P Output:\")\n",
    "print(tokenizer.decode(output_high_topp[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5g4376cOuqz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661647336,
     "user_tz": 420,
     "elapsed": 25262,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "bd86e35d-9ef7-4747-ae29-d46ce9a0b157"
   },
   "execution_count": 56,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Low Top-P Output:\n",
      "The secret door opened, revealing a large man, wearing a dark suit and black trousers, and a white hat. He was dressed in a white suit and black trousers. He walked up to the man and asked him for his name. The man said he was James, and that he was a man of great power. He asked if he could help him, and he said he would. The man\n",
      "\n",
      "High Top-P Output:\n",
      "The secret door opened, revealing that it was a boy. \"How did you find me?\" he asked, trying to get at his father's face.\n",
      "\n",
      "My mom answered, \"Well, you've been living with my father.\" I smiled, then looked up, as though I were about to cry. His face was pale, a look that made me freeze. He stared at the door\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is also possible to combine different sampling methods as in the next example."
   ],
   "metadata": {
    "id": "68UB5KB9v4z-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Combination: top_k=60, top_p=0.95, temp=1.5\n",
    "output = model.generate(input_tokens, max_length=80, do_sample=True, top_k=60, top_p=0.95, temperature=1.5)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4uIzd7IVXrf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661659026,
     "user_tz": 420,
     "elapsed": 11688,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "3ecdb2cb-f593-4faa-ddda-a048426d07e0"
   },
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The secret door opened, revealing a strange, unbroken network of human figures standing atop his bedsheets. All of them, except maybe their boss; it all seemed normal- but just the two of you!\n",
      "            \"What?\" your wife demanded from in surprise. The sound of your throat pushing against the bedspread made her panic.\n",
      "(And just maybe the way her husband tried to\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other parameters in `model.generate` include:\n",
    "\n",
    "- `repetition_penalty`: Discourages the model from repeating the same tokens or phrases. Values > 1.0 penalize repetition, and values over 1.2-1.5 reduce repetition more aggressively.\n",
    "- `length_penalty`: Controls the model's preference for longer or shorter sequences (mainly used with beam search). Values < 1.0 favor shorter responses, and values > 1.0 favorr longer responses.\n",
    "- `no_repeat_ngram_size`: Prevents repeating any n-gram of the specified size. We used it above in beam search example.\n",
    "- `min_length` and `max_lengthe`: Define the minimum and maximum number of tokens in the generated text.\n",
    "\n",
    "Recommendations for text generation:\n",
    "\n",
    "- Use lower temperature for factual tasks, and higher temperature for creative tasks.\n",
    "- Enable `do_sample=True` when using temperature, top-K, or top-P sampling.\n",
    "- Start with default values, such as `top_p=0.9`, `temperature=1.0`, and `top_k=0.9`.\n",
    "- Use `repetition_penalty` to avoid loops and repetitive phrases, especially with smaller models.\n",
    "- Combine sampling methods, such as `top_k=50`, `top_p=0.9`, `temperature=0.8`, for better control of diversity and cohernce.\n",
    "- Generate multiple sequences  with `num_return_sequences` and select the best one."
   ],
   "metadata": {
    "id": "7mayNuIbPAhc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20.6.3 Fine-tuning a Pretrained Model <a name='20.6.3-fine-tuning-a-pretrained-model'></a>\n",
    "\n",
    "The next section demonstrates how to use a pretrained model and tokenizer in Hugging Face, and fine-tune the model to a dataset. We will use the DistilBERT model for this task, and we will again use the Emotions dataset.\n",
    "\n",
    "Let's first download the tokenizer for the `\"distilbert-base-uncased\"` model."
   ],
   "metadata": {
    "id": "NWqaheyea4ru"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "WcfcBoesA0wI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661659527,
     "user_tz": 420,
     "elapsed": 503,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBEn8UnruH-l"
   },
   "source": [
    "The following code tokenizes the emotions dataset. It is common to use the `Dataset.map()` method in Hugging Face for tokenization, which applies a function on each sample in the dataset. Therefore, we first define a function `tokenize` that is used in the `Dataset.map()` method. Using the option `batched=True` in the `map()` method will apply the tokenization to a batch of input sequences instead of each text sequence, which will speed up the tokenization. And, as we learned in the previous section, `tokenize` returns a dictionary with keys `input_ids` and `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8b079eb12c63446d841c9d59d0b00020",
      "ef4df95a0a924ce2bdbe2747a2e1fbd8",
      "8ce639f358d84ef0b64f2573a5533c97",
      "f2bc832b375b45bba5800ff87dabb061",
      "ed7278cde36541fe945972e81522b38e",
      "09550e364c014243ae4c4e0e79a395e1",
      "87295c32e6d44ad7bd35bb544dc55fbb",
      "33d2b3b1e6364f73a5abe7e619f0389a",
      "8aaa8d9f464240209d03a37218cbfe4b",
      "1ce26c13de6345ba9a454cfabaaa18d8",
      "c256e3cafbad43bd9bcc98133c762504"
     ]
    },
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1762661660282,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     },
     "user_tz": 420
    },
    "outputId": "c84e122c-194f-404e-a267-7b20bc240b1d",
    "id": "NSNnNKIsuH-l"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b079eb12c63446d841c9d59d0b00020"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def tokenize(rows):\n",
    "    return tokenizer(rows['text'], truncation=True)\n",
    "\n",
    "tokenized_datasets = emotions.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Vt8YC4WkuH-m",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661660341,
     "user_tz": 420,
     "elapsed": 55,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = tokenized_datasets[\"train\"]\n",
    "tokenized_val_dataset = tokenized_datasets[\"validation\"]\n",
    "tokenized_test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6detUgMZuH-m"
   },
   "source": [
    "`DataCollatorWithPadding` in Hugging Face pads the tokenized input samples during batches preparation to have the same length. I.e., it pads each sentence to the maximum length in each batch. This is referred to as dynamic padding, and it is more efficient than padding each sequence to a fixed length, because it only pads as much as necessary. By passing the tokenizer to `DataCollatorWithPadding`, the function can handle the specific padding requirements for the used model, such as special tokens like [PAD] that are used in the DistilBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "-aj4Epp1u4ky",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661660344,
     "user_tz": 420,
     "elapsed": 2,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJmw95uVu4k5"
   },
   "source": [
    "Next, we will load the distilbert model for classification. Hugging Face Transformers library provides an `AutoModel` class which also has a `from_pretrained()` method that can be used to load a pretrained checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTC6B8G_hBS8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661660806,
     "user_tz": 420,
     "elapsed": 461,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "f15272e2-094d-4ae1-fa8e-444ed600beaa"
   },
   "execution_count": 62,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformers library in Hugging Face provides a `Trainer` class for model training, which is similar to `model.fit` in Keras-TensorFlow. That is, we just need to pass all parameters that are needed for training, and afterwards run `Trainer.train()` to fit the model to the data. The input parameters are first defined with the class `TrainingArguments` as shown in the next cell. In this case, the Training Arguments inlcude the output directory to save the training outputs, evaluation strategy, learning rate, batch sizes for training and validation, number of training epochs, weight decay, and whether to report the results to external tools."
   ],
   "metadata": {
    "id": "_Dul78wix3IN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments for the model\n",
    "training_args = TrainingArguments(\n",
    "    # Directory to save model checkpoints and logs\n",
    "    output_dir=\"bert-emotion\",\n",
    "    # Evaluation strategy - evaluate the model at the end of each epoch\n",
    "    eval_strategy=\"epoch\",\n",
    "    # Learning rate for the optimizer\n",
    "    learning_rate=2e-5,\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=16,\n",
    "    # Batch size for evaluation\n",
    "    per_device_eval_batch_size=16,\n",
    "    # Number of training epochs\n",
    "    num_train_epochs=2,\n",
    "    # Weight decay to apply for regularization\n",
    "    weight_decay=0.01,\n",
    "    # Disable reporting to external tools (e.g., WandB, TensorBoard)\n",
    "    report_to=\"none\"\n",
    ")"
   ],
   "metadata": {
    "id": "LxQJ-dFLhBQe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661660880,
     "user_tz": 420,
     "elapsed": 65,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also define a function called `compute_metrics` to calculate the accuracy at the end of each epoch."
   ],
   "metadata": {
    "id": "aGZUx3T80JaB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to compute metrics during training\n",
    "def compute_metrics(eval_pred):\n",
    "    outputs, labels = eval_pred\n",
    "    predictions = outputs.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}"
   ],
   "metadata": {
    "id": "eFq0kg_wvAPO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661660899,
     "user_tz": 420,
     "elapsed": 1,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    }
   },
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will define our `trainer` as an instance of the `Trainer` class by passing the model that we defined, the training arguments, the training and validation datasets, tokenizer, data collator, and the function for computing performance metrics (accuracy). Afterward, we will initialize the training by running `trainer.train()` as in the following cell. We can see that the loss and accuracy values are reported after each epoch."
   ],
   "metadata": {
    "id": "Z7DCOvh_0gbU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer with the required parameters\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "metadata": {
    "id": "kcQTMOaIhNsD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661662756,
     "user_tz": 420,
     "elapsed": 1844,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "6caeb278-68d3-4feb-bf63-8ef651a77504"
   },
   "execution_count": 65,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-1879187504.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "PabgljFshhzd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661969420,
     "user_tz": 420,
     "elapsed": 305527,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "c34b8753-338e-4d96-df16-ff529783090f"
   },
   "execution_count": 66,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 05:04, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.220022</td>\n",
       "      <td>0.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>0.163510</td>\n",
       "      <td>0.937000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.3261281471252441, metrics={'train_runtime': 305.4095, 'train_samples_per_second': 104.777, 'train_steps_per_second': 6.549, 'total_flos': 389287358125632.0, 'train_loss': 0.3261281471252441, 'epoch': 2.0})"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's evaluate the model performance on the test dataset. The calculated accuracy is 92.6%."
   ],
   "metadata": {
    "id": "b8dOzX7n5HYN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(test_metrics)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "vJJpQVJJuHI0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661972236,
     "user_tz": 420,
     "elapsed": 2815,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "5834802a-b146-47a6-95f8-c5ef1a54f39b"
   },
   "execution_count": 67,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:02]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.1721537709236145, 'eval_accuracy': 0.926, 'eval_runtime': 2.8069, 'eval_samples_per_second': 712.538, 'eval_steps_per_second': 44.534, 'epoch': 2.0}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition, let's predict the class labels for the four sentences shown below. First, we will need to apply the tokenizer to prepare the sentences."
   ],
   "metadata": {
    "id": "EsEFGYMyzJIi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# List of sentences to evaluate\n",
    "test_texts = [\"I feel fantastic!\", \"I'm a bit sad today.\", \"This is so exciting!\", \"I'm very stressed out.\"]\n",
    "\n",
    "# Tokenize the sentences, pad them to the same length for batch processing\n",
    "inputs = tokenizer(test_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device=0)\n",
    "inputs"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWtjA94Q6rdO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661972295,
     "user_tz": 420,
     "elapsed": 49,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "fc49f5fb-4c4c-461b-9c4d-fbac9261c768"
   },
   "execution_count": 68,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2514, 10392,   999,   102,     0,     0,     0,     0],\n",
       "        [  101,  1045,  1005,  1049,  1037,  2978,  6517,  2651,  1012,   102],\n",
       "        [  101,  2023,  2003,  2061, 10990,   999,   102,     0,     0,     0],\n",
       "        [  101,  1045,  1005,  1049,  2200, 13233,  2041,  1012,   102,     0]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')}"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next cell, we pass the inputs to the model, and use `argmax` to calculate the predicted labels for the sentences."
   ],
   "metadata": {
    "id": "EB0bReBm8Pr0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Forward pass through the model\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get predicted labels for each sentence\n",
    "predicted_labels = outputs.logits.argmax(axis=-1).cpu().tolist()\n",
    "print(\"Predicted Labels:\", predicted_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOGpPAoOzUBe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661972306,
     "user_tz": 420,
     "elapsed": 10,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "0354018b-15ae-4cce-8cbc-2f0adde319ba"
   },
   "execution_count": 69,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted Labels: [1, 0, 1, 3]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To understand the predicted labels, let's retrieve the class names, and print the sentences with the class names of the predicted labels."
   ],
   "metadata": {
    "id": "ivJxZT1o8izU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Emotion class names\n",
    "class_names = emotions[\"train\"].features[\"label\"].names\n",
    "class_names"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DT6FGwJFuHVS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661972318,
     "user_tz": 420,
     "elapsed": 10,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "41806bcd-875f-44c1-bf1f-a2209df7634d"
   },
   "execution_count": 70,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Print results\n",
    "for text, label in zip(test_texts, predicted_labels):\n",
    "    print(f\"Text: '{text}'; Predicted Label: {class_names[label]}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSj_p-207kZy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762661972333,
     "user_tz": 420,
     "elapsed": 15,
     "user": {
      "displayName": "Aleksandar Vakanski",
      "userId": "07675307153279708378"
     }
    },
    "outputId": "d045710d-1c45-42c1-e5c4-8cd6a637bb46"
   },
   "execution_count": 71,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text: 'I feel fantastic!'; Predicted Label: joy\n",
      "Text: 'I'm a bit sad today.'; Predicted Label: sadness\n",
      "Text: 'This is so exciting!'; Predicted Label: joy\n",
      "Text: 'I'm very stressed out.'; Predicted Label: anger\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vweobvFVe4RB"
   },
   "source": [
    "## References <a name='references'></a>\n",
    "\n",
    "1. Hugging Face Course, available at [https://huggingface.co/course/chapter1/1](https://huggingface.co/course/chapter1/1).\n",
    "2. Applications of Deep Neural Networks, Course at Washington University in St. Louis, Jeff Heaton, available at [https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_01_huggingface.ipynb](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_01_huggingface.ipynb).\n",
    "3. Getting Started with Hugging Face Transformers for NLP, Exxact Blog, available at [https://www.exxactcorp.com/blog/Deep-Learning/getting-started-hugging-face-transformers](https://www.exxactcorp.com/blog/Deep-Learning/getting-started-hugging-face-transformers).\n",
    "4. An Introduction to Using Transformers and Hugging Face, Zoumana Kelta, available at [https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face](https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDbzcXmWe4RB"
   },
   "source": [
    "[BACK TO TOP](#top)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1XdYOggN-wY1KLZ3O2n2oO4U1swJnXLKg",
     "timestamp": 1699134970599
    },
    {
     "file_id": "1_wSiz1ALZ1l9vda7yvvdLgEHHE8M0F66",
     "timestamp": 1666471615676
    }
   ],
   "gpuType": "T4",
   "gpuClass": "premium"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}