{"cells":[{"cell_type":"markdown","metadata":{"id":"3JhHIQqNe4Qs"},"source":["# Lecture 20 - NLP with Hugging Face"]},{"cell_type":"markdown","metadata":{"id":"kPG5INqg-_qn"},"source":["[![View notebook on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_20-NLP_with_Hugging_Face/Lecture_20-NLP_with_Hugging_Face.ipynb)\n","[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avakanski/Fall-2025-Applied-Data-Science-with-Python/blob/main/docs/Lectures/Theme_3-Model_Engineering/Lecture_20-NLP_with_Hugging_Face/Lecture_20-NLP_with_Hugging_Face.ipynb)"]},{"cell_type":"markdown","source":["<a id='top'></a>"],"metadata":{"id":"zk7UKxk5P8eH"}},{"cell_type":"markdown","metadata":{"id":"iEkmemKte4Qv"},"source":["- [20.1 Introduction to Hugging Face](#20.1-introduction-to-hugging-face)\n","- [20.2 Hugging Face Pipelines](#20.2-hugging-face-pipelines)\n","- [20.3 Pipelines for NLP Tasks](#20.3-pipelines-for-nlp-tasks)\n","    - [20.3.1 Sentiment Analysis](#20.3.1-sentiment-analysis)\n","    - [20.3.2 Question Answering](#20.3.2-question-answering)\n","    - [20.3.3 Machine Translation](#20.3.3-machine-translation)\n","    - [20.3.4 Text Summarization](#20.3.4-text-summarization)\n","    - [20.3.5 Text Generation](#20.3.5-text-generation)\n","    - [20.3.6 Named Entity Recognition](#20.3.6-named-entity-recognition)\n","    - [20.3.7 Zero-shot Classification](#20.3.7-zero-shot-classification)\n","    - [20.3.8 Mask Filling](#20.3.8-mask-filling)\n","- [20.4 Tokenizers](#20.4-tokenizers)\n","- [20.5 Datasets](#20.5-datasets)    \n","- [20.6 Models](#20.6-models)\n","- [References](#references)"]},{"cell_type":"markdown","metadata":{"id":"gTFgm_bJe4Qw"},"source":["## 20.1 Introduction to Hugging Face <a name='20.1-introduction-to-hugging-face'></a>"]},{"cell_type":"markdown","metadata":{"id":"UFMFGCdwiw5k"},"source":["**Hugging Face** ([link](https://huggingface.co/)) is a platform for Machine Learning and AI created in 2016, with the aim to \"build, train, and deploy state of the art models powered by the reference open source in machine learning\". Since then, Hugging Face has established itself as the main source for NLP and other Machine Learning tasks, providing open access to over 400,000 pre-trained models, datasets, and pertinent tools and resources. Hugging Face focuses on community-building around open-source machine learning tools and data. They also developed several [courses](https://huggingface.co/course/chapter1/1) on how to use their libraries for various tasks. Also note that while open access is provided to the core NLP libraries, Hugging Face also offers pricing options for access to AutoNLP libraries.\n","\n","<img src=\"images/hf_icon.png\" width=\"500\">\n","\n","*Figure: Hugging Face webpage.*"]},{"cell_type":"markdown","source":["Hugging Face initially focused on Transformer Networks and NLP, while recently they have expanded their libraries and tools to cover machine learning models and tasks, in general. State-of-the-art Transformer Networks are very large models, and hence, training such models from scratch is expensive and not affordable for many organizations. For example, the cost of training the GPT-4 model is estimated to be over USD $100 million. Providing access to pre-trained models for transfer learning and fine-tuning to specific tasks by Hugging Face has been a significant resource.\n","\n","The core Hugging Face libraries include Transformer models, Tokenizers, Datasets, and Accelerate. Accelerate library enables distributed training with hardware acceleration devices, such as using multiple GPUs, or cloud accelerators with TPUs. In addition to these core libraries, Hugging Face provides various community resources, which include a platform for sharing models, code versioning, Spaces allow sharing apps developed with Hugging Face libraries and browsing apps created by others, etc.\n","\n","<img src=\"images/hf_libraries.png\" width=\"400\">\n","\n","*Figure: Hugging Face libraries.*\n","\n","The key characteristics of these libraries include:\n","\n","- Ease of use and simplicity, where downloading and using state-of-the-art NLP models can be done with a few lines of code.\n","- Flexibility, since all models are implemented either using the `nn.Module` in PyTorch or `tf.keras.Model` in TensorFlow, allowing for easy model integration with these popular frameworks.\n"],"metadata":{"id":"FplNgLi89-__"}},{"cell_type":"markdown","source":["## 20.2 Hugging Face Pipelines <a name='20.2-hugging-face-pipelines'></a>"],"metadata":{"id":"aJIOCFH_GeLa"}},{"cell_type":"markdown","source":["Hugging Face uses **Pipelines** as an API allowing to perform a a variety of NLP tasks through the `pipeline()` method.\n","\n","The `pipeline()` method has the following syntax:\n","\n","```\n","from transformers import pipeline\n","\n","# Pipeline to use a default model & tokenizer for a given task\n","pipeline(\"<task-name>\")\n","\n","# Pipeline to use an existing or custom model\n","pipeline(\"<task-name>\", model=\"<model_name>\")\n","\n","# Pipeline to use an existing or custom model and tokenizer\n","pipeline('<task-name>', model='<model name>', tokenizer='<tokenizer_name>')\n","```"],"metadata":{"id":"2aMOkua_GomJ"}},{"cell_type":"markdown","source":["Among the currently available task pipelines are:\n","\n","- Sentiment analysis\n","- Question answering\n","- Translation\n","- Summarization\n","- Text generation\n","- NER (named entity recognition)\n","- Zero shot classification\n","- Fill mask"],"metadata":{"id":"bYGQQd46IBlT"}},{"cell_type":"markdown","source":["## 20.3 Pipelines for NLP Tasks <a name='20.3-pipelines-for-nlp-tasks'></a>"],"metadata":{"id":"dlAygyVAIB3v"}},{"cell_type":"markdown","source":["In this section, we will examine examples of using the `pipeline(\"<task-name>\")` method with different NLP tasks. As we mentioned, if we don't provide the names for the used model and tokenizer, the pipeline will assign a default  model and tokenizer to complete the task, and it will download the model parameters and other required elements to perform the task.\n","\n"," The Transformers library by Hugging Face is preinstalled in Google Colab. However, if you don't run this notebook in Google Colab, you will need to first install the Transformers library (`!pip install transformers`)."],"metadata":{"id":"9e49MdXTICIg"}},{"cell_type":"markdown","source":["### 20.3.1 Sentiment Analysis <a name='20.3.1-sentiment-analysis'></a>"],"metadata":{"id":"-MC9UBiqyPwV"}},{"cell_type":"markdown","source":["The first example uses `pipeline()` for sentiment analysis. We saw examples of sentiment analysis in the previous lectures, where the goal was to classify the sentiment in movie reviews text as positive or negative.\n","\n","When the cell is executed, the pipeline will select a default pretrained model for sentiment analysis in English, it will download the model and the related tokenizer, and it will instantiate a text classifier object. In this case, we can see in the cell output that the used default pretrained model is `distilbert-base-uncased-finetuned-sst-2-english`. The argument `device=0` in the pipeline assigns the pipeline to a GPU device if it is available. For running the model on CPU use `device=-1`."],"metadata":{"id":"d_7dDPXJWlLn"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\", device=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtvntkaCyHJs","executionInfo":{"status":"ok","timestamp":1731256915904,"user_tz":420,"elapsed":22048,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"34c616db-b764-44d4-fa24-fa68676ae991"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["In the next cell, the classifier is applied to a sentence. The output is the predicted label and the confidence score."],"metadata":{"id":"8H7Isab8BWWl"}},{"cell_type":"code","source":["classifier(\"I fully understand what you are saying.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oJ_4Y39uyHM0","executionInfo":{"status":"ok","timestamp":1731256917286,"user_tz":420,"elapsed":1384,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"bf1d65ec-ad00-4e4f-c059-875d0ff8a808"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9996806383132935}]"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["The pipeline allows to pass multiple sentences, and it will return a sentiment label and confidence score for each sentence."],"metadata":{"id":"l32uW3Ueyevh"}},{"cell_type":"code","source":["classifier(\n","    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RyDIrSpryWiL","executionInfo":{"status":"ok","timestamp":1731256917287,"user_tz":420,"elapsed":11,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"1bcd83e1-0fba-43db-adee-0ffe2e0da97e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9598046541213989},\n"," {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### 20.3.2 Question Answering <a name='20.3.2-question-answering'></a>"],"metadata":{"id":"NiJC0oCdXZAT"}},{"cell_type":"markdown","source":["This pipeline answers questions using information from a given context. Such pipeline can be very useful when we are dealing with long text data and finding answers to questions in the document can take time."],"metadata":{"id":"SavxGni5XZTV"}},{"cell_type":"code","source":["question_answerer = pipeline(\"question-answering\", device=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5u9R3j_XmbS","executionInfo":{"status":"ok","timestamp":1731256919295,"user_tz":420,"elapsed":2014,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7935266f-a20d-47bc-f2fb-1bedb34b4fab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}]},{"cell_type":"markdown","source":["We can provide inputs to the pipeline as a dictionary with `question` and `context` as keys. The model extracts information from the provided context and returns a dictionary with a confidence score, start and end characters of the answer in the context, and the answer. Also note that the model does not generate new text to answer the questions, but instead it searches for the answer in the supplied context sequence."],"metadata":{"id":"4alMqqSEZd01"}},{"cell_type":"code","source":["input_1 = {\n","    \"question\" : \"What didn't cross the street?\",\n","    \"context\" : \"The animal didn't cross the street because it was too tired\",\n","    }\n","\n","question_answerer(input_1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iP_KG6jqYZtU","executionInfo":{"status":"ok","timestamp":1731256919295,"user_tz":420,"elapsed":11,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"e50a2ea0-830f-477a-fbc1-39aa9ac53b98"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'score': 0.7553666234016418, 'start': 0, 'end': 10, 'answer': 'The animal'}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["input_2 = {\n","    \"question\" : \"Why the animal didn't cross the street?\",\n","    \"context\" : \"The animal didn't cross the street because it was too wide\",\n","    }\n","\n","question_answerer(input_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fn9PVBqCZHaC","executionInfo":{"status":"ok","timestamp":1731256919295,"user_tz":420,"elapsed":8,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"aebf7ca9-22b8-4d82-dc10-f299a1a32b45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'score': 0.6076135039329529,\n"," 'start': 43,\n"," 'end': 58,\n"," 'answer': 'it was too wide'}"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["### 20.3.3 Machine Translation <a name='20.3.3-machine-translation'></a>\n","\n","For machine translation, we can provide source and target languages in the pipeline, as in the next cell where the task `\"translation_en_to_fr\"` is to translate text from English to French. Although this pipeline can work with several languages, most often, machine translation requires to specify the name of the used language model, and only for several special cases it can work by specifying only the task name."],"metadata":{"id":"TfOHAyjCgTPB"}},{"cell_type":"code","source":["translator = pipeline(\"translation_en_to_fr\", device=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJvDGaAqgfZN","executionInfo":{"status":"ok","timestamp":1731256926875,"user_tz":420,"elapsed":7585,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6bf11ce2-8c0d-423b-adb5-c14a5b1b155c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}]},{"cell_type":"code","source":["translator(\"I am a student\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ab1zUwkSjcf7","executionInfo":{"status":"ok","timestamp":1731256928759,"user_tz":420,"elapsed":1887,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"08871119-9057-4556-9e66-b6a0f3a964e5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'translation_text': 'Je suis un étudiant'}]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["translator(\"Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XV3J4sQkjfub","executionInfo":{"status":"ok","timestamp":1731256929605,"user_tz":420,"elapsed":850,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"9ed8fdad-7da5-47a4-f552-a18cb5759125"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'translation_text': 'Peyton Manning est devenu le premier quarterback à conduire deux équipes différentes à plusieurs Super Bowls.'}]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### 20.3.4 Text Summarization <a name='20.3.4-text-summarization'></a>\n","\n","Text summarization reduces a longer text into a shorter summary."],"metadata":{"id":"1Z82y4RvXZmv"}},{"cell_type":"code","source":["summarizer = pipeline(\"summarization\", device=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E8uuy0L_afDZ","executionInfo":{"status":"ok","timestamp":1731256938465,"user_tz":420,"elapsed":8910,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"2bfbe392-f971-41fa-a223-87f5dd4de1ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}]},{"cell_type":"code","source":["summarizer(\n","    \"\"\"\n","    America has changed dramatically during recent years. Not only has the number of\n","    graduates in traditional engineering disciplines such as mechanical, civil,\n","    electrical, chemical, and aeronautical engineering declined, but in most of\n","    the premier American universities engineering curricula now concentrate on\n","    and encourage largely the study of engineering science. As a result, there\n","    are declining offerings in engineering subjects dealing with infrastructure,\n","    the environment, and related issues, and greater concentration on high\n","    technology subjects, largely supporting increasingly complex scientific\n","    developments. While the latter is important, it should not be at the expense\n","    of more traditional engineering.\n","\n","    Rapidly developing economies such as China and India, as well as other\n","    industrial countries in Europe and Asia, continue to encourage and advance\n","    the teaching of engineering. Both China and India, respectively, graduate\n","    six and eight times as many traditional engineers as does the United States.\n","    Other industrial countries at minimum maintain their output, while America\n","    suffers an increasingly serious decline in the number of engineering graduates\n","    and a lack of well-educated engineers.\n","\"\"\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L2XgEDYVafHp","executionInfo":{"status":"ok","timestamp":1731256939379,"user_tz":420,"elapsed":916,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"39d5495e-f808-4152-a007-1185b271ce7a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'summary_text': ' The number of engineering graduates in the United States has declined in recent years . China and India graduate six and eight times as many traditional engineers as the U.S. does . Rapidly developing economies such as China continue to encourage and advance the teaching of engineering . There are declining offerings in engineering subjects dealing with infrastructure, infrastructure, the environment, and related issues .'}]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Specifying the `min_length` and `max_length` arguments allows to control the length of the summary."],"metadata":{"id":"y3A7Zx5hdKed"}},{"cell_type":"code","source":["summarizer(\n","    \"\"\"\n","    Flooding on the Yangtze river remains serious although water levels on parts of the river decreased\n","    today, according to the state headquarters of flood control and drought relief .\n","    \"\"\", min_length=8, max_length=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLwEfNhpbtY_","executionInfo":{"status":"ok","timestamp":1731256939571,"user_tz":420,"elapsed":194,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"5e4f3fe9-1f1c-4a78-b0ad-b93b4b4fa8bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'summary_text': ' Flooding on the Yangtze river remains serious although water levels on parts of the'}]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["summarizer(\n","    \"\"\"BAGHDAD -- Archaeologists in northern Iraq last week unearthed 2,700-year-old rock carvings featuring war scenes and trees from the Assyrian Empire, an archaeologist said Wednesday.\n","    The carvings on marble slabs were discovered by a team of experts in Mosul, Iraq’s second-largest city, who have been working to restore the site of the ancient Mashki Gate, which was bulldozed by Islamic State group militants in 2016.\n","    Fadhil Mohammed, head of the restoration works, said the team was surprised by discovering “eight murals with inscriptions, decorative drawings and writings.”\n","    Mashki Gate was one of the largest gates of Nineveh, an ancient Assyrian city of this part of the historic region of Mesopotamia.\n","    The discovered carvings show, among other things, a fighter preparing to fire an arrow while others show palm trees.\n","    \"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQM9vSHgc5vB","executionInfo":{"status":"ok","timestamp":1731256940571,"user_tz":420,"elapsed":1002,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"b2214e21-a422-40de-9244-e94b7c49e5e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'summary_text': ' The carvings on marble slabs were discovered by a team of experts in Mosul, Iraq’s second-largest city . They have been working to restore the site of the ancient Mashki Gate, which was bulldozed by Islamic State group militants in 2016 . Mashki gate was one of the largest gates of Nineveh, an ancient Assyrian city of this part of Mesopotamia .'}]"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["### 20.3.5 Text Generation <a name='20.3.5-text-generation'></a>"],"metadata":{"id":"Dau77YkzJriW"}},{"cell_type":"markdown","source":["In this example, we will use the `\"text-generation\"` pipeline to generate text based on a provided prompt."],"metadata":{"id":"TcylhfOxIcC4"}},{"cell_type":"code","source":["generator = pipeline(\"text-generation\", device=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-W_ArxleIC0C","executionInfo":{"status":"ok","timestamp":1731256944375,"user_tz":420,"elapsed":3810,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"475fbe25-8df7-4206-f173-fdb630ff980f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}]},{"cell_type":"markdown","source":["\n","Now let’s provide a prompt text to the generator object, and the generator will continue the text. Note that text generation involves randomness, so some of the outputs will not be perfect. And admittedly, this is one of the most difficult NLP tasks."],"metadata":{"id":"HJ_Jv-SMVTX2"}},{"cell_type":"code","source":["outputs_1 = generator(\"In this course, we will teach you how to\")\n","\n","print(outputs_1[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kN1bhbjQIC2F","executionInfo":{"status":"ok","timestamp":1731256944549,"user_tz":420,"elapsed":179,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"671b11a2-ef08-4cec-bed4-c8f035a6952c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["In this course, we will teach you how to design beautiful, sophisticated, and complex web pages with CSS. We will use CSS, and will create pages that look like real pages, so you can build beautiful, real web pages, and then show\n"]}]},{"cell_type":"code","source":["outputs_2 = generator(\"Niagara Falls is a city located in\")\n","\n","print(outputs_2[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gp4sTF8pMAKI","executionInfo":{"status":"ok","timestamp":1731256944715,"user_tz":420,"elapsed":170,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"41a862b8-2813-43fc-a070-6cd8db9a3d49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Niagara Falls is a city located in Northern Ontario from Long Island to Vancouver. The city is home to a variety of local artists with over 8,500 employees working as a team to create and deliver art for everyday people.\n","\n","Art and Architecture\n"]}]},{"cell_type":"code","source":["outputs_3 = generator(\"Niagara Falls is a famous world attractation\")\n","\n","print(outputs_3[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJ_ml-s1Oalb","executionInfo":{"status":"ok","timestamp":1731256945205,"user_tz":420,"elapsed":494,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"b45e6ac4-6be8-481c-8700-2f129f10c0f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Niagara Falls is a famous world attractation; an excellent view of Niagara Falls on the Great Lakes. The Niagara Falls is one of the only places where Niagara is a river from Niagara Falls near the mainland of the city. The Niagara Falls is now\n"]}]},{"cell_type":"markdown","source":["### 20.3.6 Named Entity Recognition <a name='20.3.6-named-entity-recognition'></a>\n","\n","Named Entity Recognition (NER), also known as named entity tagging, is a task of identifying parts of the input that represent entities. Examples of entities are:\n","\n","- Location (LOC)\n","- Organizations (ORG)\n","- Persons (PER)\n","- Miscellaneous entities (MISC)"],"metadata":{"id":"aYISR_J8kWZQ"}},{"cell_type":"code","source":["ner = pipeline(\"ner\", grouped_entities=True, device=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QP0NdU4w-Qz","executionInfo":{"status":"ok","timestamp":1731256953608,"user_tz":420,"elapsed":8407,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a01a0033-ed39-4e39-ea63-b66beb8fbab1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["text_1 = \"Abraham Lincoln was a president who lived in the United States.\"\n","\n","print(ner(text_1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AcAecdhqxGkJ","executionInfo":{"status":"ok","timestamp":1731256953608,"user_tz":420,"elapsed":8,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"72bd707a-25e0-4c0d-f6ad-0540e0601116"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'entity_group': 'PER', 'score': 0.9988935, 'word': 'Abraham Lincoln', 'start': 0, 'end': 15}, {'entity_group': 'LOC', 'score': 0.99965084, 'word': 'United States', 'start': 49, 'end': 62}]\n"]}]},{"cell_type":"markdown","source":["Or, we can use Pandas to display the output."],"metadata":{"id":"cYtM9d4dxgVY"}},{"cell_type":"code","source":["import pandas as pd\n","\n","pd.DataFrame(ner(text_1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"3no833EHxZsn","executionInfo":{"status":"ok","timestamp":1731256953609,"user_tz":420,"elapsed":8,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"189dfdf0-318f-4433-88c7-da16868f9023"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  entity_group     score             word  start  end\n","0          PER  0.998893  Abraham Lincoln      0   15\n","1          LOC  0.999651    United States     49   62"],"text/html":["\n","  <div id=\"df-fb0fd7e6-bd20-4985-b5bf-9b451937ccd2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>entity_group</th>\n","      <th>score</th>\n","      <th>word</th>\n","      <th>start</th>\n","      <th>end</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>PER</td>\n","      <td>0.998893</td>\n","      <td>Abraham Lincoln</td>\n","      <td>0</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LOC</td>\n","      <td>0.999651</td>\n","      <td>United States</td>\n","      <td>49</td>\n","      <td>62</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb0fd7e6-bd20-4985-b5bf-9b451937ccd2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fb0fd7e6-bd20-4985-b5bf-9b451937ccd2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fb0fd7e6-bd20-4985-b5bf-9b451937ccd2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0cc8b108-3f52-4e8f-9872-bcfec3f2d64e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0cc8b108-3f52-4e8f-9872-bcfec3f2d64e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0cc8b108-3f52-4e8f-9872-bcfec3f2d64e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"pd\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"entity_group\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"LOC\",\n          \"PER\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9996508359909058,\n          0.9988934993743896\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"United States\",\n          \"Abraham Lincoln\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 34,\n        \"min\": 0,\n        \"max\": 49,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          49,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33,\n        \"min\": 15,\n        \"max\": 62,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          62,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["text_2 = \"\"\"BAGHDAD -- Archaeologists in northern Iraq last week unearthed 2,700-year-old rock carvings featuring war scenes and trees from the Assyrian Empire, an archaeologist said Wednesday.\n","    The carvings on marble slabs were discovered by a team of experts in Mosul, Iraq’s second-largest city, who have been working to restore the site of the ancient Mashki Gate, which was bulldozed by Islamic State group militants in 2016.\n","    Fadhil Mohammed, head of the restoration works, said the team was surprised by discovering “eight murals with inscriptions, decorative drawings and writings.”\n","    Mashki Gate was one of the largest gates of Nineveh, an ancient Assyrian city of this part of the historic region of Mesopotamia.\n","    The discovered carvings show, among other things, a fighter preparing to fire an arrow while others show palm trees.\n","    \"\"\"\n","\n","pd.DataFrame(ner(text_2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":457},"id":"ZzZ3GpFCxnAe","executionInfo":{"status":"ok","timestamp":1731256953803,"user_tz":420,"elapsed":200,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"5c72344c-6667-406c-a71c-2fdefb16e745"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   entity_group     score             word  start  end\n","0           LOC  0.434807               BA      0    2\n","1           LOC  0.999473             Iraq     38   42\n","2          MISC  0.893630         Assyrian    132  140\n","3           LOC  0.782092           Empire    141  147\n","4           LOC  0.999238            Mosul    255  260\n","5           LOC  0.999156             Iraq    262  266\n","6           LOC  0.971527      Mashki Gate    347  358\n","7           ORG  0.997262    Islamic State    383  396\n","8           PER  0.999300  Fadhil Mohammed    426  441\n","9           LOC  0.974939      Mashki Gate    589  600\n","10          LOC  0.975865          Nineveh    633  640\n","11         MISC  0.994617         Assyrian    653  661\n","12          LOC  0.976547      Mesopotamia    706  717"],"text/html":["\n","  <div id=\"df-501d4e0a-3832-4b38-95f0-9802be29f818\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>entity_group</th>\n","      <th>score</th>\n","      <th>word</th>\n","      <th>start</th>\n","      <th>end</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LOC</td>\n","      <td>0.434807</td>\n","      <td>BA</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LOC</td>\n","      <td>0.999473</td>\n","      <td>Iraq</td>\n","      <td>38</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>MISC</td>\n","      <td>0.893630</td>\n","      <td>Assyrian</td>\n","      <td>132</td>\n","      <td>140</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LOC</td>\n","      <td>0.782092</td>\n","      <td>Empire</td>\n","      <td>141</td>\n","      <td>147</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LOC</td>\n","      <td>0.999238</td>\n","      <td>Mosul</td>\n","      <td>255</td>\n","      <td>260</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>LOC</td>\n","      <td>0.999156</td>\n","      <td>Iraq</td>\n","      <td>262</td>\n","      <td>266</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LOC</td>\n","      <td>0.971527</td>\n","      <td>Mashki Gate</td>\n","      <td>347</td>\n","      <td>358</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>ORG</td>\n","      <td>0.997262</td>\n","      <td>Islamic State</td>\n","      <td>383</td>\n","      <td>396</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>PER</td>\n","      <td>0.999300</td>\n","      <td>Fadhil Mohammed</td>\n","      <td>426</td>\n","      <td>441</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>LOC</td>\n","      <td>0.974939</td>\n","      <td>Mashki Gate</td>\n","      <td>589</td>\n","      <td>600</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LOC</td>\n","      <td>0.975865</td>\n","      <td>Nineveh</td>\n","      <td>633</td>\n","      <td>640</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>MISC</td>\n","      <td>0.994617</td>\n","      <td>Assyrian</td>\n","      <td>653</td>\n","      <td>661</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>LOC</td>\n","      <td>0.976547</td>\n","      <td>Mesopotamia</td>\n","      <td>706</td>\n","      <td>717</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-501d4e0a-3832-4b38-95f0-9802be29f818')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-501d4e0a-3832-4b38-95f0-9802be29f818 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-501d4e0a-3832-4b38-95f0-9802be29f818');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-58df58d2-a408-40c0-b055-7d1f542ebee9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-58df58d2-a408-40c0-b055-7d1f542ebee9')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-58df58d2-a408-40c0-b055-7d1f542ebee9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"pd\",\n  \"rows\": 13,\n  \"fields\": [\n    {\n      \"column\": \"entity_group\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"MISC\",\n          \"PER\",\n          \"LOC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.9946165680885315,\n          0.9749393463134766,\n          0.4348071813583374\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Nineveh\",\n          \"Iraq\",\n          \"Mashki Gate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 239,\n        \"min\": 0,\n        \"max\": 706,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          653,\n          589,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 242,\n        \"min\": 2,\n        \"max\": 717,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          661,\n          600,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["### 20.3.7 Zero-shot Classification <a name='20.3.7-zero-shot-classification'></a>\n","\n","Zero-shot classification is a task to classify text documents, where the term\n","*zero-shot* classification refers to tasks for which a language model has not been trained. I.e., the model was not trained to classify documents using the provided type of labels in the next example."],"metadata":{"id":"gnmswDE02Or_"}},{"cell_type":"code","source":["classifier = pipeline(\"zero-shot-classification\", device=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBWehMtL2tTY","executionInfo":{"status":"ok","timestamp":1731256964407,"user_tz":420,"elapsed":10609,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"4b02c60b-2c61-4cd5-b72b-6b7e59402719"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["The pipeline allows us to list `candidate labels` to be used for the classification. For this example, the model returned confidence scores for each category, and the highest probability was assigned to the \"sports\" category."],"metadata":{"id":"8U1RWzt52zgd"}},{"cell_type":"code","source":["classifier(\n","    \"Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls.\",\n","    candidate_labels=[\"education\", \"politics\", \"business\", \"sports\"],\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pvf75PSh2tWB","executionInfo":{"status":"ok","timestamp":1731256964408,"user_tz":420,"elapsed":7,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"b3aa7440-dd2d-4b7f-baae-2191294be969"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'sequence': 'Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls.',\n"," 'labels': ['sports', 'business', 'education', 'politics'],\n"," 'scores': [0.9866245985031128,\n","  0.006729834247380495,\n","  0.0034621665254235268,\n","  0.003183396067470312]}"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["### 20.3.8 Mask Filling <a name='20.3.8-mask-filling'></a>\n","\n","The pipeline with the `fill-mask` task is used to fill in blanks in an input text."],"metadata":{"id":"NYVSf_mWB5cN"}},{"cell_type":"code","source":["mask_filling = pipeline(\"fill-mask\", device=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXoJ7ekJB4n5","executionInfo":{"status":"ok","timestamp":1731256967000,"user_tz":420,"elapsed":2598,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"502d0d43-1c9e-4f09-b5f3-065e2562940a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert/distilroberta-base and revision ec58a5b (https://huggingface.co/distilbert/distilroberta-base).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","source":["We can provide the `top_k` argument to indicate the number of returned answers."],"metadata":{"id":"a0E7QA4CBqDs"}},{"cell_type":"code","source":["mask_filling(\"Abraham Lincoln was a <mask> who lived in the United States.\", top_k=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eD9Br5uBCWas","executionInfo":{"status":"ok","timestamp":1731256967001,"user_tz":420,"elapsed":7,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"9f61b44a-b29e-400a-e89b-cc3ac8022705"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.31943777203559875,\n","  'token': 3661,\n","  'token_str': ' Democrat',\n","  'sequence': 'Abraham Lincoln was a Democrat who lived in the United States.'},\n"," {'score': 0.18588340282440186,\n","  'token': 1172,\n","  'token_str': ' Republican',\n","  'sequence': 'Abraham Lincoln was a Republican who lived in the United States.'},\n"," {'score': 0.035198844969272614,\n","  'token': 16495,\n","  'token_str': ' Jew',\n","  'sequence': 'Abraham Lincoln was a Jew who lived in the United States.'},\n"," {'score': 0.02870906889438629,\n","  'token': 24156,\n","  'token_str': ' Presbyterian',\n","  'sequence': 'Abraham Lincoln was a Presbyterian who lived in the United States.'},\n"," {'score': 0.02608238160610199,\n","  'token': 11593,\n","  'token_str': ' physician',\n","  'sequence': 'Abraham Lincoln was a physician who lived in the United States.'}]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["mask_filling(\"Flooding on the Yangtze river remains serious although <mask> levels on parts of the river decreased today.\", top_k=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kH0_LiKeB4lO","executionInfo":{"status":"ok","timestamp":1731256967001,"user_tz":420,"elapsed":6,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"ebc8e06c-06eb-4c59-b69b-7ba1a940bca7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.2372523695230484,\n","  'token': 514,\n","  'token_str': ' water',\n","  'sequence': 'Flooding on the Yangtze river remains serious although water levels on parts of the river decreased today.'},\n"," {'score': 0.13285547494888306,\n","  'token': 11747,\n","  'token_str': ' oxygen',\n","  'sequence': 'Flooding on the Yangtze river remains serious although oxygen levels on parts of the river decreased today.'}]"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## 20.4 Tokenizers <a name='20.4-tokenizers'></a>"],"metadata":{"id":"2XOmz0dFEQxj"}},{"cell_type":"markdown","source":["**Tokenizers** library in Hugging Face is used to split input text data into tokens (e.g., words, characters, N-grams), and map the tokens to integers. When we use a pretrained model from Hugging Face for a downstream task, our text data needs to be preprocessed in the same way as the training data used with the model. Therefore, we will need to download the tokenizer for that specific model.\n","\n","Let's consider the model `\"distilbert-base-uncased\"`, which is a version of the BERT transformer model, which takes case-insensitive English text as input data. Next, we will download the tokenizer for this model by using the `AutoTokenizer` class and its method `from_pretrained()`.  By using `AutoTokenizer` we don't need to manually download and manage the tokenizer files.\n","\n","Or, we can also specify the tokenizer name for the model that we wish to use. For instance, for the BERT model, Hugging Face has a `BertTokenizer` that can be directly imported from the `transformers` package."],"metadata":{"id":"mH66hloLQQdk"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","model = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model)"],"metadata":{"id":"8Jh17O93Ta7G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now use the `tokenizer` to convert text sentences into a sequence of integers, and display the output.\n","\n","The ouput of the tokenizer is a dictionary consisting of two key-value pairs:\n","\n","- `input_ids`, a list of integers, where each index identifies a token. The indexing is based on the vocabulary of the training data that was used to train the model `\"distilbert-base-uncased\"`.\n","- `attention_mask`, a list of 1's or 0's, to indicate padding of the text sequence. This sentence does not have padding, since all elements have an attention mask of 1's. The attention mask ensures that the attention mechanism in Transformer is applied only to the real tokens, and the padding tokens are ignored."],"metadata":{"id":"tegBL2hTVv2G"}},{"cell_type":"code","source":["output_tokens_1 = tokenizer('Tokenizing text is easy.')\n","print(output_tokens_1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMuXKBUZVtzU","executionInfo":{"status":"ok","timestamp":1731256967197,"user_tz":420,"elapsed":19,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"12a00ebe-7b6f-46ed-ee58-30a26663aeea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [101, 19204, 6026, 3793, 2003, 3733, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"]}]},{"cell_type":"markdown","source":["Note that the above output has 8 tokens, although the input sentence has 4 words and the period mark. To understand better how the tokenization was performed, in the next cell we used the method `covert_ids_to_tokens()` to obtain the text for each integer value. Now we can see that the tokenizer places special tokens at the beginning and end of each sequence. `[CLS]` is placed at the beginning (it stands for Classification), and `[SEP]` is placed at the end of the sequence (it stands for Separate).\n","\n","Also note that the gerund verb \"tokenizing\" is split into `'token'` and `'##izing'`. Using two tokens for the word allows to work with smaller vocabularies. I.e., instead of considering `token` and `tokenization` as two different words, by splitting the word into the root `token` and the suffix `ization`, the model will use two tokens that have a distinct semantic meaning. This approach of decomposing long words into subwords is especially efficient with some languages where one can form very long words by chaining simple subwords."],"metadata":{"id":"BJ6eWpDKXmqe"}},{"cell_type":"code","source":["tokenizer.convert_ids_to_tokens(output_tokens_1.input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AyqUqizTXiSd","executionInfo":{"status":"ok","timestamp":1731256967198,"user_tz":420,"elapsed":19,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"43f49463-74bb-4586-e6dc-b810a4fbf234"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]', 'token', '##izing', 'text', 'is', 'easy', '.', '[SEP]']"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["Most tokenizers in Hugging Face assign integers for the special tokens between 100 and 103.\n","\n","These special tokens include:\n","\n","- \\[PAD\\], padding.\n","- \\[UNK\\], unknown token.\n","- \\[CLS\\], sequence beginning.\n","- \\[SEP\\], sequence end.\n","- \\[MASK\\], masked tokens."],"metadata":{"id":"w1OSYSYBin1u"}},{"cell_type":"code","source":["tokenizer.convert_ids_to_tokens([0, 100, 101, 102, 103])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N237SDYjhuk0","executionInfo":{"status":"ok","timestamp":1731256967198,"user_tz":420,"elapsed":18,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f9fd788a-49e6-475d-b240-a58e67a38138"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["Another simple example is provided next. We can see that the word `'Transformer'` is tokenized as `'transform'` + `'##er'`."],"metadata":{"id":"TiYWpD4T_tzr"}},{"cell_type":"code","source":["output_tokens_2 = tokenizer('Using a Transformer network in Hugging Face is simple')\n","print(output_tokens_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oiJ8JGoB_2Wo","executionInfo":{"status":"ok","timestamp":1731256967198,"user_tz":420,"elapsed":16,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6d1e75c8-9525-408e-b658-5a5450dcc9ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [101, 2478, 1037, 10938, 2121, 2897, 1999, 17662, 2227, 2003, 3722, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}]},{"cell_type":"code","source":["tokenizer.convert_ids_to_tokens(output_tokens_2.input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9GzIeB0ABgf","executionInfo":{"status":"ok","timestamp":1731256967198,"user_tz":420,"elapsed":14,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a1ab04b0-f684-47bc-f900-e1487d64d91a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," 'using',\n"," 'a',\n"," 'transform',\n"," '##er',\n"," 'network',\n"," 'in',\n"," 'hugging',\n"," 'face',\n"," 'is',\n"," 'simple',\n"," '[SEP]']"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["Tokenizers can be applied to multiple text sequences. The argument `padding=True` is used below to pad the sequences to the longest sequence. Note below that 0's are added to pad the second sentence."],"metadata":{"id":"7NFyFBEtjoYz"}},{"cell_type":"code","source":["text_3 =  [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n","\n","output_tokens_3 = tokenizer(text_3, padding=True)"],"metadata":{"id":"NZUkYBXjhyry"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(output_tokens_3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XkasEZgSiJ5w","executionInfo":{"status":"ok","timestamp":1731256967198,"user_tz":420,"elapsed":12,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7c25d752-3ca3-4cd0-e6cc-2c22d867a42a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"]}]},{"cell_type":"markdown","source":["The output is more readable if we print it line by line."],"metadata":{"id":"aigtTmJxZsCU"}},{"cell_type":"code","source":["print(\"Input IDs\")\n","for item in output_tokens_3.input_ids:\n","    print(item)\n","\n","print(\"Attention Mask\")\n","for item in output_tokens_3.attention_mask:\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DrazdOxqh_t3","executionInfo":{"status":"ok","timestamp":1731256967198,"user_tz":420,"elapsed":10,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"cf015748-b73d-49e8-af3c-8c96f69ac694"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input IDs\n","[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n","[101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n","Attention Mask\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"markdown","source":["Note also that each sequence of tokens begins with 101 (`'[CLS]'`) and ends with  102 (`'[SEP]'`).\n","\n","If `max_length` is provided, the tokenizer will truncate longer sentences to the specified length, as in the example in the next cell."],"metadata":{"id":"bbLZbcH5AkOM"}},{"cell_type":"code","source":["output_tokens_4 = tokenizer(text_3, padding=False, max_length=10)\n","\n","print(\"Input IDs\")\n","for item in output_tokens_4.input_ids:\n","    print(item)\n","\n","print(\"Attention Mask\")\n","for item in output_tokens_4.attention_mask:\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EwD6z_vrGkTe","executionInfo":{"status":"ok","timestamp":1731256967198,"user_tz":420,"elapsed":8,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a4087c4c-fd28-48c8-efcd-bbb5e892316f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Input IDs\n","[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 102]\n","[101, 1045, 5223, 2023, 2061, 2172, 999, 102]\n","Attention Mask\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","[1, 1, 1, 1, 1, 1, 1, 1]\n"]}]},{"cell_type":"markdown","source":["The Tokenizers library also allows to train new tokenizers from scratch. For instance, if a large corpus of text is available in another language than English, a new tokenizer will need to be trained to efficiently deal with the differences in the punctuation and use of spaces in that language."],"metadata":{"id":"KENAhJA-HpDz"}},{"cell_type":"markdown","source":["## 20.5 Datasets <a name='20.5-datasets'></a>"],"metadata":{"id":"TIQybZu4kliK"}},{"cell_type":"markdown","source":["Hugging Face provides access to a large number of **datasets**. If you wish to check all datasets please follow this [link](https://huggingface.co/datasets).\n","\n","To use the `datasets` library in Google Colab, we need to first install it."],"metadata":{"id":"N6N0cbbjItLB"}},{"cell_type":"code","source":["!pip install -q datasets fsspec"],"metadata":{"id":"xbiQCiZORwxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","print(datasets.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9fdoLSyteol","executionInfo":{"status":"ok","timestamp":1731256971205,"user_tz":420,"elapsed":7,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"88f56b81-2b87-427d-c96c-8513d6fa0723"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.1.0\n"]}]},{"cell_type":"markdown","source":["Let's load the Emotions dataset. It contains Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. We can just use `load_dataset()` to accomplish that."],"metadata":{"id":"CnNMHQ2nKC7x"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","emotions = load_dataset(\"emotion\")"],"metadata":{"id":"-Guq9Z6wK1w3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see in the next cell that `emotions` dataset is a dictionary object that is split into training, validation, and test data sets, consisting of 16,000, 2,000, and 2,000 messages, respectively."],"metadata":{"id":"wHT8qOP_K88J"}},{"cell_type":"code","source":["emotions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r5b12OuKK1zT","executionInfo":{"status":"ok","timestamp":1731256975714,"user_tz":420,"elapsed":24,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"9134590c-7031-4e29-a182-e0bfbc80c5a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 16000\n","    })\n","    validation: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 2000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 2000\n","    })\n","})"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["The first and second training samples are shown next. They contain the text and the corresponding emotion label."],"metadata":{"id":"4xLmkX3FL-GA"}},{"cell_type":"code","source":["emotions['train'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xH-F4_s0K11C","executionInfo":{"status":"ok","timestamp":1731256975714,"user_tz":420,"elapsed":23,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"ffbd650f-e061-4c38-c949-dcff97c34bc6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': 'i didnt feel humiliated', 'label': 0}"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["emotions['train'][1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K7rETE-2L2cM","executionInfo":{"status":"ok","timestamp":1731256975715,"user_tz":420,"elapsed":22,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"9a28fd26-60ff-413a-db56-4aea39553bc9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n"," 'label': 0}"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["The order of the labels for the emotion categories are shown in the next cell."],"metadata":{"id":"NeUCFqSqMY9I"}},{"cell_type":"code","source":["emotions['train'].features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3U8k3Y5oMZQS","executionInfo":{"status":"ok","timestamp":1731256975715,"user_tz":420,"elapsed":21,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6b247c42-2e31-48e8-9296-7c516979d670"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': Value(dtype='string', id=None),\n"," 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["Hugging Face also allows to use the `set_format()` method with datasets, and to define the format of the data. For instance, by setting the type to Pandas, we can obtain the data as Pandas DataFrames."],"metadata":{"id":"A_GfT28FNmP_"}},{"cell_type":"code","source":["emotions.set_format(type='pandas')\n","df = emotions[\"train\"][:]\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"m6JsU5svMZS0","executionInfo":{"status":"ok","timestamp":1731256975715,"user_tz":420,"elapsed":19,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"0ed821fc-73ef-4e8c-b246-16ca3ef143a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                text  label\n","0                            i didnt feel humiliated      0\n","1  i can go from feeling so hopeless to so damned...      0\n","2   im grabbing a minute to post i feel greedy wrong      3\n","3  i am ever feeling nostalgic about the fireplac...      2\n","4                               i am feeling grouchy      3"],"text/html":["\n","  <div id=\"df-50ebe662-58db-4a4e-a5ff-7f712e3850e4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i didnt feel humiliated</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i can go from feeling so hopeless to so damned...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>im grabbing a minute to post i feel greedy wrong</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i am ever feeling nostalgic about the fireplac...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i am feeling grouchy</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50ebe662-58db-4a4e-a5ff-7f712e3850e4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-50ebe662-58db-4a4e-a5ff-7f712e3850e4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-50ebe662-58db-4a4e-a5ff-7f712e3850e4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-8495ac35-4fca-430f-8b9a-6044feecc567\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8495ac35-4fca-430f-8b9a-6044feecc567')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-8495ac35-4fca-430f-8b9a-6044feecc567 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 16000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15969,\n        \"samples\": [\n          \"i feel rather imbicilic or at least complacent\",\n          \"i was in the bathroom i had sat down to pee it was to make me feel submissive again per instructions\",\n          \"i am thrilled with the way my skin and hair feel if you are like me you are skeptical\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["We can use a bar plot to plot the number of values in each category."],"metadata":{"id":"inoYCH0vQf_u"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","df[\"label\"].value_counts(ascending=True).plot.barh()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"Cd1cAMoSQCC6","executionInfo":{"status":"ok","timestamp":1731256975717,"user_tz":420,"elapsed":20,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6e812aa6-f061-4dba-f28f-c00410b47d06"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["### Apply a Tokenizer to a Dataset\n","\n","Let's show next how we can use a tokenizer to convert the first 5 samples in the training dataset to sequences of tokens. Since in the previous cells we set the format of the dataset to `pandas`, we need to first re-set the format to `type=None`."],"metadata":{"id":"8ThxVoBZSggu"}},{"cell_type":"code","source":["emotions.set_format(type=None)\n","\n","training_samples_5 = emotions[\"train\"][:5]\n","training_samples_5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6I-xkx8R6O6","executionInfo":{"status":"ok","timestamp":1731256975718,"user_tz":420,"elapsed":20,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"cc02604c-96e8-444d-bcf9-116b7b9cf6a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': ['i didnt feel humiliated',\n","  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n","  'im grabbing a minute to post i feel greedy wrong',\n","  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n","  'i am feeling grouchy'],\n"," 'label': [0, 0, 3, 2, 3]}"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":["Let's extract only the text data, and not the labels."],"metadata":{"id":"dIyhVhDIS0HF"}},{"cell_type":"code","source":["text_training_5 = training_samples_5['text']\n","text_training_5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dz8BJ6J1SJm7","executionInfo":{"status":"ok","timestamp":1731256975718,"user_tz":420,"elapsed":17,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7ccc6b4c-a587-4483-fc61-6af4d31ee538"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i didnt feel humiliated',\n"," 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n"," 'im grabbing a minute to post i feel greedy wrong',\n"," 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n"," 'i am feeling grouchy']"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["Apply the tokenizer, and display the sequence of tokens."],"metadata":{"id":"CRP0h75PTQoV"}},{"cell_type":"code","source":["output_tokens_5 = tokenizer(text_training_5, padding=True)\n","\n","print(\"Input IDs\")\n","for item in output_tokens_5.input_ids:\n","    print(item)\n","\n","print(\"Attention Mask\")\n","for item in output_tokens_5.attention_mask:\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7cbhfZvcRF8h","executionInfo":{"status":"ok","timestamp":1731256975718,"user_tz":420,"elapsed":15,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6340fa44-2ca1-4dec-b355-0a5a5dd48501"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input IDs\n","[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]\n","[101, 10047, 9775, 1037, 3371, 2000, 2695, 1045, 2514, 20505, 3308, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[101, 1045, 2572, 2412, 3110, 16839, 9080, 12863, 2055, 1996, 13788, 1045, 2097, 2113, 2008, 2009, 2003, 2145, 2006, 1996, 3200, 102, 0]\n","[101, 1045, 2572, 3110, 24665, 7140, 11714, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Attention Mask\n","[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"markdown","source":["## 20.6 Models <a name='20.6-models'></a>"],"metadata":{"id":"bFNzAfcc-1kL"}},{"cell_type":"markdown","source":["### Importing a Pretrained Model\n","\n","Instead of using a default model for a task, we can also select a language model from the many available in Hugging Face. Let's use GPT-2 language model, and we will also need to use the corresponding tokenizer for GPT-2.\n","\n","The parameter `pad_token_id` is optional, and its purpose is to define the ID (assigned integer value) for the token used for padding the text sequences. In this case, the ID of the padding token is set to the ID of the end-of-sequence token (`eos_token_id`), which is a common choice for padding in language models."],"metadata":{"id":"OgvhTDO2EOXI"}},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel\n","from transformers import GPT2Tokenizer\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"],"metadata":{"id":"bcZLtN4iDM9u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731256976291,"user_tz":420,"elapsed":587,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"cd691734-666b-4430-d5f8-a41d3f03a300"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Now, let's consider the following sentence `input_string`, and first preprocess it with the tokenizer to `encode` it into a sequence of integers. The argument `return_tensors=\"pt\"` specifies that the output should be returned as PyTorch tensors, where `\"pt\"` stands for PyTorch.\n","\n","Next, we will use the GPT-2 model for text generation, to continue the input sentences. The `decode` method will convert the generated sequence by the model into text, here named `output_string`.\n","\n","Two common methods for generating text with Language Models are:\n","\n"," - **Greedy search**, selects the word with the highest probability as the next word. The major drawback of greedy search is that it can miss potentially high-probability words that follow a low-probability word. Therefore, although each individual word may be the best fit when generating a response, the entire generated text can be less relevant for the query.\n"," - **Beam search**, selects a sequence of words (beam) that has the overall highest probability. This approach reduces the risk of missing high-probability words, because rather than focusing only on the next word in a sequence, beam search looks at the probability of the entire response.\n","\n","Beam search is typically preferred over greedy search, because with beam search, the model can consider multiple routes and find the best option.\n","\n","For example, in the following figure, the input query to the model is \"The Financial Times is ...\". The model created four possible beams with a potential response, and out of the four beams, the third beam \"a newspaper founded in 1888\" was selected as the most coherent human-like response.\n","\n","<img src='images/beam_search.png' width=500px>\n","\n","*Figure: Beam search example.* Source: [link](https://ig.ft.com/generative-ai/)"],"metadata":{"id":"1Zk74QuN-glv"}},{"cell_type":"code","source":["input_string = \"Yesterday I spent several hours in the library, studying\"\n","input_tokens = tokenizer.encode(input_string, return_tensors=\"pt\")\n","\n","output_greedy = model.generate(input_tokens, max_length=64)\n","\n","output_string = tokenizer.decode(output_greedy[0], skip_special_tokens=True)\n","print(f\"Output sequence: {output_string}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLmAGyZBrFN4","executionInfo":{"status":"ok","timestamp":1731256983578,"user_tz":420,"elapsed":7290,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7e1ef5a7-14e5-4095-8d73-8f7dbea85f42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"output_type":"stream","name":"stdout","text":["Output sequence: Yesterday I spent several hours in the library, studying the books, and I was amazed at how much I had learned. I was amazed at how much I had learned. I was amazed at how much I had learned. I was amazed at how much I had learned. I was amazed at how much I had learned.\n"]}]},{"cell_type":"markdown","source":["You can notice that the generated text sequence with greedy search is not the best, since after the initial sequence of words, the model got stuck into a loop, and began repeating the same sentence. To deal with repetitive text generation we can apply a beam search strategy. Beam search examines multiple probable solutions (beams) for the generated text, which is set by the argument `num_beams=32`. We also applied a penalty term for repeating the same words. By considering several possible solutions, the beam search algorithm tries to improve the generated output text by the model."],"metadata":{"id":"b7RD1p8O_Hf1"}},{"cell_type":"code","source":["output_beam = model.generate(input_tokens, max_length=64, num_beams=32, \\\n","       no_repeat_ngram_size=2, early_stopping=True)\n","\n","output_string = tokenizer.decode(output_beam[0], skip_special_tokens=True)\n","\n","print(\"Output sequence:\\n\", output_string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEhwRR2ADdd8","executionInfo":{"status":"ok","timestamp":1731257012932,"user_tz":420,"elapsed":29356,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"a2d089bf-4dce-45bc-8209-de55ac65c385"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output sequence:\n"," Yesterday I spent several hours in the library, studying all the books I could get my hands on, and trying to figure out what I wanted to do with them. I didn't know what to expect, but I did know that it was going to be a lot of fun.\n","\n","When I got home, I\n"]}]},{"cell_type":"markdown","source":["### Fine-tuning a Pretrained Model to the Emotions Dataset\n","\n","The next section demonstrates how to use a pretrained model and tokenizer in Hugging Face, and fine-tune the model to a dataset. We will use the DistilBERT model for this task, and we will again use the Emotions dataset.\n","\n","Let's first download the tokenizer for the `\"distilbert-base-uncased\"` model."],"metadata":{"id":"NWqaheyea4ru"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WcfcBoesA0wI"},"outputs":[],"source":["model_ckpt = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"]},{"cell_type":"markdown","metadata":{"id":"IBEn8UnruH-l"},"source":["The following code tokenizes the emotions dataset. It is common to use the `Dataset.map()` method in Hugging Face for tokenization, which applies a function on each sample in the dataset. Therefore, we first define a function `tokenize` that is used in the `Dataset.map()` method. Using the option `batched=True` in the `map()` method will apply the tokenization to a batch of input sequences instead of each text sequence, which will speed up the tokenization. And, as we learned in the previous section, `tokenize` returns a dictionary with keys `input_ids` and `attention_mask`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["6f5e7be66e1e4b1c8bde5566818ad7f4","998e5f9ba8c74fec9e5d53cad980497e","78f60cdc98bb44f79f14c541c6a68001","1407d3ede2db498994f2b810c1d3d8e2","867315888ed646b69a81475f437189b7","3bbe8913b21943ffa8fa3e2a516ce0b1","90eeac4eb8f64840bf8c2dd28370d9ca","f36f7717c6ee46c28178987dfa8d3148","9ed6be77d3bf4f438641b14110af1add","e4da98f605024e5f908411f4ed97cbc9","d834c51ba7214c43afb6dd8998ac2a6d"]},"executionInfo":{"elapsed":1677,"status":"ok","timestamp":1731257014789,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"},"user_tz":420},"outputId":"db4a0e40-102d-4b23-8065-ac682e89c099","id":"NSNnNKIsuH-l"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5e7be66e1e4b1c8bde5566818ad7f4"}},"metadata":{}}],"source":["def tokenize(rows):\n","    return tokenizer(rows['text'], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = emotions.map(tokenize, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vt8YC4WkuH-m"},"outputs":[],"source":["tokenized_train_dataset = tokenized_datasets[\"train\"]\n","tokenized_val_dataset = tokenized_datasets[\"validation\"]\n","tokenized_test_dataset = tokenized_datasets[\"test\"]"]},{"cell_type":"markdown","metadata":{"id":"6detUgMZuH-m"},"source":["`DataCollatorWithPadding` in Hugging Face pads the tokenized input samples during batches preparation to have the same length. I.e., it pads each sentence to the maximum length in each batch. This is referred to as dynamic padding, and it is more efficient than padding each sequence to a fixed length, because it only pads as much as necessary. By passing the tokenizer to `DataCollatorWithPadding`, the function can handle the specific padding requirements for the used model, such as special tokens like [PAD] that are used in the DistilBERT model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aj4Epp1u4ky"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"wJmw95uVu4k5"},"source":["Next, we will load the distilbert model for classification. Hugging Face Transformers library provides an `AutoModel` class which also has a `from_pretrained()` method that can be used to load a pretrained checkpoint."]},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BTC6B8G_hBS8","executionInfo":{"status":"ok","timestamp":1731257015249,"user_tz":420,"elapsed":463,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"834b84be-8073-4d0b-eff0-de5e7db2bc4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["Transformers library in Hugging Face provides a `Trainer` class for model training, which is similar to `model.fit` in Keras-TensorFlow. That is, we just need to pass all parameters that are needed for training, and afterwards run `Trainer.train()` to fit the model to the data. The input parameters are first defined with the class `TrainingArguments` as shown in the next cell. In this case, the Training Arguments inlcude the output directory to save the training outputs, evaluation strategy, learning rate, batch sizes for training and validation, number of training epochs, weight decay, and whether to report the results to external tools."],"metadata":{"id":"_Dul78wix3IN"}},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","# Define training arguments for the model\n","training_args = TrainingArguments(\n","    # Directory to save model checkpoints and logs\n","    output_dir=\"bert-emotion\",\n","    # Evaluation strategy - evaluate the model at the end of each epoch\n","    eval_strategy=\"epoch\",\n","    # Learning rate for the optimizer\n","    learning_rate=2e-5,\n","    # Batch size for training\n","    per_device_train_batch_size=16,\n","    # Batch size for evaluation\n","    per_device_eval_batch_size=16,\n","    # Number of training epochs\n","    num_train_epochs=2,\n","    # Weight decay to apply for regularization\n","    weight_decay=0.01,\n","    # Disable reporting to external tools (e.g., WandB, TensorBoard)\n","    report_to=\"none\"\n",")"],"metadata":{"id":"LxQJ-dFLhBQe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will also define a function called `compute_metrics` to calculate the accuracy at the end of each epoch."],"metadata":{"id":"aGZUx3T80JaB"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","# Function to compute metrics during training\n","def compute_metrics(eval_pred):\n","    outputs, labels = eval_pred\n","    predictions = outputs.argmax(axis=-1)\n","    acc = accuracy_score(labels, predictions)\n","    return {\"accuracy\": acc}"],"metadata":{"id":"eFq0kg_wvAPO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will define our `trainer` as an instance of the `Trainer` class by passing the model that we defined, the training arguments, the training and validation datasets, tokenizer, data collator, and the function for computing performance metrics (accuracy). Afterward, we will initialize the training by running `trainer.train()` as in the following cell. We can see that the loss and accuracy values are reported after each epoch."],"metadata":{"id":"Z7DCOvh_0gbU"}},{"cell_type":"code","source":["from transformers import Trainer\n","\n","# Initialize the Trainer with the required parameters\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics\n",")"],"metadata":{"id":"kcQTMOaIhNsD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"PabgljFshhzd","executionInfo":{"status":"ok","timestamp":1731258654986,"user_tz":420,"elapsed":1637013,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"d4060379-e289-4f2c-b0aa-ba88880bf2f9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2000/2000 27:15, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.256300</td>\n","      <td>0.192347</td>\n","      <td>0.927000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.151500</td>\n","      <td>0.162524</td>\n","      <td>0.933500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=2000, training_loss=0.32112478637695313, metrics={'train_runtime': 1636.8576, 'train_samples_per_second': 19.55, 'train_steps_per_second': 1.222, 'total_flos': 4239259140096000.0, 'train_loss': 0.32112478637695313, 'epoch': 2.0})"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","source":["Next, let's evaluate the model performance on the test dataset. The calculated accuracy is 92.5%."],"metadata":{"id":"b8dOzX7n5HYN"}},{"cell_type":"code","source":["# Evaluate the model on the test dataset\n","test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n","\n","# Print the evaluation metrics\n","print(test_metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"vJJpQVJJuHI0","executionInfo":{"status":"ok","timestamp":1731258689826,"user_tz":420,"elapsed":34843,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"0044aa70-3610-48b6-f50f-9ab9d858ad59"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [125/125 00:34]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.1821223944425583, 'eval_accuracy': 0.924, 'eval_runtime': 34.8139, 'eval_samples_per_second': 57.448, 'eval_steps_per_second': 3.591, 'epoch': 2.0}\n"]}]},{"cell_type":"markdown","source":["In addition, let's predict the class labels for the four sentences shown below. First, we will need to apply the tokenizer to prepare the sentences."],"metadata":{"id":"EsEFGYMyzJIi"}},{"cell_type":"code","source":["# List of sentences to evaluate\n","test_texts = [\"I feel fantastic!\", \"I'm a bit sad today.\", \"This is so exciting!\", \"I'm very stressed out.\"]\n","\n","# Tokenize the sentences, pad them to the same length for batch processing\n","inputs = tokenizer(test_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device=0)\n","inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWtjA94Q6rdO","executionInfo":{"status":"ok","timestamp":1731258689826,"user_tz":420,"elapsed":16,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f295d2d9-ff90-44f3-f5b8-93b5aa0356be"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[  101,  1045,  2514, 10392,   999,   102,     0,     0,     0,     0],\n","        [  101,  1045,  1005,  1049,  1037,  2978,  6517,  2651,  1012,   102],\n","        [  101,  2023,  2003,  2061, 10990,   999,   102,     0,     0,     0],\n","        [  101,  1045,  1005,  1049,  2200, 13233,  2041,  1012,   102,     0]],\n","       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')}"]},"metadata":{},"execution_count":62}]},{"cell_type":"markdown","source":["In the next cell, we pass the inputs to the model, and use `argmax` to calculate the predicted labels for the sentences."],"metadata":{"id":"EB0bReBm8Pr0"}},{"cell_type":"code","source":["# Forward pass through the model\n","outputs = model(**inputs)\n","\n","# Get predicted labels for each sentence\n","predicted_labels = outputs.logits.argmax(axis=-1).cpu().tolist()\n","print(\"Predicted Labels:\", predicted_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOGpPAoOzUBe","executionInfo":{"status":"ok","timestamp":1731258689827,"user_tz":420,"elapsed":15,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"84166b65-5744-4fd1-8d82-0dfb5537ed47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Labels: [1, 0, 1, 3]\n"]}]},{"cell_type":"markdown","source":["To understand the predicted labels, let's retrieve the class names, and print the sentences with the class names of the predicted labels."],"metadata":{"id":"ivJxZT1o8izU"}},{"cell_type":"code","source":["# Emotion class names\n","class_names = emotions[\"train\"].features[\"label\"].names\n","class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DT6FGwJFuHVS","executionInfo":{"status":"ok","timestamp":1731258689827,"user_tz":420,"elapsed":12,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"6114b425-7616-46f8-c6c6-4d7965b062c4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["# Print results\n","for text, label in zip(test_texts, predicted_labels):\n","    print(f\"Text: '{text}'; Predicted Label: {class_names[label]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSj_p-207kZy","executionInfo":{"status":"ok","timestamp":1731258689827,"user_tz":420,"elapsed":10,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"f4cdf6b5-bc1e-4a98-bc84-c53d30f07670"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Text: 'I feel fantastic!'; Predicted Label: joy\n","Text: 'I'm a bit sad today.'; Predicted Label: sadness\n","Text: 'This is so exciting!'; Predicted Label: joy\n","Text: 'I'm very stressed out.'; Predicted Label: anger\n"]}]},{"cell_type":"markdown","metadata":{"id":"vweobvFVe4RB"},"source":["## References <a name='references'></a>\n","\n","1. Hugging Face Course, available at [https://huggingface.co/course/chapter1/1](https://huggingface.co/course/chapter1/1).\n","2. Applications of Deep Neural Networks, Course at Washington University in St. Louis, Jeff Heaton, available at [https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_01_huggingface.ipynb](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_01_huggingface.ipynb).\n","3. Getting Started with Hugging Face Transformers for NLP, Exxact Blog, available at [https://www.exxactcorp.com/blog/Deep-Learning/getting-started-hugging-face-transformers](https://www.exxactcorp.com/blog/Deep-Learning/getting-started-hugging-face-transformers).\n","4. An Introduction to Using Transformers and Hugging Face, Zoumana Kelta, available at [https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face](https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face)."]},{"cell_type":"markdown","metadata":{"id":"bDbzcXmWe4RB"},"source":["[BACK TO TOP](#top)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1XdYOggN-wY1KLZ3O2n2oO4U1swJnXLKg","timestamp":1699134970599},{"file_id":"1_wSiz1ALZ1l9vda7yvvdLgEHHE8M0F66","timestamp":1666471615676}],"gpuType":"T4","gpuClass":"premium"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6f5e7be66e1e4b1c8bde5566818ad7f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_998e5f9ba8c74fec9e5d53cad980497e","IPY_MODEL_78f60cdc98bb44f79f14c541c6a68001","IPY_MODEL_1407d3ede2db498994f2b810c1d3d8e2"],"layout":"IPY_MODEL_867315888ed646b69a81475f437189b7"}},"998e5f9ba8c74fec9e5d53cad980497e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bbe8913b21943ffa8fa3e2a516ce0b1","placeholder":"​","style":"IPY_MODEL_90eeac4eb8f64840bf8c2dd28370d9ca","value":"Map: 100%"}},"78f60cdc98bb44f79f14c541c6a68001":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f36f7717c6ee46c28178987dfa8d3148","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ed6be77d3bf4f438641b14110af1add","value":2000}},"1407d3ede2db498994f2b810c1d3d8e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4da98f605024e5f908411f4ed97cbc9","placeholder":"​","style":"IPY_MODEL_d834c51ba7214c43afb6dd8998ac2a6d","value":" 2000/2000 [00:01&lt;00:00, 1460.19 examples/s]"}},"867315888ed646b69a81475f437189b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bbe8913b21943ffa8fa3e2a516ce0b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90eeac4eb8f64840bf8c2dd28370d9ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f36f7717c6ee46c28178987dfa8d3148":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ed6be77d3bf4f438641b14110af1add":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4da98f605024e5f908411f4ed97cbc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d834c51ba7214c43afb6dd8998ac2a6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}